{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "import ipywidgets as widgets\n",
    "import numpy as np\n",
    "import torch\n",
    "from IPython.display import clear_output, display\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import eli.encoder\n",
    "\n",
    "importlib.reload(eli.encoder)\n",
    "from einops import einsum\n",
    "\n",
    "from eli.config import cfg, encoder_cfg\n",
    "from eli.encoder import (\n",
    "    PROMPT_DECODER,\n",
    "    Encoder,\n",
    "    EncoderDecoder,\n",
    "    EncoderTrainer,\n",
    "    calculate_target_prediction_loss,\n",
    "    get_embeddings_from_decoder,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load encoder and decoder\n",
    "cfg.buffer_size_samples = cfg.target_model_batch_size_samples = (\n",
    "    cfg.train_batch_size_samples\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg.decoder_model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "encoder_decoder = EncoderDecoder(cfg, encoder_cfg, tokenizer).to(cfg.device)\n",
    "\n",
    "encoder = Encoder(cfg, encoder_cfg).to(cfg.device)\n",
    "\n",
    "encoder_path = \"encoder-gpt2.pt\"\n",
    "encoder.load_state_dict(torch.load(encoder_path))\n",
    "\n",
    "encoder_decoder.encoder = encoder\n",
    "\n",
    "encoder_decoder = encoder_decoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(cfg.target_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:target_generated_tokens size: 1024 bytes (0.00 MB)\n",
      "INFO:root:target_acts size: 786432 bytes (0.75 MB)\n",
      "INFO:root:input_tokens size: 65536 bytes (0.06 MB)\n",
      "INFO:root:Total shared memory size: 0.00 GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing data collector\n",
      "Collecting data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3741c8032584068bb3b51a04d9f0efc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/41.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8138eedbb8104c71aabb19f4f29b6e22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fea6c9c55714f7ea551d711d7701aab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Tokenize and concatenate called\n",
      "INFO:root:Full text length: 43727350\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (488415 > 1024). Running this sequence through the model will result in indexing errors\n",
      "INFO:root:Num tokens: 9655191\n",
      "/root/eli/.venv/lib/python3.12/site-packages/datasets/formatting/torch_formatter.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(value, **{**default_dtype, **self.torch_tensor_kwargs})\n",
      "INFO:root:Processing data directly on cuda without workers\n",
      "INFO:root:Processing chunk 0:256 on cuda\n",
      "INFO:root:Processing batch 0:256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2 into HookedTransformer\n",
      "Moving model to device:  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Direct data processing completed\n"
     ]
    }
   ],
   "source": [
    "# Load eval data\n",
    "\n",
    "from eli.data import DataCollector\n",
    "\n",
    "cfg.use_data_collector_workers = False\n",
    "\n",
    "print(\"Initializing data collector\")\n",
    "data_collector = DataCollector(cfg)\n",
    "\n",
    "print(\"Collecting data\")\n",
    "data_collector.collect_data()\n",
    "\n",
    "data = data_collector.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 3]) torch.Size([256, 3])\n",
      "Target prediction loss: 2.9189789295196533\n"
     ]
    }
   ],
   "source": [
    "# Print loss statistics\n",
    "target_generated_tokens = data[\"target_generated_tokens\"]\n",
    "target_acts = data[\"target_acts\"]\n",
    "\n",
    "buffer_size = target_acts.shape[0]\n",
    "batch_size = cfg.train_batch_size_samples\n",
    "num_batches = buffer_size // batch_size\n",
    "\n",
    "target_prediction_losses = []\n",
    "dinalar_losses = []\n",
    "\n",
    "\n",
    "def recode_and_strip(tokens, tokenizer):\n",
    "    decoded = tokenizer.batch_decode(\n",
    "        sequences=tokens,\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=True,\n",
    "    )\n",
    "    decoded = [r.strip() for r in decoded]\n",
    "    encoded = tokenizer(\n",
    "        decoded, add_special_tokens=False, return_tensors=\"pt\", padding=True\n",
    "    )\n",
    "    output_tokens = encoded.input_ids.long()\n",
    "    attention_mask = encoded.attention_mask\n",
    "    return output_tokens, attention_mask\n",
    "\n",
    "\n",
    "for batch_idx in range(num_batches):\n",
    "    start_idx = batch_idx * batch_size\n",
    "    end_idx = start_idx + batch_size\n",
    "\n",
    "    # Extract batch data and move to device\n",
    "    batch_tokens = target_generated_tokens[start_idx:end_idx].to(cfg.device)\n",
    "    batch_acts = target_acts[start_idx:end_idx].to(cfg.device)\n",
    "\n",
    "    batch_tokens, attention_mask = recode_and_strip(batch_tokens, tokenizer)\n",
    "    batch_tokens = batch_tokens.to(cfg.device)\n",
    "\n",
    "    print(batch_tokens.shape, attention_mask.shape)\n",
    "\n",
    "    attention_mask = attention_mask.to(cfg.device)\n",
    "\n",
    "    loss = EncoderTrainer.loss(\n",
    "        cfg,\n",
    "        encoder_decoder,\n",
    "        batch_tokens,\n",
    "        attention_mask,\n",
    "        batch_acts,\n",
    "        tokenizer,\n",
    "        train_iter=-1,\n",
    "    )\n",
    "    target_prediction_losses.append(loss.item())\n",
    "\n",
    "print(f\"Target prediction loss: {np.mean(target_prediction_losses)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ab966cec649482f942103aa1c08fbf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc36a002f8db4e09b452b713df717d56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 290,   11,  198,  307,  284,  357,  393,  287,   12,  379,   13,  423,\n",
      "          319, 1011,  422,  257,  351,  329,  416,  345,  670,  373,  262,  407,\n",
      "          366,  355,  318,  286,  326,  530,   65,  389, 1282, 1222,  651,   14,\n",
      "          779, 1577,  503, 1394]], device='cuda:0')\n",
      "torch.Size([1, 40])\n",
      "top_values.shape torch.Size([1, 40])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  11,  290,  286,  284,   12,  287,  319,   13,  198,  351,  379,  416,\n",
      "            0,  329,  257,  262,  355,  357,  326,   14, 1222,  393,  366,  503,\n",
      "          422,   25,   64,  259,  530,  832,   69,  732, 1659,    5,   65,  345,\n",
      "          261,  292,  428,  636]], device='cuda:0')\n",
      "torch.Size([1, 40])\n",
      "top_values.shape torch.Size([1, 40])\n",
      "tensor([[ 11, 290, 198, 287, 319, 284,  13, 257, 422, 379,  12, 357, 355, 351,\n",
      "         326, 655, 407, 262, 393, 366, 503,  14, 329, 530, 286, 373, 318, 900,\n",
      "          64, 288, 513, 588, 925, 281, 546, 736, 628, 716, 416, 572]],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 40])\n",
      "top_values.shape torch.Size([1, 40])\n",
      "tensor([[  11,  319,  290,  287,  284,  198,   12,   13,  422,  379,  326,  351,\n",
      "          357,  257,   25,  286,  329,  318,  262,  416,    0, 1377,  428,  373,\n",
      "          503,  393,  389,   64,   14,  851,   30,  378,  355,  345,  546,  259,\n",
      "          366,  423,  938,  338]], device='cuda:0')\n",
      "torch.Size([1, 40])\n",
      "top_values.shape torch.Size([1, 40])\n",
      "tensor([[  11,   13,  290,  284,  198,   12,   14,  357,  287,  351,  329,  649,\n",
      "          366,    0,  257,  355,  734, 1222,  416,  393, 1643,  530,  319, 1256,\n",
      "          422,  326, 3155,  379,   25,   62,  670,    6,  986, 3704,  262,  423,\n",
      "          286,  373, 1178,  900]], device='cuda:0')\n",
      "torch.Size([1, 40])\n",
      "top_values.shape torch.Size([1, 40])\n",
      "tensor([[  11,  284,  198,  319,  290,   12,  287,   13,  357,  257,  379,  422,\n",
      "          351,  326, 1762,  262,  329,    6,  973,  355, 4481,  503,   14,  925,\n",
      "          366,  393,  530,  416,  588,  340,  832,  288,  670,  628,   65,   25,\n",
      "         2716,  281,  423, 2615]], device='cuda:0')\n",
      "torch.Size([1, 40])\n",
      "top_values.shape torch.Size([1, 40])\n",
      "tensor([[ 290,   11,   13,  284,  319,  198,  287,  379,  326,   12,  357,  351,\n",
      "          257,  355,  329,  422,  423,  416,  393,    0,  550,  262,  366,  530,\n",
      "          938,  503,  373, 1222,  288,   14, 4481,  523,  851, 2716,  286,  572,\n",
      "          618,   65, 1363,  736]], device='cuda:0')\n",
      "torch.Size([1, 40])\n",
      "top_values.shape torch.Size([1, 40])\n",
      "tensor([[ 290,   11,   13,  284,  319,  287,  198,   12,  357,  286,  329,  351,\n",
      "          355,  379,  326,   14,  416,  393, 1222,  422,    0,  257,  366,  530,\n",
      "          262,   25,  503,  423,  338,  900,  649,  851,  736,  288,  428,  804,\n",
      "          572, 2041,   64,  618]], device='cuda:0')\n",
      "torch.Size([1, 40])\n",
      "top_values.shape torch.Size([1, 40])\n",
      "tensor([[  11,  284,  290,  319,   13,  198,   12,  326,  546,  379,  287,  357,\n",
      "          257,  351,  286,  329,  262,  703,  314,  355,  422,  611,  644,  530,\n",
      "          508,  378,  345,  810,  938,  416,  340,  986,   65,  361,  393, 1807,\n",
      "            0,   64,  523,  366]], device='cuda:0')\n",
      "torch.Size([1, 40])\n",
      "top_values.shape torch.Size([1, 40])\n",
      "tensor([[  12,   11,  198,  290,   14,   13,  357,  284,  423,  288,    9,  393,\n",
      "          338,  299,  319,  286,  378,  326,   25,   64, 1222,  257, 4210, 1011,\n",
      "           75,  960,    0,  351,  389,    6,   79,  366,  287,  439,  262,  318,\n",
      "          314,   69,  805,  373]], device='cuda:0')\n",
      "torch.Size([1, 40])\n",
      "top_values.shape torch.Size([1, 40])\n",
      "tensor([[  198,   290,    12,    11,   284,    14,    13,  4210,   357,   393,\n",
      "           326,   960,     9,   338,   318,   423,   286,   262, 20543,    25,\n",
      "           329,   389,   287,   319,  1849,   314,   351,   299,   422,     0,\n",
      "            62,  1222,   379,   366,   460,  3907,   220,   257,  1011,   416]],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 40])\n",
      "top_values.shape torch.Size([1, 40])\n",
      "tensor([[ 198,  290,   11,  373,  284,  318,   12,  357,   13,  319,  422,  287,\n",
      "          338,   69,  351,  366,  416,  288,  379,  257,    9,  262,  550,  355,\n",
      "          481,  299,  460, 1011,  468, 1203,  329,  716,  307,  938,   65,  423,\n",
      "          832,  851, 1394,  732]], device='cuda:0')\n",
      "torch.Size([1, 40])\n",
      "top_values.shape torch.Size([1, 40])\n",
      "tensor([[ 290,  284,   11,  393,  357,   12,  326,  379,   13,  198,    6,  329,\n",
      "          293, 1363,  262,  257,  351,  422,  532,  319,  272,  546,   14,  672,\n",
      "          530,  505, 1377,  286,  851,  749,  318,  617,   70,  345,   65, 1222,\n",
      "          442,  717,  287,  288]], device='cuda:0')\n",
      "torch.Size([1, 40])\n",
      "top_values.shape torch.Size([1, 40])\n",
      "tensor([[ 290,   11,  198,  716,  423,  284,  804,  288,   13, 1807,  892,  357,\n",
      "           12,  466, 1011, 1254,  836,  389, 1394,  651, 6938,  393,  287,  366,\n",
      "          765,  460, 1101, 1842,  257,  467,  761,  379,  314,  329,  300,  373,\n",
      "          318, 1337,  550,  588]], device='cuda:0')\n",
      "torch.Size([1, 40])\n",
      "top_values.shape torch.Size([1, 40])\n",
      "tensor([[  11,  290,  198,  357,  284,   13,  287,   12,  319,  393,  326,  314,\n",
      "          379,   14,  286,  366,  329,  262,  422,  355,  257,    0,  351,  345,\n",
      "          628,  340,   25,  416,  661, 1222,  530,  318,  428,    7,   40,  373,\n",
      "          986,  851,  523,  317]], device='cuda:0')\n",
      "torch.Size([1, 40])\n",
      "top_values.shape torch.Size([1, 40])\n",
      "tensor([[  11,  290,  319,  284,  287,  351,   13,  198,   12,  329,  379,  326,\n",
      "          357,  257,  262,  422,  355,  393,  416,  546,   14,  366,  286,  503,\n",
      "          428,   64, 1222,   25,  530,    0, 1363,  314,  851,  661,  340,  625,\n",
      "          378,   75,  736,  318]], device='cuda:0')\n",
      "torch.Size([1, 40])\n",
      "top_values.shape torch.Size([1, 40])\n",
      "tensor([[ 198,  290,   11,   12,  423,   13,  389,  357, 1011,  284,  299,   14,\n",
      "          393,  716,  467,   75,  805,  314,    9,  288,  319,  378,  287,  670,\n",
      "           69, 1394,  326,  651,  321,  257,  892,  804,  547,  993,  277,  422,\n",
      "           65,  460,  351,  272]], device='cuda:0')\n",
      "torch.Size([1, 40])\n",
      "top_values.shape torch.Size([1, 40])\n",
      "tensor([[  198,    11,   290,    12,   284,    14,    13,  4210,   357,   393,\n",
      "           326,   318, 20543,     9,   960,   286,   319,   287,    25,   314,\n",
      "           338,   262,   329,   389,   351,     0,   423,   299,   422,  1222,\n",
      "           257,   416,    62,  1849,    30,   366,   220,   628,  3907,   851]],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 40])\n",
      "top_values.shape torch.Size([1, 40])\n",
      "tensor([[ 198,  290,   11,  284,  373,  318,   13,  319,   12, 4210,  357,  351,\n",
      "          422,  338,   69,  287,  257,  851,  288,  262,  366,  416,  379,  299,\n",
      "          460,  938,    9,  329,  481,  291,  271,  355,  550,  960,  314,  832,\n",
      "         1011,  468,  277,  423]], device='cuda:0')\n",
      "torch.Size([1, 40])\n",
      "top_values.shape torch.Size([1, 40])\n",
      "tensor([[ 198,  716,  423, 1101,  290, 1394,  892,  288,  651,   11, 1011, 1053,\n",
      "         1254, 1807,  284,  545, 4398,  836,  467,  373,  460,  766,  588,  357,\n",
      "          550, 1612, 1975,  351,  804, 1577,   13,  307, 1514, 1842,  805,  277,\n",
      "          321, 3505,  993,    9]], device='cuda:0')\n",
      "torch.Size([1, 40])\n",
      "top_values.shape torch.Size([1, 40])\n",
      "tensor([[ 290,   11,  357,  198,  284,   12,   13,  257,  329,  393,  262,  319,\n",
      "         1100,  287,  379,  366,  423,  307,  766,  351, 1011,   14,  651,  326,\n",
      "          355,  422,  661,  804,    6,  530, 1394,  340, 1222,  467,  407, 1377,\n",
      "           65,  345,  655,  373]], device='cuda:0')\n",
      "torch.Size([1, 40])\n",
      "top_values.shape torch.Size([1, 40])\n",
      "tensor([[ 290,   11,  319,  198,  373,  284,   12,  338,  318,   13,  287,  257,\n",
      "          366,  422,   69,  468,  262,  357,  351, 1625,  550,  732,   75,  938,\n",
      "          416,  851,  291,  355,  379,  307,  460,  423,   64, 1363,  404,  326,\n",
      "         2107,  481,  503,  832]], device='cuda:0')\n",
      "torch.Size([1, 40])\n",
      "top_values.shape torch.Size([1, 40])\n"
     ]
    }
   ],
   "source": [
    "from jaxtyping import Float\n",
    "from torch import Tensor\n",
    "\n",
    "# Create output widgets for displaying sample information\n",
    "sample_output = widgets.Output()\n",
    "button_output = widgets.Output()\n",
    "\n",
    "# Create a counter and button\n",
    "current_sample = 179\n",
    "\n",
    "\n",
    "def get_similarities_to_embeddings(\n",
    "    embeddings: Float[Tensor, \"batch tok d_embed\"],\n",
    "    target_embeddings: Float[Tensor, \"vocab d_embed\"],\n",
    ") -> Float[Tensor, \"batch tok vocab\"]:\n",
    "    \"\"\"Computes the cosine similarity between each token embedding and each target embedding.\"\"\"\n",
    "    # embeddings shape: [batch, tok, d_embed]\n",
    "    # target_embeddings shape: [vocab, d_embed]\n",
    "\n",
    "    # Reshape embeddings to [batch*tok, d_embed]\n",
    "    batch_size, seq_len, d_embed = embeddings.shape\n",
    "\n",
    "    target_embeddings_norm = target_embeddings / target_embeddings.norm(\n",
    "        dim=-1, keepdim=True\n",
    "    )\n",
    "    embeddings_norm = embeddings / embeddings.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    return einsum(\n",
    "        embeddings_norm,\n",
    "        target_embeddings_norm,\n",
    "        \"batch tok d_embed, vocab d_embed -> batch tok vocab\",\n",
    "    )\n",
    "\n",
    "\n",
    "def on_button_click(b):\n",
    "    global current_sample\n",
    "    if current_sample < batch_size:\n",
    "        display_sample(current_sample)\n",
    "        current_sample += 1\n",
    "    else:\n",
    "        with sample_output:\n",
    "            print(\"End of batch reached!\")\n",
    "\n",
    "\n",
    "def create_table(title, headers, rows, col_widths=None):\n",
    "    \"\"\"Helper function to create formatted tables\n",
    "\n",
    "    Args:\n",
    "        title: Table title string\n",
    "        headers: List of header strings\n",
    "        rows: List of rows, where each row is a list of values\n",
    "        col_widths: List of column widths (defaults to 15 for all columns)\n",
    "\n",
    "    Returns:\n",
    "        Formatted table string\n",
    "    \"\"\"\n",
    "    if col_widths is None:\n",
    "        col_widths = [15] * len(headers)\n",
    "\n",
    "    # Ensure first column width accommodates row labels\n",
    "    col_widths[0] = max(col_widths[0], 8)\n",
    "\n",
    "    # Create table string\n",
    "    table = f\"{title}\\n\"\n",
    "\n",
    "    # Create header\n",
    "    header_row = headers[0].ljust(col_widths[0])\n",
    "    for i, header in enumerate(headers[1:], 1):\n",
    "        header_row += header.ljust(col_widths[i])\n",
    "    table += header_row + \"\\n\"\n",
    "\n",
    "    # Add separator\n",
    "    table += \"-\" * len(header_row) + \"\\n\"\n",
    "\n",
    "    # Add rows\n",
    "    for row in rows:\n",
    "        row_str = str(row[0]).ljust(col_widths[0])\n",
    "        for i, cell in enumerate(row[1:], 1):\n",
    "            row_str += str(cell).ljust(col_widths[i])\n",
    "        table += row_str + \"\\n\"\n",
    "\n",
    "    return table\n",
    "\n",
    "\n",
    "def shuffle_data(tokens, acts, seed=None):\n",
    "    \"\"\"\n",
    "    Shuffle the tokens and acts tensors in the same order.\n",
    "\n",
    "    Args:\n",
    "        tokens: Tensor of token IDs\n",
    "        acts: Tensor of activations\n",
    "        seed: Optional random seed for reproducibility\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (shuffled_tokens, shuffled_acts)\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "    # Get the number of samples\n",
    "    num_samples = tokens.size(0)\n",
    "\n",
    "    # Generate random permutation indices\n",
    "    indices = torch.randperm(num_samples)\n",
    "\n",
    "    # Shuffle both tensors using the same indices\n",
    "    shuffled_tokens = tokens[indices]\n",
    "    shuffled_acts = acts[indices]\n",
    "\n",
    "    return shuffled_tokens, shuffled_acts\n",
    "\n",
    "\n",
    "losses = []\n",
    "\n",
    "\n",
    "# Function to display a single sample\n",
    "def display_sample(sample_idx):\n",
    "    with torch.no_grad():\n",
    "        with torch.autocast(device_type=cfg.device.type, dtype=cfg.dtype):\n",
    "            # Extract single sample as a \"batch\" of size 1\n",
    "            sample_tokens = target_generated_tokens[sample_idx : sample_idx + 1].to(\n",
    "                cfg.device, dtype=torch.long\n",
    "            )\n",
    "            sample_tokens, attention_mask = recode_and_strip(sample_tokens, tokenizer)\n",
    "            sample_tokens = sample_tokens.to(cfg.device)\n",
    "            attention_mask = attention_mask.to(cfg.device)\n",
    "            sample_acts = target_acts[sample_idx : sample_idx + 1].to(cfg.device)\n",
    "\n",
    "            # encoder = encoder_decoder.encoder\n",
    "            # encoder_output_logits = encoder(sample_acts) # [batch tok vocab]\n",
    "\n",
    "            # Convert logits to one-hot-like by making the max value very large and others small\n",
    "            # max_values, max_indices = torch.max(encoder_output_logits, dim=-1, keepdim=True)\n",
    "            # one_hot_logits = torch.ones_like(encoder_output_logits) * -100.0  # Set all values to a small number\n",
    "            # one_hot_logits.scatter_(dim=-1, index=max_indices, value=100.0)   # Set max values to a large number\n",
    "            # encoder_output_logits = one_hot_logits\n",
    "\n",
    "            # Get model outputs for this single sample\n",
    "            (decoder_logits_target, decoder_logits_encoding, virtual_embs) = (\n",
    "                encoder_decoder(\n",
    "                    sample_acts, sample_tokens, attention_mask, train_iter=-1\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # Calculate losses using existing functions\n",
    "            pred_loss = calculate_target_prediction_loss(\n",
    "                decoder_logits_target, sample_tokens, tokenizer\n",
    "            ).item()\n",
    "            losses.append(pred_loss)\n",
    "            # din_loss = calculate_dinalar_loss(\n",
    "            #     decoder_logits_encoding,\n",
    "            #     encoder_output_logits,\n",
    "            # ).item()\n",
    "\n",
    "            embeddings = get_embeddings_from_decoder(encoder_decoder.decoder).weight\n",
    "            similarities = get_similarities_to_embeddings(\n",
    "                virtual_embs, embeddings\n",
    "            )  # [batch tok vocab]\n",
    "\n",
    "            # print(\"distances.shape\", distances.shape)\n",
    "\n",
    "            # Get top k tokens by encoder output logits\n",
    "            top_k = 40\n",
    "            top_values, top_indices = torch.topk(\n",
    "                similarities[0], k=top_k, dim=-1\n",
    "            )  # [batch tok]\n",
    "\n",
    "            print(top_indices)\n",
    "            print(top_indices.shape)\n",
    "\n",
    "            print(\"top_values.shape\", top_values.shape)\n",
    "\n",
    "            # Decode tokens for display\n",
    "            sample_decoded = tokenizer.decode(sample_tokens[0])\n",
    "\n",
    "            # Display results\n",
    "            with sample_output:\n",
    "                sample_output.clear_output(wait=True)\n",
    "                print(f\"Sample {sample_idx + 1}/{batch_size}\")\n",
    "                print(f\"Target prediction loss: {pred_loss:.6f}\")\n",
    "                print(\"\\nTarget tokens:\")\n",
    "                print(sample_decoded, \"\\n\")\n",
    "\n",
    "                prompt_prefix, prompt_suffix = PROMPT_DECODER.split(\"<thought>\")\n",
    "                # Also decode and display the prefix and suffix tokens\n",
    "                prefix_tokens = tokenizer(prompt_prefix, return_tensors=\"pt\").input_ids[\n",
    "                    0\n",
    "                ]\n",
    "                suffix_tokens = tokenizer(prompt_suffix, return_tensors=\"pt\").input_ids[\n",
    "                    0\n",
    "                ]\n",
    "\n",
    "                print(tokenizer.decode(prefix_tokens))\n",
    "\n",
    "                col_width = 15\n",
    "                headers = [\"Token\"]\n",
    "                for i in range(virtual_embs.shape[1]):\n",
    "                    headers.extend([f\"Emb {i}\", f\"Sim {i}\"])\n",
    "\n",
    "                token_rows = []\n",
    "                for k in range(top_k):\n",
    "                    row = [f\"Top {k + 1}:\"]\n",
    "                    for j in range(virtual_embs.shape[1]):\n",
    "                        token_id = top_indices[j, k].item()\n",
    "                        token_text = tokenizer.decode([token_id])\n",
    "                        # Replace newlines and tabs for cleaner display\n",
    "                        token_text = token_text.replace(\"\\n\", \"\\\\n\").replace(\n",
    "                            \"\\t\", \"\\\\t\"\n",
    "                        )\n",
    "                        # Truncate to fit in column\n",
    "                        token_display = token_text[: col_width - 2]\n",
    "                        # Add token and similarity as separate columns\n",
    "                        row.append(token_display)\n",
    "                        row.append(f\"{top_values[j, k].item():.3f}\")\n",
    "                    token_rows.append(row)\n",
    "\n",
    "                # Create and display the token table\n",
    "                token_table = create_table(\n",
    "                    \"\",\n",
    "                    headers,\n",
    "                    token_rows,\n",
    "                    [8]\n",
    "                    + [col_width, 8] * virtual_embs.shape[1],  # Adjusted column widths\n",
    "                )\n",
    "                print(token_table)\n",
    "\n",
    "                print(tokenizer.decode(suffix_tokens))\n",
    "\n",
    "                print(sum(losses) / len(losses))\n",
    "\n",
    "\n",
    "# Interactive sample investigation\n",
    "next_button = widgets.Button(description=\"Next Sample\")\n",
    "next_button.on_click(on_button_click)\n",
    "\n",
    "# Display the button and sample output in separate areas\n",
    "with button_output:\n",
    "    display(next_button)\n",
    "display(button_output)\n",
    "display(sample_output)\n",
    "\n",
    "# Show the first sample\n",
    "if batch_size > 0:\n",
    "    display_sample(current_sample)\n",
    "    current_sample += 1\n",
    "else:\n",
    "    with sample_output:\n",
    "        print(\"No samples in batch!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([], device='cuda:0', size=(1, 0), dtype=torch.int64)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_idx = 60\n",
    "\n",
    "sample_tokens = target_generated_tokens[sample_idx : sample_idx + 1].to(\n",
    "    cfg.device, dtype=torch.long\n",
    ")\n",
    "sample_tokens, attention_mask = recode_and_strip(sample_tokens, tokenizer)\n",
    "sample_tokens = sample_tokens.to(cfg.device)\n",
    "attention_mask = attention_mask.to(cfg.device)\n",
    "sample_acts = target_acts[sample_idx : sample_idx + 1].to(cfg.device)\n",
    "\n",
    "print(sample_tokens)\n",
    "print(tokenizer.decode(sample_tokens[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = \"Bob\"\n",
    "# tokens = tokenizer(text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "# Print each token separately\n",
    "# print(\"Tokens for text:\", text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[44484,   474,   330,   672,  1559,   373,   257,   922,  1048,    13,\n",
      "         14862,   220]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "tensor([[44484,   474,   330,   672,  1559,   373,   257,   922,  1048,    13,\n",
      "         14862,   220]])\n",
      "torch.Size([50257])\n",
      "Top 10 tokens by logit:\n",
      "1. Token: 'Â ', ID: 1849, Logit: -46.7644\n",
      "2. Token: 'iced', ID: 3711, Logit: -47.3270\n",
      "3. Token: 'ich', ID: 488, Logit: -48.3128\n",
      "4. Token: 'ive', ID: 425, Logit: -48.6372\n",
      "5. Token: 'irl', ID: 1901, Logit: -48.7395\n",
      "6. Token: 'ix', ID: 844, Logit: -49.0847\n",
      "7. Token: 'iz', ID: 528, Logit: -49.1540\n",
      "8. Token: 'izzy', ID: 40593, Logit: -49.2239\n",
      "9. Token: 'ike', ID: 522, Logit: -49.2902\n",
      "10. Token: '________', ID: 2602, Logit: -49.3509\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "text = \"Alice jacobson was a good person. Alice \"\n",
    "tokens = tokenizer(text, add_special_tokens=True, return_tensors=\"pt\")\n",
    "print(tokens)\n",
    "print(tokens.input_ids)\n",
    "tokens = torch.cat([torch.tensor([[tokenizer.bos_token_id]]), tokens.input_ids], dim=1)\n",
    "\n",
    "# Get model outputs\n",
    "outputs = model(tokens)\n",
    "logits = outputs.logits\n",
    "\n",
    "# Get the logits for the last token\n",
    "last_token_logits = logits[0, -1, :]\n",
    "print(last_token_logits.shape)\n",
    "\n",
    "# Get the top 3 token indices and their corresponding logits\n",
    "k = 10\n",
    "top_values, top_indices = torch.topk(last_token_logits, k)\n",
    "\n",
    "# Print the top 3 tokens and their logits\n",
    "print(f\"Top {k} tokens by logit:\")\n",
    "for i, (token_id, logit_value) in enumerate(zip(top_indices, top_values)):\n",
    "    token_text = tokenizer.decode([token_id.item()])\n",
    "    print(\n",
    "        f\"{i + 1}. Token: '{token_text}', ID: {token_id.item()}, Logit: {logit_value.item():.4f}\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
