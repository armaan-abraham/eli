{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'PROMPT_PREFIX' from 'eli.encoder' (/root/eli/src/eli/encoder.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01meli\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cfg, encoder_cfg\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01meli\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mencoder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      9\u001b[39m     PROMPT_PREFIX,\n\u001b[32m     10\u001b[39m     PROMPT_SUFFIX,\n\u001b[32m     11\u001b[39m     Encoder,\n\u001b[32m     12\u001b[39m     EncoderDecoder,\n\u001b[32m     13\u001b[39m     EncoderTrainer,\n\u001b[32m     14\u001b[39m     calculate_dinalar_loss,\n\u001b[32m     15\u001b[39m     get_embeddings_from_decoder,\n\u001b[32m     16\u001b[39m     kl_div,\n\u001b[32m     17\u001b[39m )\n\u001b[32m     19\u001b[39m cfg.buffer_size_samples = cfg.target_model_batch_size_samples = cfg.train_batch_size_samples\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'PROMPT_PREFIX' from 'eli.encoder' (/root/eli/src/eli/encoder.py)"
     ]
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "import numpy as np\n",
    "import torch\n",
    "from IPython.display import clear_output, display\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from eli.config import cfg, encoder_cfg\n",
    "from eli.encoder import (\n",
    "    PROMPT_PREFIX,\n",
    "    PROMPT_SUFFIX,\n",
    "    Encoder,\n",
    "    EncoderDecoder,\n",
    "    EncoderTrainer,\n",
    "    calculate_dinalar_loss,\n",
    "    get_embeddings_from_decoder,\n",
    "    kl_div,\n",
    ")\n",
    "\n",
    "cfg.buffer_size_samples = cfg.target_model_batch_size_samples = cfg.train_batch_size_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(cfg.target_model_name).to(cfg.device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg.target_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing data collector\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:target_generated_tokens size: 16384 bytes (0.02 MB)\n",
      "INFO:root:target_acts size: 262144 bytes (0.25 MB)\n",
      "INFO:root:input_tokens size: 65536 bytes (0.06 MB)\n",
      "INFO:root:Total shared memory size: 0.00 GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1461b86c3734ecfb491c9b5c93a88c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7f694f461e245478174845e551af57b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Tokenize and concatenate called\n",
      "INFO:root:Full text length: 43727350\n",
      "INFO:root:Num tokens: 9672977\n",
      "/root/eli/.venv/lib/python3.12/site-packages/datasets/formatting/torch_formatter.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(value, **{**default_dtype, **self.torch_tensor_kwargs})\n",
      "INFO:root:Processing data directly on cuda without workers\n",
      "INFO:root:Processing chunk 0:256 on cuda\n",
      "INFO:root:Processing batch 0:256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model EleutherAI/pythia-31m into HookedTransformer\n",
      "Moving model to device:  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:CHUNK 0:256 COMPLETED, Max GPU memory allocated: 2.14 GB, Max GPU memory reserved: 2.39 GB\n",
      "INFO:root:Direct data processing completed\n"
     ]
    }
   ],
   "source": [
    "# Load eval data\n",
    "\n",
    "from eli.data import DataCollector\n",
    "\n",
    "cfg.use_data_collector_workers = False\n",
    "\n",
    "print(\"Initializing data collector\")\n",
    "data_collector = DataCollector(cfg)\n",
    "\n",
    "print(\"Collecting data\")\n",
    "data_collector.collect_data()\n",
    "\n",
    "data = data_collector.data\n",
    "\n",
    "target_generated_tokens = data[\"target_generated_tokens\"]\n",
    "\n",
    "batch_size = target_generated_tokens.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_generated_tokens = tokenizer(\". My style was great. How did it happen?\", return_tensors=\"pt\").input_ids.to(cfg.device)\n",
    "# print(target_generated_tokens)\n",
    "target_generated_tokens = torch.tensor([[  15, 2752, 3740,  369, 1270,   15, 1359,  858,  352, 5108,   32,  187,\n",
    "           42,  812, 1611,  562]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39fa439ebe7045e1927a2e6a2bec72f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a22be6ebf1f14eef9576cf7a28ab843b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokens shape: torch.Size([1, 16])\n",
      "User text tokens shape: torch.Size([1, 4])\n",
      "torch.Size([1, 16, 50304])\n"
     ]
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# Create output widgets\n",
    "context_output = widgets.Output()\n",
    "button_area = widgets.Output()\n",
    "\n",
    "# Create sample counter\n",
    "current_sample = 0\n",
    "\n",
    "from eli.encoder import PROMPT_PREFIX, PROMPT_SUFFIX, prepend_bos_token\n",
    "\n",
    "def assemble_decoder_context_without_virtual(text, sample_idx=0):\n",
    "    \"\"\"Assemble decoder context using only user-provided text (no virtual embeddings)\"\"\"\n",
    "    with torch.no_grad():\n",
    "        with torch.autocast(device_type=cfg.device.type, dtype=cfg.dtype):\n",
    "            # Get a sample from the data\n",
    "            sample_tokens = target_generated_tokens[sample_idx:sample_idx+1].to(cfg.device)\n",
    "            print(\"Sample tokens shape:\", sample_tokens.shape)\n",
    "            \n",
    "            # Tokenize the user text\n",
    "            # user_text_tokens = tokenizer(text, return_tensors=\"pt\", add_special_tokens=False).input_ids.to(cfg.device)\n",
    "            user_text_tokens = torch.tensor([[3, 2, 3, 1]], device=cfg.device)\n",
    "            print(\"User text tokens shape:\", user_text_tokens.shape)\n",
    "            \n",
    "            # Generate tokens for prompt components\n",
    "            prefix_tokens = tokenizer(PROMPT_PREFIX, return_tensors=\"pt\", add_special_tokens=False).input_ids.to(cfg.device)\n",
    "            prefix_tokens = prepend_bos_token(prefix_tokens, tokenizer)\n",
    "            suffix_tokens = tokenizer(PROMPT_SUFFIX, return_tensors=\"pt\", add_special_tokens=False).input_ids.to(cfg.device)\n",
    "            \n",
    "            # Repeat for batch size\n",
    "            batch_size = sample_tokens.shape[0]\n",
    "            prefix_tokens = prefix_tokens.repeat(batch_size, 1)\n",
    "            suffix_tokens = suffix_tokens.repeat(batch_size, 1)\n",
    "            user_text_tokens = user_text_tokens.repeat(batch_size, 1)\n",
    "            \n",
    "            # Concatenate all tokens to create the full context\n",
    "            # Format: [prefix] + [user_text] + [suffix] + [target_tokens]\n",
    "            input_tokens = torch.cat([\n",
    "                prefix_tokens, \n",
    "                user_text_tokens, \n",
    "                suffix_tokens, \n",
    "                sample_tokens\n",
    "            ], dim=1)\n",
    "            \n",
    "            # Get embeddings from decoder\n",
    "            embeddings = model.get_input_embeddings()\n",
    "            input_embeds = embeddings(input_tokens)\n",
    "            \n",
    "            # Create attention mask (all 1s since we're not using padding)\n",
    "            attention_mask = torch.ones(\n",
    "                input_embeds.shape[0],\n",
    "                input_embeds.shape[1],\n",
    "                device=input_embeds.device,\n",
    "            )\n",
    "            \n",
    "            # Get decoder output\n",
    "            decoder_output = model(\n",
    "                input_ids=input_tokens, \n",
    "                # attention_mask=attention_mask\n",
    "            )\n",
    "            \n",
    "            # Extract target logits for prediction loss calculation\n",
    "            decoder_logits_target = decoder_output.logits[:, -sample_tokens.shape[1] - 1: -1, :]\n",
    "            print(decoder_logits_target.shape)\n",
    "            \n",
    "            # Calculate per-token cross entropy loss\n",
    "            per_token_ce = []\n",
    "            overall_ce_loss = 0.0\n",
    "            valid_token_count = 0\n",
    "            \n",
    "            for i in range(sample_tokens.shape[1]):\n",
    "                # Calculate cross entropy for this position\n",
    "                ce_loss = torch.nn.functional.cross_entropy(\n",
    "                    decoder_logits_target[:, i], \n",
    "                    sample_tokens[:, i].long(),\n",
    "                    reduction='mean'\n",
    "                )\n",
    "                per_token_ce.append(ce_loss.item())\n",
    "                overall_ce_loss += ce_loss.item()\n",
    "                valid_token_count += 1\n",
    "            \n",
    "            # Calculate average cross entropy loss\n",
    "            overall_ce_loss = overall_ce_loss / valid_token_count if valid_token_count > 0 else float('nan')\n",
    "            \n",
    "            # Decode tokens for display\n",
    "            context_decoded = tokenizer.decode(input_tokens[0])\n",
    "            target_decoded = tokenizer.decode(sample_tokens[0])\n",
    "            \n",
    "            # Return results for display\n",
    "            return {\n",
    "                \"input_tokens\": input_tokens,\n",
    "                \"context_decoded\": context_decoded,\n",
    "                \"target_decoded\": target_decoded,\n",
    "                \"per_token_ce\": per_token_ce,  # Add per-token cross entropy\n",
    "                \"overall_ce_loss\": overall_ce_loss,  # Add overall cross entropy loss\n",
    "                \"sample_tokens\": sample_tokens,\n",
    "                \"prefix_len\": prefix_tokens.shape[1],\n",
    "                \"user_text_len\": user_text_tokens.shape[1],\n",
    "                \"suffix_len\": suffix_tokens.shape[1],\n",
    "                \"decoder_logits\": decoder_logits_target[0],\n",
    "            }\n",
    "\n",
    "def display_context_and_results(results):\n",
    "    with context_output:\n",
    "        context_output.clear_output(wait=True)\n",
    "        \n",
    "        print(f\"Sample {current_sample+1}/{batch_size}\")\n",
    "        print(f\"User text: \\\"{text_input.value}\\\"\")\n",
    "        print(f\"Overall cross entropy loss: {results['overall_ce_loss']:.6f}\")\n",
    "        \n",
    "        # Display token lengths\n",
    "        print(\"\\nContext Structure:\")\n",
    "        print(f\"Prefix tokens: {results['prefix_len']} tokens\")\n",
    "        print(f\"User text tokens: {results['user_text_len']} tokens\")\n",
    "        print(f\"Suffix tokens: {results['suffix_len']} tokens\")\n",
    "        print(f\"Target tokens: {results['sample_tokens'].shape[1]} tokens\")\n",
    "        print(f\"Total tokens: {results['input_tokens'].shape[1]} tokens\")\n",
    "        \n",
    "        # Display the entire assembled context\n",
    "        print(\"\\nAssembled Context (decoded):\")\n",
    "        print(\"-\" * 80)\n",
    "        print(results[\"context_decoded\"])\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Display the target part with both KL divergence and cross entropy\n",
    "        print(\"\\nToken-by-token metrics:\")\n",
    "        print(\"(Lower values indicate better predictions)\")\n",
    "        \n",
    "        # Create color representation\n",
    "        target_tokens = results['sample_tokens'][0]\n",
    "        per_token_ce = results['per_token_ce']\n",
    "        \n",
    "        # Create a color scale from blue (good) to red (bad)\n",
    "        def get_color_for_value(value, max_value=5.0):\n",
    "            import math\n",
    "            if math.isnan(value):\n",
    "                return \"\\033[90m\"  # gray for NaN\n",
    "            \n",
    "            # Transform value to a 0-1 scale with a logarithmic mapping\n",
    "            # Values > max_value will be bright red, values near 0 will be bright blue\n",
    "            normalized = min(1.0, max(0.0, value / max_value))\n",
    "            \n",
    "            # RGB interpolation from blue (0,0,255) to red (255,0,0)\n",
    "            if normalized < 0.5:\n",
    "                # Blue to purple\n",
    "                r = int(255 * (normalized * 2))\n",
    "                g = 0\n",
    "                b = 255\n",
    "            else:\n",
    "                # Purple to red\n",
    "                r = 255\n",
    "                g = 0\n",
    "                b = int(255 * (1 - (normalized - 0.5) * 2))\n",
    "            \n",
    "            return f\"\\033[38;2;{r};{g};{b}m\"\n",
    "        \n",
    "        # Reset color code\n",
    "        reset_color = \"\\033[0m\"\n",
    "        \n",
    "        # Display tokens with their KL and CE values, colorized\n",
    "        print(\"       Token         |  KL Div  |  CE Loss \")\n",
    "        print(\"-\" * 45)\n",
    "        \n",
    "        for i, token_id in enumerate(target_tokens):\n",
    "            token_str = tokenizer.decode([token_id.item()])\n",
    "            ce_value = per_token_ce[i] if i < len(per_token_ce) else float('nan')\n",
    "            \n",
    "            # Format token string for display (handle whitespace)\n",
    "            token_repr = repr(token_str)\n",
    "            # Pad for alignment\n",
    "            padded_token = token_repr.ljust(16)\n",
    "            \n",
    "            ce_color = get_color_for_value(ce_value)\n",
    "            \n",
    "            print(f\"{i:2d}:  {padded_token} | {ce_color}{ce_value:8.4f}{reset_color}\")\n",
    "        \n",
    "        # Also show overall prediction\n",
    "        print(\"\\nTarget tokens (what the model should predict):\")\n",
    "        print(results[\"target_decoded\"])\n",
    "\n",
    "# Create widgets\n",
    "text_input = widgets.Textarea(\n",
    "    value=\"The model is thinking about the relationship between cause and effect.\",\n",
    "    placeholder=\"Enter text to use as context...\",\n",
    "    description=\"Input Text:\",\n",
    "    disabled=False,\n",
    "    layout=widgets.Layout(width=\"100%\", height=\"100px\")\n",
    ")\n",
    "\n",
    "run_button = widgets.Button(\n",
    "    description='Run Test',\n",
    "    disabled=False,\n",
    "    button_style='primary', \n",
    "    tooltip='Run the test with the provided text'\n",
    ")\n",
    "\n",
    "next_button = widgets.Button(\n",
    "    description='Next Sample',\n",
    "    disabled=False,\n",
    "    button_style='info',\n",
    "    tooltip='Move to the next sample'\n",
    ")\n",
    "\n",
    "def on_run_button_clicked(b):\n",
    "    with context_output:\n",
    "        print(\"Running test...\")\n",
    "    results = assemble_decoder_context_without_virtual(text_input.value, current_sample)\n",
    "    display_context_and_results(results)\n",
    "\n",
    "def on_next_button_clicked(b):\n",
    "    global current_sample\n",
    "    if current_sample < batch_size - 1:\n",
    "        current_sample += 1\n",
    "        results = assemble_decoder_context_without_virtual(text_input.value, current_sample)\n",
    "        display_context_and_results(results)\n",
    "    else:\n",
    "        with context_output:\n",
    "            context_output.clear_output(wait=True)\n",
    "            print(\"End of batch reached!\")\n",
    "\n",
    "# Connect the buttons to handlers\n",
    "run_button.on_click(on_run_button_clicked)\n",
    "next_button.on_click(on_next_button_clicked)\n",
    "\n",
    "# Layout\n",
    "controls = widgets.VBox([text_input, widgets.HBox([run_button, next_button])])\n",
    "with button_area:\n",
    "    display(controls)\n",
    "    \n",
    "display(button_area)\n",
    "display(context_output)\n",
    "\n",
    "\n",
    "# Run initial test with default text\n",
    "if batch_size > 0:\n",
    "    results = assemble_decoder_context_without_virtual(text_input.value, current_sample)\n",
    "    display_context_and_results(results)\n",
    "\n",
    "\n",
    "else:\n",
    "    with context_output:\n",
    "        print(\"No samples in batch!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing target logits...\n",
      "\n",
      "Target logits for sample 0:\n",
      "================================================================================\n",
      "Full token sequence:  poet, Robert Frost, wrote this poem in 1936. It is\n",
      "================================================================================\n",
      "Sequence length: 15 tokens\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Position 0 - Actual token: ' poet' (ID: 40360, Prob: 0.3603)\n",
      "--------------------------------------------------\n",
      "  1. ' poet' (ID: 40360, Prob: 0.3603)\n",
      "  2. ' D' (ID: 423, Prob: 0.0804)\n",
      "  3. ' Po' (ID: 14128, Prob: 0.0357)\n",
      "\n",
      "Position 1 - Actual token: ',' (ID: 11, Prob: 0.2068)\n",
      "--------------------------------------------------\n",
      "  1. ',' (ID: 11, Prob: 0.2068)\n",
      "  2. ' and' (ID: 323, Prob: 0.1513)\n",
      "  3. ' Robert' (ID: 8563, Prob: 0.0862)\n",
      "\n",
      "Position 2 - Actual token: ' Robert' (ID: 8563, Prob: 0.1292)\n",
      "--------------------------------------------------\n",
      "  1. ' Robert' (ID: 8563, Prob: 0.1292)\n",
      "  2. ' Sylvia' (ID: 89406, Prob: 0.0506)\n",
      "  3. ' Walt' (ID: 36367, Prob: 0.0506)\n",
      "\n",
      "Position 3 - Actual token: ' Frost' (ID: 42320, Prob: 0.8940)\n",
      "--------------------------------------------------\n",
      "  1. ' Frost' (ID: 42320, Prob: 0.8940)\n",
      "  2. ' Lowell' (ID: 96230, Prob: 0.0306)\n",
      "  3. ' Louis' (ID: 12140, Prob: 0.0099)\n",
      "\n",
      "Position 4 - Actual token: ',' (ID: 11, Prob: 0.6452)\n",
      "--------------------------------------------------\n",
      "  1. ',' (ID: 11, Prob: 0.6452)\n",
      "  2. ' (' (ID: 320, Prob: 0.0771)\n",
      "  3. '.' (ID: 13, Prob: 0.0530)\n",
      "\n",
      "Position 5 - Actual token: ' wrote' (ID: 6267, Prob: 0.1301)\n",
      "--------------------------------------------------\n",
      "  1. ' wrote' (ID: 6267, Prob: 0.1301)\n",
      "  2. ' was' (ID: 574, Prob: 0.0894)\n",
      "  3. ' is' (ID: 374, Prob: 0.0894)\n",
      "\n",
      "Position 6 - Actual token: ' this' (ID: 420, Prob: 0.1717)\n",
      "--------------------------------------------------\n",
      "  1. ' this' (ID: 420, Prob: 0.1717)\n",
      "  2. ' the' (ID: 279, Prob: 0.1515)\n",
      "  3. ' a' (ID: 264, Prob: 0.1515)\n",
      "\n",
      "Position 7 - Actual token: ' poem' (ID: 33894, Prob: 0.5644)\n",
      "--------------------------------------------------\n",
      "  1. ' poem' (ID: 33894, Prob: 0.5644)\n",
      "  2. ' in' (ID: 304, Prob: 0.0525)\n",
      "  3. ' beautiful' (ID: 6366, Prob: 0.0361)\n",
      "\n",
      "Position 8 - Actual token: ' in' (ID: 304, Prob: 0.3690)\n",
      "--------------------------------------------------\n",
      "  1. ' in' (ID: 304, Prob: 0.3690)\n",
      "  2. ',' (ID: 11, Prob: 0.1975)\n",
      "  3. '.' (ID: 13, Prob: 0.0641)\n",
      "\n",
      "Position 9 - Actual token: ' ' (ID: 220, Prob: 0.8604)\n",
      "--------------------------------------------------\n",
      "  1. ' ' (ID: 220, Prob: 0.8604)\n",
      "  2. ' the' (ID: 279, Prob: 0.0706)\n",
      "  3. ' his' (ID: 813, Prob: 0.0139)\n",
      "\n",
      "Position 10 - Actual token: '193' (ID: 7285, Prob: 0.3294)\n",
      "--------------------------------------------------\n",
      "  1. '193' (ID: 7285, Prob: 0.3294)\n",
      "  2. '194' (ID: 6393, Prob: 0.1763)\n",
      "  3. '192' (ID: 5926, Prob: 0.1763)\n",
      "\n",
      "Position 11 - Actual token: '6' (ID: 21, Prob: 0.6731)\n",
      "--------------------------------------------------\n",
      "  1. '6' (ID: 21, Prob: 0.6731)\n",
      "  2. '9' (ID: 24, Prob: 0.0626)\n",
      "  3. '7' (ID: 22, Prob: 0.0626)\n",
      "\n",
      "Position 12 - Actual token: '.' (ID: 13, Prob: 0.5481)\n",
      "--------------------------------------------------\n",
      "  1. '.' (ID: 13, Prob: 0.5481)\n",
      "  2. ',' (ID: 11, Prob: 0.1570)\n",
      "  3. '.\n",
      "' (ID: 627, Prob: 0.0841)\n",
      "\n",
      "Position 13 - Actual token: ' It' (ID: 1102, Prob: 0.3038)\n",
      "--------------------------------------------------\n",
      "  1. ' It' (ID: 1102, Prob: 0.3038)\n",
      "  2. ' The' (ID: 578, Prob: 0.1626)\n",
      "  3. '<|eot_id|>' (ID: 128009, Prob: 0.1117)\n",
      "\n",
      "Position 14 - Actual token: ' is' (ID: 374, Prob: 0.4558)\n",
      "--------------------------------------------------\n",
      "  1. ' is' (ID: 374, Prob: 0.4558)\n",
      "  2. ' was' (ID: 574, Prob: 0.1677)\n",
      "  3. ''s' (ID: 596, Prob: 0.1480)\n",
      "\n",
      "\n",
      "Analyzing decoder logits...\n",
      "The model is thinking about cause and effect\n",
      "\n",
      "Decoder logits for sample 0 with text: 'The model is thinking about cause and effect':\n",
      "================================================================================\n",
      "Full token sequence:  poet, Robert Frost, wrote this poem in 1936. It is\n",
      "================================================================================\n",
      "Sequence length: 15 tokens\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Position 0 - Actual token: ' poet' (ID: 40360, Prob: 0.0000)\n",
      "--------------------------------------------------\n",
      "  1. '\"' (ID: 1, Prob: 0.2317)\n",
      "  2. 'It' (ID: 2181, Prob: 0.2045)\n",
      "  3. 'The' (ID: 791, Prob: 0.2045)\n",
      "\n",
      "Position 1 - Actual token: ',' (ID: 11, Prob: 0.0097)\n",
      "--------------------------------------------------\n",
      "  1. '<|eot_id|>' (ID: 128009, Prob: 0.3423)\n",
      "  2. ''s' (ID: 596, Prob: 0.0674)\n",
      "  3. '.' (ID: 13, Prob: 0.0384)\n",
      "\n",
      "Position 2 - Actual token: ' Robert' (ID: 8563, Prob: 0.0000)\n",
      "--------------------------------------------------\n",
      "  1. ' the' (ID: 279, Prob: 0.1063)\n",
      "  2. ' it' (ID: 433, Prob: 0.0502)\n",
      "  3. ' ' (ID: 220, Prob: 0.0502)\n",
      "\n",
      "Position 3 - Actual token: ' Frost' (ID: 42320, Prob: 0.8319)\n",
      "--------------------------------------------------\n",
      "  1. ' Frost' (ID: 42320, Prob: 0.8319)\n",
      "  2. ' Lowell' (ID: 96230, Prob: 0.0303)\n",
      "  3. ',' (ID: 11, Prob: 0.0173)\n",
      "\n",
      "Position 4 - Actual token: ',' (ID: 11, Prob: 0.4995)\n",
      "--------------------------------------------------\n",
      "  1. ',' (ID: 11, Prob: 0.4995)\n",
      "  2. '<|eot_id|>' (ID: 128009, Prob: 0.1838)\n",
      "  3. '.' (ID: 13, Prob: 0.0766)\n",
      "\n",
      "Position 5 - Actual token: ' wrote' (ID: 6267, Prob: 0.1987)\n",
      "--------------------------------------------------\n",
      "  1. ' wrote' (ID: 6267, Prob: 0.1987)\n",
      "  2. ' is' (ID: 374, Prob: 0.1205)\n",
      "  3. ' was' (ID: 574, Prob: 0.0569)\n",
      "\n",
      "Position 6 - Actual token: ' this' (ID: 420, Prob: 0.0036)\n",
      "--------------------------------------------------\n",
      "  1. ' \"' (ID: 330, Prob: 0.3282)\n",
      "  2. ':' (ID: 25, Prob: 0.1207)\n",
      "  3. ',' (ID: 11, Prob: 0.1207)\n",
      "\n",
      "Position 7 - Actual token: ' poem' (ID: 33894, Prob: 0.1988)\n",
      "--------------------------------------------------\n",
      "  1. ' poem' (ID: 33894, Prob: 0.1988)\n",
      "  2. '.' (ID: 13, Prob: 0.1548)\n",
      "  3. ' sentence' (ID: 11914, Prob: 0.1064)\n",
      "\n",
      "Position 8 - Actual token: ' in' (ID: 304, Prob: 0.0376)\n",
      "--------------------------------------------------\n",
      "  1. '.' (ID: 13, Prob: 0.2162)\n",
      "  2. ':' (ID: 25, Prob: 0.1312)\n",
      "  3. ',' (ID: 11, Prob: 0.1312)\n",
      "\n",
      "Position 9 - Actual token: ' ' (ID: 220, Prob: 0.8210)\n",
      "--------------------------------------------------\n",
      "  1. ' ' (ID: 220, Prob: 0.8210)\n",
      "  2. ' the' (ID: 279, Prob: 0.0525)\n",
      "  3. ' his' (ID: 813, Prob: 0.0361)\n",
      "\n",
      "Position 10 - Actual token: '193' (ID: 7285, Prob: 0.0116)\n",
      "--------------------------------------------------\n",
      "  1. '191' (ID: 7529, Prob: 0.6323)\n",
      "  2. '192' (ID: 5926, Prob: 0.3385)\n",
      "  3. '193' (ID: 7285, Prob: 0.0116)\n",
      "\n",
      "Position 11 - Actual token: '6' (ID: 21, Prob: 0.4127)\n",
      "--------------------------------------------------\n",
      "  1. '6' (ID: 21, Prob: 0.4127)\n",
      "  2. '2' (ID: 17, Prob: 0.1950)\n",
      "  3. '0' (ID: 15, Prob: 0.1340)\n",
      "\n",
      "Position 12 - Actual token: '.' (ID: 13, Prob: 0.4455)\n",
      "--------------------------------------------------\n",
      "  1. '.' (ID: 13, Prob: 0.4455)\n",
      "  2. ',' (ID: 11, Prob: 0.2104)\n",
      "  3. '<|eot_id|>' (ID: 128009, Prob: 0.0994)\n",
      "\n",
      "Position 13 - Actual token: ' It' (ID: 1102, Prob: 0.0086)\n",
      "--------------------------------------------------\n",
      "  1. '<|eot_id|>' (ID: 128009, Prob: 0.8748)\n",
      "  2. ' The' (ID: 578, Prob: 0.0384)\n",
      "  3. ' \n",
      "\n",
      "' (ID: 4815, Prob: 0.0097)\n",
      "\n",
      "Position 14 - Actual token: ' is' (ID: 374, Prob: 0.2920)\n",
      "--------------------------------------------------\n",
      "  1. ' is' (ID: 374, Prob: 0.2920)\n",
      "  2. ''s' (ID: 596, Prob: 0.0948)\n",
      "  3. ' explores' (ID: 41424, Prob: 0.0575)\n"
     ]
    }
   ],
   "source": [
    "def analyze_logits(logits, tokens, tokenizer, top_k=3, title=\"Analyzing logits\"):\n",
    "    \"\"\"\n",
    "    Analyze logits for a given sequence of tokens.\n",
    "    \n",
    "    Args:\n",
    "        logits: Tensor of logits to analyze (shape: [seq_len, vocab_size])\n",
    "        tokens: Tensor of token IDs corresponding to the actual tokens (shape: [seq_len])\n",
    "        tokenizer: Tokenizer to decode tokens\n",
    "        top_k: Number of top tokens to show for each position\n",
    "        title: Title for the analysis output\n",
    "    \"\"\"\n",
    "    # Get the true tokens for reference\n",
    "    true_tokens = tokens.tolist() if isinstance(tokens, torch.Tensor) else tokens\n",
    "    \n",
    "    print(f\"\\n{title}:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Full token sequence: {tokenizer.decode(tokens)}\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Sequence length: {len(true_tokens)} tokens\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # For each position, find the top-k tokens by logit value\n",
    "    for pos in range(min(len(true_tokens), logits.shape[0])):\n",
    "        logits_at_pos = logits[pos]\n",
    "        \n",
    "        # Convert logits to probabilities using softmax\n",
    "        probs = torch.nn.functional.softmax(logits_at_pos, dim=0)\n",
    "        \n",
    "        # Get top k tokens\n",
    "        top_values, top_indices = torch.topk(probs, top_k)\n",
    "        \n",
    "        # Get the actual token at this position\n",
    "        actual_token_id = true_tokens[pos]\n",
    "        actual_token_str = tokenizer.decode([actual_token_id])\n",
    "        actual_prob = probs[actual_token_id].item()\n",
    "        \n",
    "        # Display results\n",
    "        print(f\"\\nPosition {pos} - Actual token: '{actual_token_str}' (ID: {actual_token_id}, Prob: {actual_prob:.4f})\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        for i, (token_id, prob) in enumerate(zip(top_indices.tolist(), top_values.tolist())):\n",
    "            token_str = tokenizer.decode([token_id])\n",
    "            print(f\"  {i+1}. '{token_str}' (ID: {token_id}, Prob: {prob:.4f})\")\n",
    "\n",
    "def analyze_target_logits(sample_idx=0, top_k=3):\n",
    "    \"\"\"Analyze the target logits for a specific sample.\"\"\"\n",
    "    # Get target tokens and logits for the specified sample\n",
    "    target_tokens_sample = target_generated_tokens[sample_idx].to(cfg.device)\n",
    "    target_logits_sample = target_logits[sample_idx].to(cfg.device)\n",
    "    \n",
    "    analyze_logits(\n",
    "        target_logits_sample, \n",
    "        target_tokens_sample, \n",
    "        tokenizer, \n",
    "        top_k=top_k, \n",
    "        title=f\"Target logits for sample {sample_idx}\"\n",
    "    )\n",
    "\n",
    "def analyze_decoder_logits(text, sample_idx=0, top_k=3):\n",
    "    \"\"\"Analyze the decoder logits for a specific input text and sample.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        with torch.autocast(device_type=cfg.device.type, dtype=cfg.dtype):\n",
    "            # Get current target tokens\n",
    "            target_tokens = target_generated_tokens[sample_idx:sample_idx+1].to(cfg.device)\n",
    "            \n",
    "            # Build the full context\n",
    "            prefix_tokens = tokenizer(PROMPT_PREFIX, return_tensors=\"pt\", add_special_tokens=False).input_ids.to(cfg.device)\n",
    "            user_text_tokens = tokenizer(text, return_tensors=\"pt\", add_special_tokens=False).input_ids.to(cfg.device)\n",
    "            suffix_tokens = tokenizer(PROMPT_SUFFIX, return_tensors=\"pt\", add_special_tokens=False).input_ids.to(cfg.device)\n",
    "            \n",
    "            # Concatenate all parts except the target tokens (we'll predict them)\n",
    "            input_tokens = torch.cat([\n",
    "                prefix_tokens, \n",
    "                user_text_tokens, \n",
    "                suffix_tokens\n",
    "            ], dim=1)\n",
    "            \n",
    "            # Get the starting position for prediction\n",
    "            start_pos = input_tokens.shape[1]\n",
    "            \n",
    "            # Run the model to get decoder output\n",
    "            full_tokens = torch.cat([input_tokens, target_tokens], dim=1)\n",
    "            decoder_output = model(input_ids=full_tokens)\n",
    "            \n",
    "            # Extract logits for the positions where target tokens should be predicted\n",
    "            decoder_logits = decoder_output.logits[0, start_pos-1:start_pos-1+len(target_tokens[0])]\n",
    "            \n",
    "            analyze_logits(\n",
    "                decoder_logits,\n",
    "                target_tokens[0],\n",
    "                tokenizer,\n",
    "                top_k=top_k,\n",
    "                title=f\"Decoder logits for sample {sample_idx} with text: '{text}'\"\n",
    "            )\n",
    "\n",
    "# Run both analyses\n",
    "print(\"Analyzing target logits...\")\n",
    "analyze_target_logits(current_sample)\n",
    "\n",
    "print(\"\\n\\nAnalyzing decoder logits...\")\n",
    "print(text_input.value)\n",
    "analyze_decoder_logits(text_input.value, current_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     position  token_id              token_str    section  is_special\n",
      "0           0    128000    '<|begin_of_text|>'     PREFIX        True\n",
      "1           1       271                 '\\n\\n'     PREFIX       False\n",
      "2           2    128006  '<|start_header_id|>'     PREFIX       False\n",
      "3           3      9125               'system'     PREFIX       False\n",
      "4           4    128007    '<|end_header_id|>'     PREFIX       False\n",
      "5           5       271                 '\\n\\n'     PREFIX       False\n",
      "6           6      2675                  'You'     PREFIX       False\n",
      "7           7       990                ' work'     PREFIX       False\n",
      "8           8       439                  ' as'     PREFIX       False\n",
      "9           9       961                ' part'     PREFIX       False\n",
      "10         10       315                  ' of'     PREFIX       False\n",
      "11         11       459                  ' an'     PREFIX       False\n",
      "12         12       445                   ' L'     PREFIX       False\n",
      "13         13     11237                   'LM'     PREFIX       False\n",
      "14         14      7852              ' mechan'     PREFIX       False\n",
      "15         15      4633                'istic'     PREFIX       False\n",
      "16         16     14532           ' interpret'     PREFIX       False\n",
      "17         17      2968              'ability'     PREFIX       False\n",
      "18         18     15660            ' pipeline'     PREFIX       False\n",
      "19         19        13                    '.'     PREFIX       False\n",
      "20         20      1472                 ' You'     PREFIX       False\n",
      "21         21       527                 ' are'     PREFIX       False\n",
      "22         22       459                  ' an'     PREFIX       False\n",
      "23         23       198                   '\\n'     PREFIX       False\n",
      "24         24     74987               'expert'     PREFIX       False\n",
      "25         25       520                  ' at'     PREFIX       False\n",
      "26         26     52997          ' predicting'     PREFIX       False\n",
      "27         27      1148                ' what'     PREFIX       False\n",
      "28         28      2500             ' another'     PREFIX       False\n",
      "29         29       445                   ' L'     PREFIX       False\n",
      "30         30     11237                   'LM'     PREFIX       False\n",
      "31         31       690                ' will'     PREFIX       False\n",
      "32         32      2019                 ' say'     PREFIX       False\n",
      "33         33      1828                ' next'     PREFIX       False\n",
      "34         34        11                    ','     PREFIX       False\n",
      "35         35      2728               ' given'     PREFIX       False\n",
      "36         36       264                   ' a'     PREFIX       False\n",
      "37         37      4096         ' description'     PREFIX       False\n",
      "38         38       315                  ' of'     PREFIX       False\n",
      "39         39      1148                ' what'     PREFIX       False\n",
      "40         40       198                   '\\n'     PREFIX       False\n",
      "41         41      9210                 'that'     PREFIX       False\n",
      "42         42       445                   ' L'     PREFIX       False\n",
      "43         43     11237                   'LM'     PREFIX       False\n",
      "44         44       374                  ' is'     PREFIX       False\n",
      "45         45      5131           ' currently'     PREFIX       False\n",
      "46         46      7422            ' thinking'     PREFIX       False\n",
      "47         47        13                    '.'     PREFIX       False\n",
      "48         48    128009           '<|eot_id|>'     PREFIX        True\n",
      "49         49       271                 '\\n\\n'     PREFIX       False\n",
      "50         50    128006  '<|start_header_id|>'     PREFIX       False\n",
      "51         51       882                 'user'     PREFIX       False\n",
      "52         52    128007    '<|end_header_id|>'     PREFIX       False\n",
      "53         53       198                   '\\n'     PREFIX       False\n",
      "54         54     54644              'Predict'     PREFIX       False\n",
      "55         55      1148                ' what'     PREFIX       False\n",
      "56         56       279                 ' the'     PREFIX       False\n",
      "57         57      2768           ' following'     PREFIX       False\n",
      "58         58       445                   ' L'     PREFIX       False\n",
      "59         59     11237                   'LM'     PREFIX       False\n",
      "60         60       690                ' will'     PREFIX       False\n",
      "61         61      2019                 ' say'     PREFIX       False\n",
      "62         62      1828                ' next'     PREFIX       False\n",
      "63         63        11                    ','     PREFIX       False\n",
      "64         64      2728               ' given'     PREFIX       False\n",
      "65         65       420                ' this'     PREFIX       False\n",
      "66         66      4096         ' description'     PREFIX       False\n",
      "67         67       315                  ' of'     PREFIX       False\n",
      "68         68      1148                ' what'     PREFIX       False\n",
      "69         69       433                  ' it'     PREFIX       False\n",
      "70         70       374                  ' is'     PREFIX       False\n",
      "71         71      5131           ' currently'     PREFIX       False\n",
      "72         72      7422            ' thinking'     PREFIX       False\n",
      "73         73       512                  ':\\n'     PREFIX       False\n",
      "74         74         1                    '\"'     PREFIX       False\n",
      "75         75       791                  'The'  USER_TEXT       False\n",
      "76         76      1646               ' model'  USER_TEXT       False\n",
      "77         77       374                  ' is'  USER_TEXT       False\n",
      "78         78      7422            ' thinking'  USER_TEXT       False\n",
      "79         79       922               ' about'  USER_TEXT       False\n",
      "80         80       279                 ' the'  USER_TEXT       False\n",
      "81         81      5133        ' relationship'  USER_TEXT       False\n",
      "82         82      1990             ' between'  USER_TEXT       False\n",
      "83         83      5353               ' cause'  USER_TEXT       False\n",
      "84         84       323                 ' and'  USER_TEXT       False\n",
      "85         85      2515              ' effect'  USER_TEXT       False\n",
      "86         86        13                    '.'  USER_TEXT       False\n",
      "87         87      3343                   '\".'     SUFFIX       False\n",
      "88         88     40665             ' Provide'     SUFFIX       False\n",
      "89         89       701                ' your'     SUFFIX       False\n",
      "90         90     20212          ' prediction'     SUFFIX       False\n",
      "91         91       323                 ' and'     SUFFIX       False\n",
      "92         92      4400             ' nothing'     SUFFIX       False\n",
      "93         93       775                ' else'     SUFFIX       False\n",
      "94         94        13                    '.'     SUFFIX       False\n",
      "95         95    128009           '<|eot_id|>'     SUFFIX        True\n",
      "96         96       198                   '\\n'     SUFFIX       False\n",
      "97         97    128006  '<|start_header_id|>'     SUFFIX       False\n",
      "98         98     78191            'assistant'     SUFFIX       False\n",
      "99         99    128007    '<|end_header_id|>'     SUFFIX       False\n",
      "100       100       198                   '\\n'     SUFFIX       False\n",
      "101       101     40360                ' poet'     TARGET       False\n",
      "102       102        11                    ','     TARGET       False\n",
      "103       103      8563              ' Robert'     TARGET       False\n",
      "104       104     42320               ' Frost'     TARGET       False\n",
      "105       105        11                    ','     TARGET       False\n",
      "106       106      6267               ' wrote'     TARGET       False\n",
      "107       107       420                ' this'     TARGET       False\n",
      "108       108     33894                ' poem'     TARGET       False\n",
      "109       109       304                  ' in'     TARGET       False\n",
      "110       110       220                    ' '     TARGET       False\n",
      "111       111      7285                  '193'     TARGET       False\n",
      "112       112        21                    '6'     TARGET       False\n",
      "113       113        13                    '.'     TARGET       False\n",
      "114       114      1102                  ' It'     TARGET       False\n",
      "115       115       374                  ' is'     TARGET       False\n",
      "\n",
      "Token count by section:\n",
      "PREFIX: 75 tokens\n",
      "USER_TEXT: 12 tokens\n",
      "SUFFIX: 14 tokens\n",
      "TARGET: 15 tokens\n",
      "TOTAL: 116 tokens\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Print the token information as a plain text table\n",
    "def print_token_table(sample_idx=0):\n",
    "    # Get components\n",
    "    prefix_tokens = tokenizer(PROMPT_PREFIX, return_tensors=\"pt\", add_special_tokens=False).input_ids[0]\n",
    "    user_text_tokens = tokenizer(text_input.value, return_tensors=\"pt\", add_special_tokens=False).input_ids[0]\n",
    "    suffix_tokens = tokenizer(PROMPT_SUFFIX, return_tensors=\"pt\", add_special_tokens=False).input_ids[0]\n",
    "    target_tokens = target_generated_tokens[sample_idx]\n",
    "    \n",
    "    # Combine all tokens\n",
    "    all_tokens = torch.cat([prefix_tokens, user_text_tokens, suffix_tokens, target_tokens])\n",
    "    \n",
    "    # Create rows for the table\n",
    "    rows = []\n",
    "    for i, id in enumerate(all_tokens):\n",
    "        token_str = tokenizer.decode([id.item()])\n",
    "        section = \"PREFIX\" if i < len(prefix_tokens) else \\\n",
    "                 \"USER_TEXT\" if i < len(prefix_tokens) + len(user_text_tokens) else \\\n",
    "                 \"SUFFIX\" if i < len(prefix_tokens) + len(user_text_tokens) + len(suffix_tokens) else \\\n",
    "                 \"TARGET\"\n",
    "        rows.append({\n",
    "            \"position\": i,\n",
    "            \"token_id\": id.item(),\n",
    "            \"token_str\": repr(token_str),  # Use repr to show whitespace\n",
    "            \"section\": section,\n",
    "            \"is_special\": id.item() in tokenizer.all_special_ids\n",
    "        })\n",
    "    \n",
    "    # Convert to dataframe and print as string\n",
    "    df = pd.DataFrame(rows)\n",
    "    print(df.to_string())\n",
    "    \n",
    "    # Also print section counts for quick reference\n",
    "    print(\"\\nToken count by section:\")\n",
    "    print(f\"PREFIX: {len(prefix_tokens)} tokens\")\n",
    "    print(f\"USER_TEXT: {len(user_text_tokens)} tokens\")\n",
    "    print(f\"SUFFIX: {len(suffix_tokens)} tokens\")\n",
    "    print(f\"TARGET: {len(target_tokens)} tokens\")\n",
    "    print(f\"TOTAL: {len(all_tokens)} tokens\")\n",
    "\n",
    "# Execute the function to print the table\n",
    "print_token_table(current_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
