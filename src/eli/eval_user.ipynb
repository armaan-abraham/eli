{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "import numpy as np\n",
    "import torch\n",
    "from IPython.display import clear_output, display\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from eli.config import cfg, encoder_cfg\n",
    "from eli.encoder import (\n",
    "    PROMPT_PREFIX,\n",
    "    PROMPT_SUFFIX,\n",
    "    Encoder,\n",
    "    EncoderDecoder,\n",
    "    EncoderTrainer,\n",
    "    calculate_dinalar_loss,\n",
    "    get_embeddings_from_decoder,\n",
    "    kl_div,\n",
    ")\n",
    "\n",
    "cfg.buffer_size_samples = cfg.target_model_batch_size_samples = cfg.train_batch_size_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(cfg.target_model_name).to(cfg.device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg.target_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing data collector\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:target_generated_tokens size: 480 bytes (0.00 MB)\n",
      "INFO:root:target_logits size: 65667072 bytes (62.62 MB)\n",
      "INFO:root:target_acts size: 65536 bytes (0.06 MB)\n",
      "INFO:root:input_tokens size: 2048 bytes (0.00 MB)\n",
      "INFO:root:Total shared memory size: 0.06 GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b09b623c6054493819bc7a891aa0050",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d165dce7012488a91ec35be78f2ca64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Tokenize and concatenate called\n",
      "INFO:root:Full text length: 43667353\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (472110 > 131072). Running this sequence through the model will result in indexing errors\n",
      "INFO:root:Num tokens: 9321501\n",
      "/root/eli/.venv/lib/python3.12/site-packages/datasets/formatting/torch_formatter.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(value, **{**default_dtype, **self.torch_tensor_kwargs})\n",
      "INFO:root:Processing data directly on cuda without workers\n",
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n",
      "INFO:root:Processing chunk 0:8 on cuda\n",
      "INFO:root:Processing batch 0:8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model meta-llama/Llama-3.2-1B-Instruct into HookedTransformer\n",
      "Moving model to device:  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/eli/.venv/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/root/eli/.venv/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "INFO:root:CHUNK 0:8 COMPLETED, Max GPU memory allocated: 13.68 GB, Max GPU memory reserved: 13.78 GB\n",
      "INFO:root:Direct data processing completed\n"
     ]
    }
   ],
   "source": [
    "# Load eval data\n",
    "\n",
    "from eli.data import DataCollector\n",
    "\n",
    "print(\"Initializing data collector\")\n",
    "data_collector = DataCollector(use_workers=False)\n",
    "\n",
    "print(\"Collecting data\")\n",
    "data_collector.collect_data()\n",
    "\n",
    "data = data_collector.data\n",
    "\n",
    "target_generated_tokens = data[\"target_generated_tokens\"]\n",
    "target_logits = data[\"target_logits\"]\n",
    "\n",
    "batch_size = target_generated_tokens.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14d61d69a9194985bab1b14bfb92df1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "034c2cf95e734d1281b025ba4e80958c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# Create output widgets\n",
    "context_output = widgets.Output()\n",
    "button_area = widgets.Output()\n",
    "\n",
    "# Create sample counter\n",
    "current_sample = 0\n",
    "\n",
    "PROMPT_PREFIX = \"\"\"\n",
    "User: Your task is to predict what another LLM will say, given the following\n",
    "description of what the LLM is currently thinking: \\\" \n",
    "\"\"\"\n",
    "\n",
    "PROMPT_SUFFIX = \"\"\"\\\". Provide your prediction and nothing else.\n",
    "Assistant:\n",
    "\"\"\"\n",
    "\n",
    "def assemble_decoder_context_without_virtual(text, sample_idx=0):\n",
    "    \"\"\"Assemble decoder context using only user-provided text (no virtual embeddings)\"\"\"\n",
    "    with torch.no_grad():\n",
    "        with torch.autocast(device_type=cfg.device.type, dtype=cfg.dtype):\n",
    "            # Get a sample from the data\n",
    "            sample_tokens = target_generated_tokens[sample_idx:sample_idx+1].to(cfg.device)\n",
    "            sample_logits = target_logits[sample_idx:sample_idx+1].to(cfg.device, dtype=torch.float32)\n",
    "            \n",
    "            # Tokenize the user text\n",
    "            user_text_tokens = tokenizer(text, return_tensors=\"pt\", add_special_tokens=False).input_ids.to(cfg.device)\n",
    "            \n",
    "            # Generate tokens for prompt components\n",
    "            prefix_tokens = tokenizer(PROMPT_PREFIX, return_tensors=\"pt\").input_ids.to(cfg.device)\n",
    "            suffix_tokens = tokenizer(PROMPT_SUFFIX, return_tensors=\"pt\", add_special_tokens=False).input_ids.to(cfg.device)\n",
    "            \n",
    "            # Repeat for batch size\n",
    "            batch_size = sample_tokens.shape[0]\n",
    "            prefix_tokens = prefix_tokens.repeat(batch_size, 1)\n",
    "            suffix_tokens = suffix_tokens.repeat(batch_size, 1)\n",
    "            user_text_tokens = user_text_tokens.repeat(batch_size, 1)\n",
    "            \n",
    "            # Concatenate all tokens to create the full context\n",
    "            # Format: [prefix] + [user_text] + [suffix] + [target_tokens]\n",
    "            input_tokens = torch.cat([\n",
    "                prefix_tokens, \n",
    "                user_text_tokens, \n",
    "                suffix_tokens, \n",
    "                sample_tokens\n",
    "            ], dim=1)\n",
    "            \n",
    "            # Get embeddings from decoder\n",
    "            embeddings = model.get_input_embeddings()\n",
    "            input_embeds = embeddings(input_tokens)\n",
    "            \n",
    "            # Create attention mask (all 1s since we're not using padding)\n",
    "            attention_mask = torch.ones(\n",
    "                input_embeds.shape[0],\n",
    "                input_embeds.shape[1],\n",
    "                device=input_embeds.device,\n",
    "            )\n",
    "            \n",
    "            # Get decoder output\n",
    "            decoder_output = model(\n",
    "                input_ids=input_tokens, \n",
    "                # attention_mask=attention_mask\n",
    "            )\n",
    "            \n",
    "            # Extract target logits for prediction loss calculation\n",
    "            decoder_logits_target = decoder_output.logits[:, -cfg.decoder_pred_len_toks:, :]\n",
    "            \n",
    "            # Calculate prediction loss\n",
    "            prediction_loss = kl_div(decoder_logits_target, sample_logits).item()\n",
    "            \n",
    "            # Decode tokens for display\n",
    "            context_decoded = tokenizer.decode(input_tokens[0])\n",
    "            target_decoded = tokenizer.decode(sample_tokens[0])\n",
    "            \n",
    "            # Return results for display\n",
    "            return {\n",
    "                \"input_tokens\": input_tokens,\n",
    "                \"context_decoded\": context_decoded,\n",
    "                \"target_decoded\": target_decoded,\n",
    "                \"prediction_loss\": prediction_loss,\n",
    "                \"sample_tokens\": sample_tokens,\n",
    "                \"prefix_len\": prefix_tokens.shape[1],\n",
    "                \"user_text_len\": user_text_tokens.shape[1],\n",
    "                \"suffix_len\": suffix_tokens.shape[1],\n",
    "            }\n",
    "\n",
    "def display_context_and_results(results):\n",
    "    with context_output:\n",
    "        context_output.clear_output(wait=True)\n",
    "        \n",
    "        print(f\"Sample {current_sample+1}/{batch_size}\")\n",
    "        print(f\"User text: \\\"{text_input.value}\\\"\")\n",
    "        print(f\"Prediction loss: {results['prediction_loss']:.6f}\")\n",
    "        \n",
    "        # Display token lengths\n",
    "        print(\"\\nContext Structure:\")\n",
    "        print(f\"Prefix tokens: {results['prefix_len']} tokens\")\n",
    "        print(f\"User text tokens: {results['user_text_len']} tokens\")\n",
    "        print(f\"Suffix tokens: {results['suffix_len']} tokens\")\n",
    "        print(f\"Target tokens: {results['sample_tokens'].shape[1]} tokens\")\n",
    "        print(f\"Total tokens: {results['input_tokens'].shape[1]} tokens\")\n",
    "        \n",
    "        # Display the entire assembled context\n",
    "        print(\"\\nAssembled Context (decoded):\")\n",
    "        print(\"-\" * 80)\n",
    "        print(results[\"context_decoded\"])\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Also show just the target part\n",
    "        print(\"\\nTarget tokens (what the model should predict):\")\n",
    "        print(results[\"target_decoded\"])\n",
    "\n",
    "# Create widgets\n",
    "text_input = widgets.Textarea(\n",
    "    value=\"The model is thinking about the relationship between cause and effect.\",\n",
    "    placeholder=\"Enter text to use as context...\",\n",
    "    description=\"Input Text:\",\n",
    "    disabled=False,\n",
    "    layout=widgets.Layout(width=\"100%\", height=\"100px\")\n",
    ")\n",
    "\n",
    "run_button = widgets.Button(\n",
    "    description='Run Test',\n",
    "    disabled=False,\n",
    "    button_style='primary', \n",
    "    tooltip='Run the test with the provided text'\n",
    ")\n",
    "\n",
    "next_button = widgets.Button(\n",
    "    description='Next Sample',\n",
    "    disabled=False,\n",
    "    button_style='info',\n",
    "    tooltip='Move to the next sample'\n",
    ")\n",
    "\n",
    "def on_run_button_clicked(b):\n",
    "    with context_output:\n",
    "        print(\"Running test...\")\n",
    "    results = assemble_decoder_context_without_virtual(text_input.value, current_sample)\n",
    "    display_context_and_results(results)\n",
    "\n",
    "def on_next_button_clicked(b):\n",
    "    global current_sample\n",
    "    if current_sample < batch_size - 1:\n",
    "        current_sample += 1\n",
    "        results = assemble_decoder_context_without_virtual(text_input.value, current_sample)\n",
    "        display_context_and_results(results)\n",
    "    else:\n",
    "        with context_output:\n",
    "            context_output.clear_output(wait=True)\n",
    "            print(\"End of batch reached!\")\n",
    "\n",
    "# Connect the buttons to handlers\n",
    "run_button.on_click(on_run_button_clicked)\n",
    "next_button.on_click(on_next_button_clicked)\n",
    "\n",
    "# Layout\n",
    "controls = widgets.VBox([text_input, widgets.HBox([run_button, next_button])])\n",
    "with button_area:\n",
    "    display(controls)\n",
    "    \n",
    "display(button_area)\n",
    "display(context_output)\n",
    "\n",
    "# Run initial test with default text\n",
    "if batch_size > 0:\n",
    "    results = assemble_decoder_context_without_virtual(text_input.value, current_sample)\n",
    "    display_context_and_results(results)\n",
    "else:\n",
    "    with context_output:\n",
    "        print(\"No samples in batch!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
