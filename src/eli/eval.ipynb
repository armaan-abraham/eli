{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "import numpy as np\n",
    "import torch\n",
    "from IPython.display import clear_output, display\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import importlib\n",
    "\n",
    "import eli.encoder\n",
    "\n",
    "importlib.reload(eli.encoder)\n",
    "from eli.config import cfg, encoder_cfg\n",
    "from eli.encoder import (\n",
    "    PROMPT_DECODER,\n",
    "    Encoder,\n",
    "    EncoderDecoder,\n",
    "    EncoderTrainer,\n",
    "    get_embeddings_from_decoder,\n",
    "    calculate_target_prediction_loss,\n",
    ")\n",
    "from einops import einsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load encoder and decoder\n",
    "\n",
    "cfg.buffer_size_samples = cfg.target_model_batch_size_samples = cfg.train_batch_size_samples\n",
    "cfg.site = \"resid_pre\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg.decoder_model_name)\n",
    "\n",
    "encoder_decoder = EncoderDecoder(cfg, encoder_cfg, tokenizer).to(cfg.device)\n",
    "\n",
    "encoder = Encoder(cfg, encoder_cfg).to(cfg.device)\n",
    "\n",
    "encoder_path = \"saved_models/encoder-14m-2toks.pt\"\n",
    "encoder.load_state_dict(torch.load(encoder_path))\n",
    "\n",
    "encoder_decoder.encoder = encoder\n",
    "\n",
    "encoder_decoder = encoder_decoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(cfg.target_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing data collector\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:target_generated_tokens size: 2048 bytes (0.00 MB)\n",
      "INFO:root:target_acts size: 524288 bytes (0.50 MB)\n",
      "INFO:root:input_tokens size: 65536 bytes (0.06 MB)\n",
      "INFO:root:Total shared memory size: 0.00 GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21c29bcfefd94ad7a4d28282fb1519d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f3ee9faeb854e309d6685e2a83a57ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Tokenize and concatenate called\n",
      "INFO:root:Full text length: 43727350\n",
      "INFO:root:Num tokens: 9672977\n",
      "/root/eli/.venv/lib/python3.12/site-packages/datasets/formatting/torch_formatter.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(value, **{**default_dtype, **self.torch_tensor_kwargs})\n",
      "INFO:root:Processing data directly on cuda without workers\n",
      "INFO:root:Processing chunk 0:256 on cuda\n",
      "INFO:root:Processing batch 0:256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model EleutherAI/pythia-70m into HookedTransformer\n",
      "Moving model to device:  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Direct data processing completed\n"
     ]
    }
   ],
   "source": [
    "# Load eval data\n",
    "\n",
    "from eli.data import DataCollector\n",
    "\n",
    "cfg.use_data_collector_workers = False\n",
    "\n",
    "print(\"Initializing data collector\")\n",
    "data_collector = DataCollector(cfg)\n",
    "\n",
    "print(\"Collecting data\")\n",
    "data_collector.collect_data()\n",
    "\n",
    "data = data_collector.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target prediction loss: 4.207995414733887\n",
      "Dinalar loss: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Print loss statistics\n",
    "target_generated_tokens = data[\"target_generated_tokens\"]\n",
    "target_acts = data[\"target_acts\"]\n",
    "\n",
    "buffer_size = target_acts.shape[0]\n",
    "batch_size = cfg.train_batch_size_samples\n",
    "num_batches = buffer_size // batch_size\n",
    "\n",
    "target_prediction_losses = []\n",
    "dinalar_losses = []\n",
    "\n",
    "for batch_idx in range(num_batches):\n",
    "    start_idx = batch_idx * batch_size\n",
    "    end_idx = start_idx + batch_size\n",
    "\n",
    "    # Extract batch data and move to device\n",
    "    batch_tokens = target_generated_tokens[start_idx:end_idx].to(cfg.device)\n",
    "    batch_acts = target_acts[start_idx:end_idx].to(cfg.device)\n",
    "\n",
    "    loss, target_prediction_loss, dinalar_loss, encoder_output_logits = EncoderTrainer.loss(\n",
    "        cfg, encoder_decoder, batch_tokens, batch_acts, -1\n",
    "    )\n",
    "\n",
    "    target_prediction_losses.append(target_prediction_loss.item())\n",
    "    dinalar_losses.append(dinalar_loss.item())\n",
    "\n",
    "print(f\"Target prediction loss: {np.mean(target_prediction_losses)}\")\n",
    "print(f\"Dinalar loss: {np.mean(dinalar_losses)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jaxtyping import Float\n",
    "from torch import Tensor\n",
    "\n",
    "def get_distances_to_embeddings(embeddings: Float[Tensor, \"batch tok d_embed\"], target_embeddings: Float[Tensor, \"vocab d_embed\"]) -> Float[Tensor, \"batch tok vocab\"]:\n",
    "    \"\"\"Computes the L2 distance between each token embedding and each target embedding.\"\"\"\n",
    "    # embeddings shape: [batch, tok, d_embed]\n",
    "    # target_embeddings shape: [vocab, d_embed]\n",
    "    \n",
    "    # Reshape embeddings to [batch*tok, d_embed]\n",
    "    batch_size, seq_len, d_embed = embeddings.shape\n",
    "    embeddings_flat = embeddings.reshape(-1, d_embed)\n",
    "    \n",
    "    # Compute pairwise distances between all embeddings and target embeddings\n",
    "    # Returns tensor of shape [batch*tok, vocab]\n",
    "    distances = torch.cdist(embeddings_flat, target_embeddings, p=2)\n",
    "    \n",
    "    # Reshape back to [batch, tok, vocab]\n",
    "    return distances.reshape(batch_size, seq_len, -1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f21c2e5e8c2c4ac5bc32dcd94b3f9109",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d80d3dc4f31461da9e1c6219ce7618c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[31034,  7412, 13850, 19247, 12693],\n",
      "        [ 4031,  8223, 31350, 27743,  9646]], device='cuda:0')\n",
      "torch.Size([2, 5])\n",
      "top_values.shape torch.Size([2, 5])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[40871, 18388, 48497, 26023, 34892],\n",
      "        [ 8814, 17051, 17444,  7130, 42472]], device='cuda:0')\n",
      "torch.Size([2, 5])\n",
      "top_values.shape torch.Size([2, 5])\n",
      "tensor([[33902,  8397, 33356, 30988, 25630],\n",
      "        [28192, 15642, 49287, 23604, 17106]], device='cuda:0')\n",
      "torch.Size([2, 5])\n",
      "top_values.shape torch.Size([2, 5])\n",
      "tensor([[37424,  9986,  3665, 36891, 11480],\n",
      "        [47988,  1145,   308, 38868,  8389]], device='cuda:0')\n",
      "torch.Size([2, 5])\n",
      "top_values.shape torch.Size([2, 5])\n",
      "tensor([[  658,   854, 45152, 46170, 23502],\n",
      "        [ 1813, 33876, 42472, 50063, 49942]], device='cuda:0')\n",
      "torch.Size([2, 5])\n",
      "top_values.shape torch.Size([2, 5])\n",
      "tensor([[30988, 46315, 18147,   760, 39168],\n",
      "        [36771, 15703, 18221,  5769,  7388]], device='cuda:0')\n",
      "torch.Size([2, 5])\n",
      "top_values.shape torch.Size([2, 5])\n",
      "tensor([[ 8962, 14278,  5533,  2103, 27211],\n",
      "        [44232, 42053, 19505,  1360,  5013]], device='cuda:0')\n",
      "torch.Size([2, 5])\n",
      "top_values.shape torch.Size([2, 5])\n",
      "tensor([[39087, 23808,  1517,  9932, 15691],\n",
      "        [24446,  1222, 22368,   708, 38579]], device='cuda:0')\n",
      "torch.Size([2, 5])\n",
      "top_values.shape torch.Size([2, 5])\n",
      "tensor([[ 8649,   143,  2982, 26490, 12875],\n",
      "        [42053, 28320, 44447, 18221, 15272]], device='cuda:0')\n",
      "torch.Size([2, 5])\n",
      "top_values.shape torch.Size([2, 5])\n",
      "tensor([[37436, 11709,  4915,  2552, 24672],\n",
      "        [ 7415, 18291, 20919, 40527, 43743]], device='cuda:0')\n",
      "torch.Size([2, 5])\n",
      "top_values.shape torch.Size([2, 5])\n",
      "tensor([[    6, 13262, 15728, 21315, 49522],\n",
      "        [47132, 33771, 44425, 10131, 34867]], device='cuda:0')\n",
      "torch.Size([2, 5])\n",
      "top_values.shape torch.Size([2, 5])\n",
      "tensor([[20233, 50026,  7412, 25379, 47919],\n",
      "        [49275, 36291, 12899,   180,  4409]], device='cuda:0')\n",
      "torch.Size([2, 5])\n",
      "top_values.shape torch.Size([2, 5])\n",
      "tensor([[ 5527, 23589,  6047,  4915,  1960],\n",
      "        [14010, 23673, 29898, 27180, 46865]], device='cuda:0')\n",
      "torch.Size([2, 5])\n",
      "top_values.shape torch.Size([2, 5])\n",
      "tensor([[37299, 50186,  7583,  2853, 20255],\n",
      "        [25932,  5225, 46584, 18342, 26017]], device='cuda:0')\n",
      "torch.Size([2, 5])\n",
      "top_values.shape torch.Size([2, 5])\n"
     ]
    }
   ],
   "source": [
    "# Create output widgets for displaying sample information\n",
    "sample_output = widgets.Output()\n",
    "button_output = widgets.Output()\n",
    "\n",
    "# Create a counter and button\n",
    "current_sample = 0\n",
    "\n",
    "\n",
    "def on_button_click(b):\n",
    "    global current_sample\n",
    "    if current_sample < batch_size:\n",
    "        display_sample(current_sample)\n",
    "        current_sample += 1\n",
    "    else:\n",
    "        with sample_output:\n",
    "            print(\"End of batch reached!\")\n",
    "\n",
    "\n",
    "def create_table(title, headers, rows, col_widths=None):\n",
    "    \"\"\"Helper function to create formatted tables\n",
    "\n",
    "    Args:\n",
    "        title: Table title string\n",
    "        headers: List of header strings\n",
    "        rows: List of rows, where each row is a list of values\n",
    "        col_widths: List of column widths (defaults to 15 for all columns)\n",
    "\n",
    "    Returns:\n",
    "        Formatted table string\n",
    "    \"\"\"\n",
    "    if col_widths is None:\n",
    "        col_widths = [15] * len(headers)\n",
    "\n",
    "    # Ensure first column width accommodates row labels\n",
    "    col_widths[0] = max(col_widths[0], 8)\n",
    "\n",
    "    # Create table string\n",
    "    table = f\"{title}\\n\"\n",
    "\n",
    "    # Create header\n",
    "    header_row = headers[0].ljust(col_widths[0])\n",
    "    for i, header in enumerate(headers[1:], 1):\n",
    "        header_row += header.ljust(col_widths[i])\n",
    "    table += header_row + \"\\n\"\n",
    "\n",
    "    # Add separator\n",
    "    table += \"-\" * len(header_row) + \"\\n\"\n",
    "\n",
    "    # Add rows\n",
    "    for row in rows:\n",
    "        row_str = str(row[0]).ljust(col_widths[0])\n",
    "        for i, cell in enumerate(row[1:], 1):\n",
    "            row_str += str(cell).ljust(col_widths[i])\n",
    "        table += row_str + \"\\n\"\n",
    "\n",
    "    return table\n",
    "\n",
    "\n",
    "def shuffle_data(tokens, acts, seed=None):\n",
    "    \"\"\"\n",
    "    Shuffle the tokens and acts tensors in the same order.\n",
    "    \n",
    "    Args:\n",
    "        tokens: Tensor of token IDs\n",
    "        acts: Tensor of activations\n",
    "        seed: Optional random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (shuffled_tokens, shuffled_acts)\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "    \n",
    "    # Get the number of samples\n",
    "    num_samples = tokens.size(0)\n",
    "    \n",
    "    # Generate random permutation indices\n",
    "    indices = torch.randperm(num_samples)\n",
    "    \n",
    "    # Shuffle both tensors using the same indices\n",
    "    shuffled_tokens = tokens[indices]\n",
    "    shuffled_acts = acts[indices]\n",
    "    \n",
    "    return shuffled_tokens, shuffled_acts\n",
    "\n",
    "\n",
    "\n",
    "# Function to display a single sample\n",
    "def display_sample(sample_idx):\n",
    "    with torch.no_grad():\n",
    "        with torch.autocast(device_type=cfg.device.type, dtype=cfg.dtype):\n",
    "            # Extract single sample as a \"batch\" of size 1\n",
    "            sample_tokens = target_generated_tokens[sample_idx : sample_idx + 1].to(\n",
    "                cfg.device\n",
    "            )\n",
    "            sample_acts = target_acts[sample_idx : sample_idx + 1].to(cfg.device)\n",
    "\n",
    "\n",
    "            # encoder = encoder_decoder.encoder\n",
    "            # encoder_output_logits = encoder(sample_acts) # [batch tok vocab]\n",
    "\n",
    "            # Convert logits to one-hot-like by making the max value very large and others small\n",
    "            # max_values, max_indices = torch.max(encoder_output_logits, dim=-1, keepdim=True)\n",
    "            # one_hot_logits = torch.ones_like(encoder_output_logits) * -100.0  # Set all values to a small number\n",
    "            # one_hot_logits.scatter_(dim=-1, index=max_indices, value=100.0)   # Set max values to a large number\n",
    "            # encoder_output_logits = one_hot_logits\n",
    "\n",
    "            # Get model outputs for this single sample\n",
    "            (decoder_logits_target, decoder_logits_encoding, virtual_embs, encoder_output_logits) = (\n",
    "                encoder_decoder(sample_acts, sample_tokens, train_iter=-1)\n",
    "            )\n",
    "\n",
    "            # Calculate losses using existing functions\n",
    "            pred_loss = calculate_target_prediction_loss(decoder_logits_target, sample_tokens).item()\n",
    "            # din_loss = calculate_dinalar_loss(\n",
    "            #     decoder_logits_encoding,\n",
    "            #     encoder_output_logits,\n",
    "            # ).item()\n",
    "\n",
    "            # embeddings = get_embeddings_from_decoder(encoder_decoder.decoder).weight\n",
    "            # distances = get_distances_to_embeddings(virtual_embs, embeddings) # [batch tok vocab]\n",
    "\n",
    "            # print(\"distances.shape\", distances.shape)\n",
    "\n",
    "            encoder_output_probs = torch.softmax(encoder_output_logits, dim=-1)\n",
    "\n",
    "            # Get top 3 tokens by encoder output logits\n",
    "            top_k = 5\n",
    "            top_values, top_indices = torch.topk(\n",
    "                encoder_output_probs[0], k=top_k, dim=-1\n",
    "            )  # [batch tok]\n",
    "\n",
    "            print(top_indices)\n",
    "            print(top_indices.shape)\n",
    "\n",
    "            print(\"top_values.shape\", top_values.shape)\n",
    "\n",
    "            # Decode tokens for display\n",
    "            sample_decoded = tokenizer.decode(sample_tokens[0])\n",
    "\n",
    "            # Display results\n",
    "            with sample_output:\n",
    "                sample_output.clear_output(wait=True)\n",
    "                print(f\"Sample {sample_idx+1}/{batch_size}\")\n",
    "                print(f\"Target prediction loss: {pred_loss:.6f}\")\n",
    "                print(\"\\nTarget tokens:\")\n",
    "                print(sample_decoded, \"\\n\")\n",
    "\n",
    "\n",
    "                prompt_prefix, prompt_suffix = PROMPT_DECODER.split(\"<thought>\")\n",
    "                # Also decode and display the prefix and suffix tokens\n",
    "                prefix_tokens = tokenizer(prompt_prefix, return_tensors=\"pt\").input_ids[\n",
    "                    0\n",
    "                ]\n",
    "                suffix_tokens = tokenizer(prompt_suffix, return_tensors=\"pt\").input_ids[\n",
    "                    0\n",
    "                ]\n",
    "\n",
    "                print(tokenizer.decode(prefix_tokens))\n",
    "\n",
    "                # Prepare data for the tokens table\n",
    "                col_width = 15\n",
    "                headers = [\"Token\"] + [f\"Emb {i}\" for i in range(virtual_embs.shape[1])]\n",
    "\n",
    "                token_rows = []\n",
    "                for k in range(top_k):\n",
    "                    row = [f\"Top {k+1}:\"]\n",
    "                    for j in range(virtual_embs.shape[1]):\n",
    "                        token_id = top_indices[j, k].item()\n",
    "                        token_text = tokenizer.decode([token_id])\n",
    "                        # Replace newlines and tabs for cleaner display\n",
    "                        token_text = token_text.replace(\"\\n\", \"\\\\n\").replace(\n",
    "                            \"\\t\", \"\\\\t\"\n",
    "                        )\n",
    "                        # Truncate to fit in column\n",
    "                        token_display = token_text[: col_width - 2]\n",
    "                        row.append(token_display)\n",
    "                    token_rows.append(row)\n",
    "\n",
    "                # Create and display the token table\n",
    "                token_table = create_table(\n",
    "                    \"\", headers, token_rows, [8] + [col_width] * virtual_embs.shape[1]\n",
    "                )\n",
    "                print(token_table)\n",
    "\n",
    "                # Create table for logit values\n",
    "                logit_rows = []\n",
    "                for k in range(top_k):\n",
    "                    row = [f\"Top {k+1}:\"]\n",
    "                    for j in range(virtual_embs.shape[1]):\n",
    "                        # Format logit value with 5 decimal places\n",
    "                        prob = top_values[j, k].item()\n",
    "                        row.append(f\"{prob:.5f}\")\n",
    "                    logit_rows.append(row)\n",
    "\n",
    "                # Create and display the logits table\n",
    "                logit_table = create_table(\n",
    "                    \"Logit Values:\",\n",
    "                    headers,\n",
    "                    logit_rows,\n",
    "                    [8] + [col_width] * virtual_embs.shape[1],\n",
    "                )\n",
    "                print(logit_table)\n",
    "\n",
    "                print(tokenizer.decode(suffix_tokens))\n",
    "\n",
    "\n",
    "# Interactive sample investigation\n",
    "next_button = widgets.Button(description=\"Next Sample\")\n",
    "next_button.on_click(on_button_click)\n",
    "\n",
    "# Display the button and sample output in separate areas\n",
    "with button_output:\n",
    "    display(next_button)\n",
    "display(button_output)\n",
    "display(sample_output)\n",
    "\n",
    "# Show the first sample\n",
    "if batch_size > 0:\n",
    "    display_sample(current_sample)\n",
    "    current_sample += 1\n",
    "else:\n",
    "    with sample_output:\n",
    "        print(\"No samples in batch!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = \"Bob\"\n",
    "# tokens = tokenizer(text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "# Print each token separately\n",
    "# print(\"Tokens for text:\", text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[44484,   474,   330,   672,  1559,   373,   257,   922,  1048,    13,\n",
      "         14862,   220]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "tensor([[44484,   474,   330,   672,  1559,   373,   257,   922,  1048,    13,\n",
      "         14862,   220]])\n",
      "torch.Size([50257])\n",
      "Top 10 tokens by logit:\n",
      "1. Token: 'Â ', ID: 1849, Logit: -46.7644\n",
      "2. Token: 'iced', ID: 3711, Logit: -47.3270\n",
      "3. Token: 'ich', ID: 488, Logit: -48.3128\n",
      "4. Token: 'ive', ID: 425, Logit: -48.6372\n",
      "5. Token: 'irl', ID: 1901, Logit: -48.7395\n",
      "6. Token: 'ix', ID: 844, Logit: -49.0847\n",
      "7. Token: 'iz', ID: 528, Logit: -49.1540\n",
      "8. Token: 'izzy', ID: 40593, Logit: -49.2239\n",
      "9. Token: 'ike', ID: 522, Logit: -49.2902\n",
      "10. Token: '________', ID: 2602, Logit: -49.3509\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "text = \"Alice jacobson was a good person. Alice \"\n",
    "tokens = tokenizer(text, add_special_tokens=True, return_tensors=\"pt\")\n",
    "print(tokens)\n",
    "print(tokens.input_ids)\n",
    "tokens = torch.cat([torch.tensor([[tokenizer.bos_token_id]]), tokens.input_ids], dim=1)\n",
    "\n",
    "# Get model outputs\n",
    "outputs = model(tokens)\n",
    "logits = outputs.logits\n",
    "\n",
    "# Get the logits for the last token\n",
    "last_token_logits = logits[0, -1, :]\n",
    "print(last_token_logits.shape)\n",
    "\n",
    "# Get the top 3 token indices and their corresponding logits\n",
    "k = 10\n",
    "top_values, top_indices = torch.topk(last_token_logits, k)\n",
    "\n",
    "# Print the top 3 tokens and their logits\n",
    "print(f\"Top {k} tokens by logit:\")\n",
    "for i, (token_id, logit_value) in enumerate(zip(top_indices, top_values)):\n",
    "    token_text = tokenizer.decode([token_id.item()])\n",
    "    print(f\"{i+1}. Token: '{token_text}', ID: {token_id.item()}, Logit: {logit_value.item():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
