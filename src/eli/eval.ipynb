{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "import numpy as np\n",
    "import torch\n",
    "from IPython.display import clear_output, display\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from eli.config import cfg, encoder_cfg\n",
    "from eli.encoder import (\n",
    "    PROMPT_PREFIX,\n",
    "    PROMPT_SUFFIX,\n",
    "    Encoder,\n",
    "    EncoderDecoder,\n",
    "    EncoderTrainer,\n",
    "    calculate_dinalar_loss,\n",
    "    get_embeddings_from_decoder,\n",
    "    kl_div,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load encoder and decoder\n",
    "\n",
    "cfg.buffer_size_samples = cfg.target_model_batch_size_samples = cfg.train_batch_size_samples\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg.decoder_model_name)\n",
    "\n",
    "encoder_decoder = EncoderDecoder(cfg, encoder_cfg, tokenizer).to(cfg.device)\n",
    "\n",
    "encoder = Encoder(cfg, encoder_cfg).to(cfg.device)\n",
    "\n",
    "encoder_path = \"saved_models/encoder-dinalar-1e-2.pt\"\n",
    "encoder.load_state_dict(torch.load(encoder_path))\n",
    "\n",
    "encoder_decoder.encoder = encoder\n",
    "\n",
    "encoder_decoder = encoder_decoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(cfg.target_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing data collector\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:target_generated_tokens size: 480 bytes (0.00 MB)\n",
      "INFO:root:target_logits size: 65667072 bytes (62.62 MB)\n",
      "INFO:root:target_acts size: 65536 bytes (0.06 MB)\n",
      "INFO:root:input_tokens size: 2048 bytes (0.00 MB)\n",
      "INFO:root:Total shared memory size: 0.06 GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2d24264c93849f89bdb83c45e30a191",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52fb3e6b215543b69628d3ea630d2b5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Tokenize and concatenate called\n",
      "INFO:root:Full text length: 43667353\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (472110 > 131072). Running this sequence through the model will result in indexing errors\n",
      "INFO:root:Num tokens: 9321501\n",
      "/root/eli/.venv/lib/python3.12/site-packages/datasets/formatting/torch_formatter.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(value, **{**default_dtype, **self.torch_tensor_kwargs})\n",
      "INFO:root:Processing data directly on cuda without workers\n",
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n",
      "INFO:root:Processing chunk 0:8 on cuda\n",
      "INFO:root:Processing batch 0:8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model meta-llama/Llama-3.2-1B-Instruct into HookedTransformer\n",
      "Moving model to device:  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/eli/.venv/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/root/eli/.venv/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "INFO:root:CHUNK 0:8 COMPLETED, Max GPU memory allocated: 18.30 GB, Max GPU memory reserved: 18.42 GB\n",
      "INFO:root:Direct data processing completed\n"
     ]
    }
   ],
   "source": [
    "# Load eval data\n",
    "\n",
    "from eli.data import DataCollector\n",
    "\n",
    "print(\"Initializing data collector\")\n",
    "data_collector = DataCollector(use_workers=False)\n",
    "\n",
    "print(\"Collecting data\")\n",
    "data_collector.collect_data()\n",
    "\n",
    "data = data_collector.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target prediction loss: 0.499033123254776\n",
      "Dinalar loss: 0.48165082931518555\n"
     ]
    }
   ],
   "source": [
    "# Print loss statistics\n",
    "target_generated_tokens = data[\"target_generated_tokens\"]\n",
    "target_logits = data[\"target_logits\"]\n",
    "target_acts = data[\"target_acts\"]\n",
    "\n",
    "buffer_size = target_acts.shape[0]\n",
    "batch_size = cfg.train_batch_size_samples\n",
    "num_batches = buffer_size // batch_size\n",
    "\n",
    "target_prediction_losses = []\n",
    "dinalar_losses = []\n",
    "\n",
    "for batch_idx in range(num_batches):\n",
    "    start_idx = batch_idx * batch_size\n",
    "    end_idx = start_idx + batch_size\n",
    "\n",
    "    # Extract batch data and move to device\n",
    "    batch_tokens = target_generated_tokens[start_idx:end_idx].to(cfg.device)\n",
    "    batch_logits = target_logits[start_idx:end_idx].to(cfg.device, dtype=torch.float32)\n",
    "    batch_acts = target_acts[start_idx:end_idx].to(cfg.device)\n",
    "\n",
    "    loss, target_prediction_loss, dinalar_loss = EncoderTrainer.loss(\n",
    "        cfg, encoder_decoder, batch_tokens, batch_logits, batch_acts, -1\n",
    "    )\n",
    "\n",
    "    target_prediction_losses.append(target_prediction_loss.item())\n",
    "    dinalar_losses.append(dinalar_loss.item())\n",
    "\n",
    "print(f\"Target prediction loss: {np.mean(target_prediction_losses)}\")\n",
    "print(f\"Dinalar loss: {np.mean(dinalar_losses)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a54d88ea7a545fa895ca0ff42a9beca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef33e28f72404013bf6f3981aa249f50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create output widgets for displaying sample information\n",
    "sample_output = widgets.Output()\n",
    "button_output = widgets.Output()\n",
    "\n",
    "# Create a counter and button\n",
    "current_sample = 0\n",
    "\n",
    "\n",
    "def on_button_click(b):\n",
    "    global current_sample\n",
    "    if current_sample < batch_size:\n",
    "        display_sample(current_sample)\n",
    "        current_sample += 1\n",
    "    else:\n",
    "        with sample_output:\n",
    "            print(\"End of batch reached!\")\n",
    "\n",
    "\n",
    "def create_table(title, headers, rows, col_widths=None):\n",
    "    \"\"\"Helper function to create formatted tables\n",
    "\n",
    "    Args:\n",
    "        title: Table title string\n",
    "        headers: List of header strings\n",
    "        rows: List of rows, where each row is a list of values\n",
    "        col_widths: List of column widths (defaults to 15 for all columns)\n",
    "\n",
    "    Returns:\n",
    "        Formatted table string\n",
    "    \"\"\"\n",
    "    if col_widths is None:\n",
    "        col_widths = [15] * len(headers)\n",
    "\n",
    "    # Ensure first column width accommodates row labels\n",
    "    col_widths[0] = max(col_widths[0], 8)\n",
    "\n",
    "    # Create table string\n",
    "    table = f\"{title}\\n\"\n",
    "\n",
    "    # Create header\n",
    "    header_row = headers[0].ljust(col_widths[0])\n",
    "    for i, header in enumerate(headers[1:], 1):\n",
    "        header_row += header.ljust(col_widths[i])\n",
    "    table += header_row + \"\\n\"\n",
    "\n",
    "    # Add separator\n",
    "    table += \"-\" * len(header_row) + \"\\n\"\n",
    "\n",
    "    # Add rows\n",
    "    for row in rows:\n",
    "        row_str = str(row[0]).ljust(col_widths[0])\n",
    "        for i, cell in enumerate(row[1:], 1):\n",
    "            row_str += str(cell).ljust(col_widths[i])\n",
    "        table += row_str + \"\\n\"\n",
    "\n",
    "    return table\n",
    "\n",
    "\n",
    "# Function to display a single sample\n",
    "def display_sample(sample_idx):\n",
    "    with torch.no_grad():\n",
    "        with torch.autocast(device_type=cfg.device.type, dtype=cfg.dtype):\n",
    "            # Extract single sample as a \"batch\" of size 1\n",
    "            sample_tokens = target_generated_tokens[sample_idx : sample_idx + 1].to(\n",
    "                cfg.device\n",
    "            )\n",
    "            sample_logits = target_logits[sample_idx : sample_idx + 1].to(\n",
    "                cfg.device, dtype=torch.float32\n",
    "            )\n",
    "            sample_acts = target_acts[sample_idx : sample_idx + 1].to(cfg.device)\n",
    "\n",
    "            # Get model outputs for this single sample\n",
    "            (decoder_logits_target, decoder_logits_encoding, virtual_embs) = (\n",
    "                encoder_decoder(sample_acts, sample_tokens, -1)\n",
    "            )\n",
    "\n",
    "            # Calculate losses using existing functions\n",
    "            pred_loss = kl_div(decoder_logits_target, sample_logits).item()\n",
    "            din_loss = calculate_dinalar_loss(\n",
    "                decoder_logits_encoding,\n",
    "                virtual_embs,\n",
    "                encoder_decoder.decoder\n",
    "                if not isinstance(encoder_decoder, torch.nn.DataParallel)\n",
    "                else encoder_decoder.module.decoder,\n",
    "            ).item()\n",
    "\n",
    "            # Find top 3 closest tokens to each virtual embedding\n",
    "            decoder = (\n",
    "                encoder_decoder.decoder\n",
    "                if not isinstance(encoder_decoder, torch.nn.DataParallel)\n",
    "                else encoder_decoder.module.decoder\n",
    "            )\n",
    "            token_embeddings = get_embeddings_from_decoder(\n",
    "                decoder\n",
    "            ).weight  # [vocab_size, d_embed]\n",
    "\n",
    "            # Calculate L2 distances between virtual embeddings and token embeddings\n",
    "            # Reshape for broadcasting: [1, encoding_len, d_embed] and [vocab_size, 1, d_embed]\n",
    "            v_embs = virtual_embs[0].unsqueeze(0)  # [1, encoding_len, d_embed]\n",
    "            t_embs = token_embeddings.unsqueeze(1)  # [vocab_size, 1, d_embed]\n",
    "\n",
    "            # Calculate squared distances\n",
    "            distances = torch.sum(\n",
    "                (v_embs - t_embs) ** 2, dim=2\n",
    "            )  # [vocab_size, encoding_len]\n",
    "\n",
    "            # Get top 3 closest tokens for each virtual embedding\n",
    "            top_k = 3\n",
    "            top_values, top_indices = torch.topk(\n",
    "                distances, k=top_k, dim=0, largest=False\n",
    "            )\n",
    "\n",
    "            # Decode tokens for display\n",
    "            sample_decoded = tokenizer.decode(sample_tokens[0])\n",
    "\n",
    "            # Display results\n",
    "            with sample_output:\n",
    "                sample_output.clear_output(wait=True)\n",
    "                print(f\"Sample {sample_idx+1}/{batch_size}\")\n",
    "                print(f\"Target prediction loss: {pred_loss:.6f}\")\n",
    "                print(f\"Dinalar loss: {din_loss:.6f}\")\n",
    "                print(\"\\nTarget tokens:\")\n",
    "                print(sample_decoded, \"\\n\")\n",
    "\n",
    "                # Also decode and display the prefix and suffix tokens\n",
    "                prefix_tokens = tokenizer(PROMPT_PREFIX, return_tensors=\"pt\").input_ids[\n",
    "                    0\n",
    "                ]\n",
    "                suffix_tokens = tokenizer(PROMPT_SUFFIX, return_tensors=\"pt\").input_ids[\n",
    "                    0\n",
    "                ]\n",
    "\n",
    "                print(tokenizer.decode(prefix_tokens))\n",
    "\n",
    "                # Prepare data for the tokens table\n",
    "                col_width = 15\n",
    "                headers = [\"Token\"] + [f\"Emb {i}\" for i in range(virtual_embs.shape[1])]\n",
    "\n",
    "                token_rows = []\n",
    "                for k in range(top_k):\n",
    "                    row = [f\"Top {k+1}:\"]\n",
    "                    for j in range(virtual_embs.shape[1]):\n",
    "                        token_id = top_indices[k, j].item()\n",
    "                        token_text = tokenizer.decode([token_id])\n",
    "                        # Replace newlines and tabs for cleaner display\n",
    "                        token_text = token_text.replace(\"\\n\", \"\\\\n\").replace(\n",
    "                            \"\\t\", \"\\\\t\"\n",
    "                        )\n",
    "                        # Truncate to fit in column\n",
    "                        token_display = token_text[: col_width - 2]\n",
    "                        row.append(token_display)\n",
    "                    token_rows.append(row)\n",
    "\n",
    "                # Create and display the token table\n",
    "                token_table = create_table(\n",
    "                    \"\", headers, token_rows, [8] + [col_width] * virtual_embs.shape[1]\n",
    "                )\n",
    "                print(token_table)\n",
    "\n",
    "                # Create table for distances\n",
    "                distance_rows = []\n",
    "                for k in range(top_k):\n",
    "                    row = [f\"Top {k+1}:\"]\n",
    "                    for j in range(virtual_embs.shape[1]):\n",
    "                        # Format distance value with 5 decimal places\n",
    "                        distance = top_values[k, j].item()\n",
    "                        row.append(f\"{distance:.5f}\")\n",
    "                    distance_rows.append(row)\n",
    "\n",
    "                # Create and display the distances table\n",
    "                distance_table = create_table(\n",
    "                    \"L2 Distances:\",\n",
    "                    headers,\n",
    "                    distance_rows,\n",
    "                    [8] + [col_width] * virtual_embs.shape[1],\n",
    "                )\n",
    "                print(distance_table)\n",
    "\n",
    "                print(tokenizer.decode(suffix_tokens))\n",
    "\n",
    "\n",
    "# Interactive sample investigation\n",
    "next_button = widgets.Button(description=\"Next Sample\")\n",
    "next_button.on_click(on_button_click)\n",
    "\n",
    "# Display the button and sample output in separate areas\n",
    "with button_output:\n",
    "    display(next_button)\n",
    "display(button_output)\n",
    "display(sample_output)\n",
    "\n",
    "# Show the first sample\n",
    "if batch_size > 0:\n",
    "    display_sample(current_sample)\n",
    "    current_sample += 1\n",
    "else:\n",
    "    with sample_output:\n",
    "        print(\"No samples in batch!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0b2a450818245d18a174c74b77e487e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69b9ca760dad4620ab64aee001f986d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# Create output widgets\n",
    "test_output = widgets.Output()\n",
    "button_area = widgets.Output()\n",
    "\n",
    "def get_embeddings_for_text(text, tokenizer, decoder, required_len):\n",
    "    \"\"\"Convert text to token embeddings, matching required length.\"\"\"\n",
    "    # Tokenize the text\n",
    "    tokens = tokenizer(text, return_tensors=\"pt\").input_ids.to(cfg.device)\n",
    "    \n",
    "    # Get embeddings from decoder\n",
    "    embeddings = get_embeddings_from_decoder(decoder)\n",
    "    text_embeddings = embeddings(tokens)\n",
    "    \n",
    "    # Truncate or pad to match required length\n",
    "    if text_embeddings.shape[1] > required_len:\n",
    "        # Truncate\n",
    "        text_embeddings = text_embeddings[:, :required_len, :]\n",
    "    elif text_embeddings.shape[1] < required_len:\n",
    "        # Pad with zeros\n",
    "        padding = torch.zeros(\n",
    "            (text_embeddings.shape[0], \n",
    "             required_len - text_embeddings.shape[1], \n",
    "             text_embeddings.shape[2]),\n",
    "            device=text_embeddings.device\n",
    "        )\n",
    "        text_embeddings = torch.cat([text_embeddings, padding], dim=1)\n",
    "    \n",
    "    return tokens, text_embeddings\n",
    "\n",
    "def test_embedding_comparison(user_text, sample_idx=0):\n",
    "    \"\"\"Test and compare prediction loss between user text embeddings and virtual embeddings.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        with torch.autocast(device_type=cfg.device.type, dtype=cfg.dtype):\n",
    "            # Get a sample from the data\n",
    "            sample_tokens = target_generated_tokens[sample_idx:sample_idx+1].to(cfg.device)\n",
    "            sample_logits = target_logits[sample_idx:sample_idx+1].to(cfg.device, dtype=torch.float32)\n",
    "            sample_acts = target_acts[sample_idx:sample_idx+1].to(cfg.device)\n",
    "            \n",
    "            # Get embeddings for user text\n",
    "            user_tokens, user_embeddings = get_embeddings_for_text(\n",
    "                user_text, tokenizer, \n",
    "                encoder_decoder.decoder if not isinstance(encoder_decoder, torch.nn.DataParallel) \n",
    "                else encoder_decoder.module.decoder,\n",
    "                cfg.encoding_len_toks\n",
    "            )\n",
    "            \n",
    "            # Generate virtual embeddings with the encoder for comparison\n",
    "            virtual_embeddings = encoder_decoder.encoder(sample_acts)\n",
    "            \n",
    "            # Run decoder with user embeddings\n",
    "            user_context_embeddings, user_attention_mask, user_token_lens = (\n",
    "                encoder_decoder.assemble_decoder_context_embeddings(\n",
    "                    sample_tokens, user_embeddings, -1\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            user_decoder_logits = encoder_decoder.decoder(\n",
    "                inputs_embeds=user_context_embeddings, \n",
    "                attention_mask=user_attention_mask\n",
    "            ).logits\n",
    "            \n",
    "            user_decoder_logits_target = user_decoder_logits[:, -cfg.decoder_pred_len_toks:, :]\n",
    "            user_pred_loss = kl_div(user_decoder_logits_target, sample_logits).item()\n",
    "            \n",
    "            # Run decoder with virtual embeddings\n",
    "            virtual_context_embeddings, virtual_attention_mask, virtual_token_lens = (\n",
    "                encoder_decoder.assemble_decoder_context_embeddings(\n",
    "                    sample_tokens, virtual_embeddings, -1\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            virtual_decoder_logits = encoder_decoder.decoder(\n",
    "                inputs_embeds=virtual_context_embeddings, \n",
    "                attention_mask=virtual_attention_mask\n",
    "            ).logits\n",
    "            \n",
    "            virtual_decoder_logits_target = virtual_decoder_logits[:, -cfg.decoder_pred_len_toks:, :]\n",
    "            virtual_pred_loss = kl_div(virtual_decoder_logits_target, sample_logits).item()\n",
    "            \n",
    "            # Calculate decoder logits for encoding region (for closest token analysis)\n",
    "            prefix_len = user_token_lens[\"prefix_tokens_len\"]\n",
    "            user_decoder_logits_encoding = user_decoder_logits[:, \n",
    "                prefix_len:(prefix_len + cfg.encoding_len_toks), :]\n",
    "            \n",
    "            # Find closest tokens for both sets of embeddings\n",
    "            decoder = (\n",
    "                encoder_decoder.decoder\n",
    "                if not isinstance(encoder_decoder, torch.nn.DataParallel)\n",
    "                else encoder_decoder.module.decoder\n",
    "            )\n",
    "            token_embeddings = get_embeddings_from_decoder(decoder).weight  # [vocab_size, d_embed]\n",
    "            \n",
    "            # For user embeddings\n",
    "            u_embs = user_embeddings[0].unsqueeze(0)  # [1, encoding_len, d_embed]\n",
    "            t_embs = token_embeddings.unsqueeze(1)  # [vocab_size, 1, d_embed]\n",
    "            user_distances = torch.sum((u_embs - t_embs) ** 2, dim=2)  # [vocab_size, encoding_len]\n",
    "            \n",
    "            # For virtual embeddings\n",
    "            v_embs = virtual_embeddings[0].unsqueeze(0)  # [1, encoding_len, d_embed]\n",
    "            virtual_distances = torch.sum((v_embs - t_embs) ** 2, dim=2)  # [vocab_size, encoding_len]\n",
    "            \n",
    "            # Get top closest tokens\n",
    "            top_k = 3\n",
    "            user_top_values, user_top_indices = torch.topk(\n",
    "                user_distances, k=top_k, dim=0, largest=False\n",
    "            )\n",
    "            virtual_top_values, virtual_top_indices = torch.topk(\n",
    "                virtual_distances, k=top_k, dim=0, largest=False\n",
    "            )\n",
    "            \n",
    "            return {\n",
    "                \"user_text\": user_text,\n",
    "                \"user_tokens\": user_tokens,\n",
    "                \"user_embeddings\": user_embeddings,\n",
    "                \"virtual_embeddings\": virtual_embeddings,\n",
    "                \"user_pred_loss\": user_pred_loss,\n",
    "                \"virtual_pred_loss\": virtual_pred_loss,\n",
    "                \"sample_tokens\": sample_tokens,\n",
    "                \"sample_decoded\": tokenizer.decode(sample_tokens[0]),\n",
    "                \"user_top_indices\": user_top_indices,\n",
    "                \"user_top_values\": user_top_values,\n",
    "                \"virtual_top_indices\": virtual_top_indices,\n",
    "                \"virtual_top_values\": virtual_top_values,\n",
    "            }\n",
    "\n",
    "# Create widgets\n",
    "text_input = widgets.Textarea(\n",
    "    value=\"The model is thinking about the relationship between cause and effect.\",\n",
    "    placeholder=\"Enter text to embed...\",\n",
    "    description=\"Input Text:\",\n",
    "    disabled=False,\n",
    "    layout=widgets.Layout(width=\"100%\", height=\"100px\")\n",
    ")\n",
    "\n",
    "# Create a counter for the current sample\n",
    "current_sample = 0\n",
    "\n",
    "run_button = widgets.Button(\n",
    "    description='Run Test',\n",
    "    disabled=False,\n",
    "    button_style='primary', \n",
    "    tooltip='Run the test with the provided text'\n",
    ")\n",
    "\n",
    "next_button = widgets.Button(\n",
    "    description='Next Sample',\n",
    "    disabled=False,\n",
    "    button_style='info',\n",
    "    tooltip='Move to the next sample'\n",
    ")\n",
    "\n",
    "# Function to display results\n",
    "def display_test_results(results):\n",
    "    with test_output:\n",
    "        test_output.clear_output(wait=True)\n",
    "        print(f\"Sample {current_sample+1}/{batch_size}\")\n",
    "        print(f\"User text: \\\"{results['user_text']}\\\"\")\n",
    "        print(f\"User embeddings prediction loss: {results['user_pred_loss']:.6f}\")\n",
    "        print(f\"Virtual embeddings prediction loss: {results['virtual_pred_loss']:.6f}\")\n",
    "        print(f\"\\nTarget sample text:\")\n",
    "        print(results['sample_decoded'])\n",
    "        \n",
    "        # Re-use the create_table function from the previous cell\n",
    "        # Display closest tokens for user embeddings\n",
    "        col_width = 15\n",
    "        headers = [\"Token\"] + [f\"Emb {i}\" for i in range(results['user_embeddings'].shape[1])]\n",
    "        \n",
    "        print(\"\\nClosest tokens to USER embeddings:\")\n",
    "        user_token_rows = []\n",
    "        for k in range(3):  # Top 3\n",
    "            row = [f\"Top {k+1}:\"]\n",
    "            for j in range(results['user_embeddings'].shape[1]):\n",
    "                token_id = results['user_top_indices'][k, j].item()\n",
    "                token_text = tokenizer.decode([token_id])\n",
    "                token_text = token_text.replace(\"\\n\", \"\\\\n\").replace(\"\\t\", \"\\\\t\")\n",
    "                token_display = token_text[:col_width-2]\n",
    "                row.append(token_display)\n",
    "            user_token_rows.append(row)\n",
    "        \n",
    "        user_token_table = create_table(\n",
    "            \"\", headers, user_token_rows, [8] + [col_width] * results['user_embeddings'].shape[1]\n",
    "        )\n",
    "        print(user_token_table)\n",
    "        \n",
    "        print(\"\\nClosest tokens to VIRTUAL embeddings:\")\n",
    "        virtual_token_rows = []\n",
    "        for k in range(3):  # Top 3\n",
    "            row = [f\"Top {k+1}:\"]\n",
    "            for j in range(results['virtual_embeddings'].shape[1]):\n",
    "                token_id = results['virtual_top_indices'][k, j].item()\n",
    "                token_text = tokenizer.decode([token_id])\n",
    "                token_text = token_text.replace(\"\\n\", \"\\\\n\").replace(\"\\t\", \"\\\\t\")\n",
    "                token_display = token_text[:col_width-2]\n",
    "                row.append(token_display)\n",
    "            virtual_token_rows.append(row)\n",
    "        \n",
    "        virtual_token_table = create_table(\n",
    "            \"\", headers, virtual_token_rows, [8] + [col_width] * results['virtual_embeddings'].shape[1]\n",
    "        )\n",
    "        print(virtual_token_table)\n",
    "        \n",
    "        # Distance tables\n",
    "        print(\"\\nL2 Distances for USER embeddings:\")\n",
    "        user_distance_rows = []\n",
    "        for k in range(3):\n",
    "            row = [f\"Top {k+1}:\"]\n",
    "            for j in range(results['user_embeddings'].shape[1]):\n",
    "                distance = results['user_top_values'][k, j].item()\n",
    "                row.append(f\"{distance:.5f}\")\n",
    "            user_distance_rows.append(row)\n",
    "        \n",
    "        user_distance_table = create_table(\n",
    "            \"\", headers, user_distance_rows, [8] + [col_width] * results['user_embeddings'].shape[1]\n",
    "        )\n",
    "        print(user_distance_table)\n",
    "        \n",
    "        print(\"\\nL2 Distances for VIRTUAL embeddings:\")\n",
    "        virtual_distance_rows = []\n",
    "        for k in range(3):\n",
    "            row = [f\"Top {k+1}:\"]\n",
    "            for j in range(results['virtual_embeddings'].shape[1]):\n",
    "                distance = results['virtual_top_values'][k, j].item()\n",
    "                row.append(f\"{distance:.5f}\")\n",
    "            virtual_distance_rows.append(row)\n",
    "        \n",
    "        virtual_distance_table = create_table(\n",
    "            \"\", headers, virtual_distance_rows, [8] + [col_width] * results['virtual_embeddings'].shape[1]\n",
    "        )\n",
    "        print(virtual_distance_table)\n",
    "\n",
    "# Define button click handlers\n",
    "def on_run_button_clicked(b):\n",
    "    with test_output:\n",
    "        print(\"Running test...\")\n",
    "    results = test_embedding_comparison(text_input.value, current_sample)\n",
    "    display_test_results(results)\n",
    "\n",
    "def on_next_button_clicked(b):\n",
    "    global current_sample\n",
    "    if current_sample < batch_size - 1:\n",
    "        current_sample += 1\n",
    "        results = test_embedding_comparison(text_input.value, current_sample)\n",
    "        display_test_results(results)\n",
    "    else:\n",
    "        with test_output:\n",
    "            test_output.clear_output(wait=True)\n",
    "            print(\"End of batch reached!\")\n",
    "\n",
    "# Connect the buttons to handlers\n",
    "run_button.on_click(on_run_button_clicked)\n",
    "next_button.on_click(on_next_button_clicked)\n",
    "\n",
    "# Layout\n",
    "controls = widgets.VBox([text_input, widgets.HBox([run_button, next_button])])\n",
    "with button_area:\n",
    "    display(controls)\n",
    "    \n",
    "display(button_area)\n",
    "display(test_output)\n",
    "\n",
    "# Run initial test with default text\n",
    "if batch_size > 0:\n",
    "    results = test_embedding_comparison(text_input.value, current_sample)\n",
    "    display_test_results(results)\n",
    "else:\n",
    "    with test_output:\n",
    "        print(\"No samples in batch!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efabcc5cd9f5468ca333a70fe679ab20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2820eff1c241403881fa3b7161c281c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# Create output widgets\n",
    "context_output = widgets.Output()\n",
    "button_area = widgets.Output()\n",
    "\n",
    "# Create sample counter\n",
    "current_sample = 0\n",
    "\n",
    "PROMPT_PREFIX = \"\"\"\n",
    "User: Your task is to predict what another LLM will say, given the following\n",
    "description of what the LLM is currently thinking: \\\" \n",
    "\"\"\"\n",
    "\n",
    "PROMPT_SUFFIX = \"\"\"\\\". Provide your prediction and nothing else.\n",
    "Assistant:\n",
    "\"\"\"\n",
    "\n",
    "def assemble_decoder_context_without_virtual(text, sample_idx=0):\n",
    "    \"\"\"Assemble decoder context using only user-provided text (no virtual embeddings)\"\"\"\n",
    "    with torch.no_grad():\n",
    "        with torch.autocast(device_type=cfg.device.type, dtype=cfg.dtype):\n",
    "            # Get a sample from the data\n",
    "            sample_tokens = target_generated_tokens[sample_idx:sample_idx+1].to(cfg.device)\n",
    "            sample_logits = target_logits[sample_idx:sample_idx+1].to(cfg.device, dtype=torch.float32)\n",
    "            \n",
    "            # Tokenize the user text\n",
    "            user_text_tokens = tokenizer(text, return_tensors=\"pt\").input_ids.to(cfg.device)\n",
    "            \n",
    "            # Generate tokens for prompt components\n",
    "            prefix_tokens = tokenizer(PROMPT_PREFIX, return_tensors=\"pt\").input_ids.to(cfg.device)\n",
    "            suffix_tokens = tokenizer(PROMPT_SUFFIX, return_tensors=\"pt\").input_ids.to(cfg.device)\n",
    "            \n",
    "            # Repeat for batch size\n",
    "            batch_size = sample_tokens.shape[0]\n",
    "            prefix_tokens = prefix_tokens.repeat(batch_size, 1)\n",
    "            suffix_tokens = suffix_tokens.repeat(batch_size, 1)\n",
    "            user_text_tokens = user_text_tokens.repeat(batch_size, 1)\n",
    "            \n",
    "            # Concatenate all tokens to create the full context\n",
    "            # Format: [prefix] + [user_text] + [suffix] + [target_tokens]\n",
    "            input_tokens = torch.cat([\n",
    "                prefix_tokens, \n",
    "                user_text_tokens, \n",
    "                suffix_tokens, \n",
    "                sample_tokens\n",
    "            ], dim=1)\n",
    "            \n",
    "            # Get embeddings from decoder\n",
    "            embeddings = get_embeddings_from_decoder(encoder_decoder.decoder)\n",
    "            input_embeds = embeddings(input_tokens)\n",
    "            \n",
    "            # Create attention mask (all 1s since we're not using padding)\n",
    "            attention_mask = torch.ones(\n",
    "                input_embeds.shape[0],\n",
    "                input_embeds.shape[1],\n",
    "                device=input_embeds.device,\n",
    "            )\n",
    "            \n",
    "            # Get decoder output\n",
    "            decoder_output = encoder_decoder.decoder(\n",
    "                input_ids=input_tokens, \n",
    "                # attention_mask=attention_mask\n",
    "            )\n",
    "            \n",
    "            # Extract target logits for prediction loss calculation\n",
    "            decoder_logits_target = decoder_output.logits[:, -cfg.decoder_pred_len_toks:, :]\n",
    "            \n",
    "            # Calculate prediction loss\n",
    "            prediction_loss = kl_div(decoder_logits_target, sample_logits).item()\n",
    "            \n",
    "            # Decode tokens for display\n",
    "            context_decoded = tokenizer.decode(input_tokens[0])\n",
    "            target_decoded = tokenizer.decode(sample_tokens[0])\n",
    "            \n",
    "            # Return results for display\n",
    "            return {\n",
    "                \"input_tokens\": input_tokens,\n",
    "                \"context_decoded\": context_decoded,\n",
    "                \"target_decoded\": target_decoded,\n",
    "                \"prediction_loss\": prediction_loss,\n",
    "                \"sample_tokens\": sample_tokens,\n",
    "                \"prefix_len\": prefix_tokens.shape[1],\n",
    "                \"user_text_len\": user_text_tokens.shape[1],\n",
    "                \"suffix_len\": suffix_tokens.shape[1],\n",
    "            }\n",
    "\n",
    "def display_context_and_results(results):\n",
    "    with context_output:\n",
    "        context_output.clear_output(wait=True)\n",
    "        \n",
    "        print(f\"Sample {current_sample+1}/{batch_size}\")\n",
    "        print(f\"User text: \\\"{text_input.value}\\\"\")\n",
    "        print(f\"Prediction loss: {results['prediction_loss']:.6f}\")\n",
    "        \n",
    "        # Display token lengths\n",
    "        print(\"\\nContext Structure:\")\n",
    "        print(f\"Prefix tokens: {results['prefix_len']} tokens\")\n",
    "        print(f\"User text tokens: {results['user_text_len']} tokens\")\n",
    "        print(f\"Suffix tokens: {results['suffix_len']} tokens\")\n",
    "        print(f\"Target tokens: {results['sample_tokens'].shape[1]} tokens\")\n",
    "        print(f\"Total tokens: {results['input_tokens'].shape[1]} tokens\")\n",
    "        \n",
    "        # Display the entire assembled context\n",
    "        print(\"\\nAssembled Context (decoded):\")\n",
    "        print(\"-\" * 80)\n",
    "        print(results[\"context_decoded\"])\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Also show just the target part\n",
    "        print(\"\\nTarget tokens (what the model should predict):\")\n",
    "        print(results[\"target_decoded\"])\n",
    "\n",
    "# Create widgets\n",
    "text_input = widgets.Textarea(\n",
    "    value=\"The model is thinking about the relationship between cause and effect.\",\n",
    "    placeholder=\"Enter text to use as context...\",\n",
    "    description=\"Input Text:\",\n",
    "    disabled=False,\n",
    "    layout=widgets.Layout(width=\"100%\", height=\"100px\")\n",
    ")\n",
    "\n",
    "run_button = widgets.Button(\n",
    "    description='Run Test',\n",
    "    disabled=False,\n",
    "    button_style='primary', \n",
    "    tooltip='Run the test with the provided text'\n",
    ")\n",
    "\n",
    "next_button = widgets.Button(\n",
    "    description='Next Sample',\n",
    "    disabled=False,\n",
    "    button_style='info',\n",
    "    tooltip='Move to the next sample'\n",
    ")\n",
    "\n",
    "def on_run_button_clicked(b):\n",
    "    with context_output:\n",
    "        print(\"Running test...\")\n",
    "    results = assemble_decoder_context_without_virtual(text_input.value, current_sample)\n",
    "    display_context_and_results(results)\n",
    "\n",
    "def on_next_button_clicked(b):\n",
    "    global current_sample\n",
    "    if current_sample < batch_size - 1:\n",
    "        current_sample += 1\n",
    "        results = assemble_decoder_context_without_virtual(text_input.value, current_sample)\n",
    "        display_context_and_results(results)\n",
    "    else:\n",
    "        with context_output:\n",
    "            context_output.clear_output(wait=True)\n",
    "            print(\"End of batch reached!\")\n",
    "\n",
    "# Connect the buttons to handlers\n",
    "run_button.on_click(on_run_button_clicked)\n",
    "next_button.on_click(on_next_button_clicked)\n",
    "\n",
    "# Layout\n",
    "controls = widgets.VBox([text_input, widgets.HBox([run_button, next_button])])\n",
    "with button_area:\n",
    "    display(controls)\n",
    "    \n",
    "display(button_area)\n",
    "display(context_output)\n",
    "\n",
    "# Run initial test with default text\n",
    "if batch_size > 0:\n",
    "    results = assemble_decoder_context_without_virtual(text_input.value, current_sample)\n",
    "    display_context_and_results(results)\n",
    "else:\n",
    "    with context_output:\n",
    "        print(\"No samples in batch!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
