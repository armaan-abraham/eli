{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "import ipywidgets as widgets\n",
    "import numpy as np\n",
    "import torch\n",
    "from IPython.display import clear_output, display\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import eli.encoder\n",
    "\n",
    "importlib.reload(eli.encoder)\n",
    "from einops import einsum\n",
    "\n",
    "from eli.config import cfg, encoder_cfg\n",
    "from eli.encoder import (\n",
    "    PROMPT_DECODER,\n",
    "    Encoder,\n",
    "    EncoderDecoder,\n",
    "    EncoderTrainer,\n",
    "    calculate_target_prediction_loss,\n",
    "    get_embeddings_from_decoder,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load encoder and decoder\n",
    "cfg.buffer_size_samples = cfg.target_model_batch_size_samples = (\n",
    "    cfg.train_batch_size_samples\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg.decoder_model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "encoder_decoder = EncoderDecoder(cfg, encoder_cfg, tokenizer).to(cfg.device)\n",
    "\n",
    "encoder = Encoder(cfg, encoder_cfg).to(cfg.device)\n",
    "\n",
    "encoder_path = \"saved_models/encoder.pt\"\n",
    "encoder.load_state_dict(torch.load(encoder_path))\n",
    "\n",
    "encoder_decoder.encoder = encoder\n",
    "\n",
    "encoder_decoder = encoder_decoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(cfg.target_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing data collector\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:target_generated_tokens size: 1024 bytes (0.00 MB)\n",
      "INFO:root:target_acts size: 786432 bytes (0.75 MB)\n",
      "INFO:root:input_tokens size: 65536 bytes (0.06 MB)\n",
      "INFO:root:Total shared memory size: 0.00 GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdba8e94c89944959cc4882380a775b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c5eecddbda44eb9a2103b1f509d62e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Tokenize and concatenate called\n",
      "INFO:root:Full text length: 43727350\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (488415 > 1024). Running this sequence through the model will result in indexing errors\n",
      "INFO:root:Num tokens: 9655191\n",
      "/root/eli/.venv/lib/python3.12/site-packages/datasets/formatting/torch_formatter.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(value, **{**default_dtype, **self.torch_tensor_kwargs})\n",
      "INFO:root:Processing data directly on cuda without workers\n",
      "INFO:root:Processing chunk 0:256 on cuda\n",
      "INFO:root:Processing batch 0:256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2 into HookedTransformer\n",
      "Moving model to device:  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Direct data processing completed\n"
     ]
    }
   ],
   "source": [
    "# Load eval data\n",
    "\n",
    "from eli.data import DataCollector\n",
    "\n",
    "cfg.use_data_collector_workers = False\n",
    "\n",
    "print(\"Initializing data collector\")\n",
    "data_collector = DataCollector(cfg)\n",
    "\n",
    "print(\"Collecting data\")\n",
    "data_collector.collect_data()\n",
    "\n",
    "data = data_collector.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 3]) torch.Size([256, 3])\n",
      "Target prediction loss: 3.6078619956970215\n"
     ]
    }
   ],
   "source": [
    "# Print loss statistics\n",
    "target_generated_tokens = data[\"target_generated_tokens\"]\n",
    "target_acts = data[\"target_acts\"]\n",
    "\n",
    "buffer_size = target_acts.shape[0]\n",
    "batch_size = cfg.train_batch_size_samples\n",
    "num_batches = buffer_size // batch_size\n",
    "\n",
    "target_prediction_losses = []\n",
    "dinalar_losses = []\n",
    "\n",
    "def recode_and_strip(tokens, tokenizer):\n",
    "    decoded = tokenizer.batch_decode(\n",
    "        sequences=tokens,\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=True,\n",
    "    )\n",
    "    decoded = [r.strip() for r in decoded]\n",
    "    encoded = tokenizer(\n",
    "        decoded, add_special_tokens=False, return_tensors=\"pt\", padding=True\n",
    "    )\n",
    "    output_tokens = encoded.input_ids.long()\n",
    "    attention_mask = encoded.attention_mask\n",
    "    return output_tokens, attention_mask\n",
    "\n",
    "for batch_idx in range(num_batches):\n",
    "    start_idx = batch_idx * batch_size\n",
    "    end_idx = start_idx + batch_size\n",
    "\n",
    "    # Extract batch data and move to device\n",
    "    batch_tokens = target_generated_tokens[start_idx:end_idx].to(cfg.device)\n",
    "    batch_acts = target_acts[start_idx:end_idx].to(cfg.device)\n",
    "\n",
    "    batch_tokens, attention_mask = recode_and_strip(batch_tokens, tokenizer)\n",
    "    batch_tokens = batch_tokens.to(cfg.device)\n",
    "\n",
    "    print(batch_tokens.shape, attention_mask.shape)\n",
    "\n",
    "    attention_mask = attention_mask.to(cfg.device)\n",
    "\n",
    "    loss = EncoderTrainer.loss(\n",
    "        cfg, encoder_decoder, batch_tokens, attention_mask, batch_acts, tokenizer, train_iter=-1\n",
    "    )\n",
    "    target_prediction_losses.append(loss.item())\n",
    "\n",
    "print(f\"Target prediction loss: {np.mean(target_prediction_losses)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "592ff14c763e478bafd287afed8577ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c40a1216853e4077a3745018a60e8cbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[46136, 45348, 16207, 18658, 17773]], device='cuda:0')\n",
      "torch.Size([1, 5])\n",
      "top_values.shape torch.Size([1, 5])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[36473, 46136, 17576, 21364, 28599]], device='cuda:0')\n",
      "torch.Size([1, 5])\n",
      "top_values.shape torch.Size([1, 5])\n",
      "tensor([[17773, 15838, 39021, 41407, 36622]], device='cuda:0')\n",
      "torch.Size([1, 5])\n",
      "top_values.shape torch.Size([1, 5])\n",
      "tensor([[41407, 36473, 14369, 23785, 17773]], device='cuda:0')\n",
      "torch.Size([1, 5])\n",
      "top_values.shape torch.Size([1, 5])\n",
      "tensor([[15838, 11411, 23785, 16207, 45348]], device='cuda:0')\n",
      "torch.Size([1, 5])\n",
      "top_values.shape torch.Size([1, 5])\n",
      "tensor([[41407, 36622, 30202, 41230, 45999]], device='cuda:0')\n",
      "torch.Size([1, 5])\n",
      "top_values.shape torch.Size([1, 5])\n",
      "tensor([[23785, 15838, 36473, 14369, 36622]], device='cuda:0')\n",
      "torch.Size([1, 5])\n",
      "top_values.shape torch.Size([1, 5])\n",
      "tensor([[29823, 48382,  6909, 45260, 19593]], device='cuda:0')\n",
      "torch.Size([1, 5])\n",
      "top_values.shape torch.Size([1, 5])\n",
      "tensor([[23785, 50179, 11411, 15838, 39021]], device='cuda:0')\n",
      "torch.Size([1, 5])\n",
      "top_values.shape torch.Size([1, 5])\n",
      "tensor([[23785, 46659, 28420, 46136, 17773]], device='cuda:0')\n",
      "torch.Size([1, 5])\n",
      "top_values.shape torch.Size([1, 5])\n",
      "tensor([[36473, 36622, 23785, 11411, 41407]], device='cuda:0')\n",
      "torch.Size([1, 5])\n",
      "top_values.shape torch.Size([1, 5])\n",
      "tensor([[17773, 41407, 32524, 11411, 48527]], device='cuda:0')\n",
      "torch.Size([1, 5])\n",
      "top_values.shape torch.Size([1, 5])\n",
      "tensor([[11411, 15838, 31732, 36473, 17773]], device='cuda:0')\n",
      "torch.Size([1, 5])\n",
      "top_values.shape torch.Size([1, 5])\n"
     ]
    }
   ],
   "source": [
    "from jaxtyping import Float\n",
    "from torch import Tensor\n",
    "\n",
    "# Create output widgets for displaying sample information\n",
    "sample_output = widgets.Output()\n",
    "button_output = widgets.Output()\n",
    "\n",
    "# Create a counter and button\n",
    "current_sample = 0\n",
    "\n",
    "def get_distances_to_embeddings(\n",
    "    embeddings: Float[Tensor, \"batch tok d_embed\"],\n",
    "    target_embeddings: Float[Tensor, \"vocab d_embed\"],\n",
    ") -> Float[Tensor, \"batch tok vocab\"]:\n",
    "    \"\"\"Computes the L2 distance between each token embedding and each target embedding.\"\"\"\n",
    "    # embeddings shape: [batch, tok, d_embed]\n",
    "    # target_embeddings shape: [vocab, d_embed]\n",
    "\n",
    "    # Reshape embeddings to [batch*tok, d_embed]\n",
    "    batch_size, seq_len, d_embed = embeddings.shape\n",
    "    embeddings_flat = embeddings.reshape(-1, d_embed)\n",
    "\n",
    "    # Compute pairwise distances between all embeddings and target embeddings\n",
    "    # Returns tensor of shape [batch*tok, vocab]\n",
    "    distances = torch.cdist(embeddings_flat, target_embeddings, p=2)\n",
    "\n",
    "    # Reshape back to [batch, tok, vocab]\n",
    "    return distances.reshape(batch_size, seq_len, -1)\n",
    "\n",
    "def on_button_click(b):\n",
    "    global current_sample\n",
    "    if current_sample < batch_size:\n",
    "        display_sample(current_sample)\n",
    "        current_sample += 1\n",
    "    else:\n",
    "        with sample_output:\n",
    "            print(\"End of batch reached!\")\n",
    "\n",
    "\n",
    "def create_table(title, headers, rows, col_widths=None):\n",
    "    \"\"\"Helper function to create formatted tables\n",
    "\n",
    "    Args:\n",
    "        title: Table title string\n",
    "        headers: List of header strings\n",
    "        rows: List of rows, where each row is a list of values\n",
    "        col_widths: List of column widths (defaults to 15 for all columns)\n",
    "\n",
    "    Returns:\n",
    "        Formatted table string\n",
    "    \"\"\"\n",
    "    if col_widths is None:\n",
    "        col_widths = [15] * len(headers)\n",
    "\n",
    "    # Ensure first column width accommodates row labels\n",
    "    col_widths[0] = max(col_widths[0], 8)\n",
    "\n",
    "    # Create table string\n",
    "    table = f\"{title}\\n\"\n",
    "\n",
    "    # Create header\n",
    "    header_row = headers[0].ljust(col_widths[0])\n",
    "    for i, header in enumerate(headers[1:], 1):\n",
    "        header_row += header.ljust(col_widths[i])\n",
    "    table += header_row + \"\\n\"\n",
    "\n",
    "    # Add separator\n",
    "    table += \"-\" * len(header_row) + \"\\n\"\n",
    "\n",
    "    # Add rows\n",
    "    for row in rows:\n",
    "        row_str = str(row[0]).ljust(col_widths[0])\n",
    "        for i, cell in enumerate(row[1:], 1):\n",
    "            row_str += str(cell).ljust(col_widths[i])\n",
    "        table += row_str + \"\\n\"\n",
    "\n",
    "    return table\n",
    "\n",
    "\n",
    "def shuffle_data(tokens, acts, seed=None):\n",
    "    \"\"\"\n",
    "    Shuffle the tokens and acts tensors in the same order.\n",
    "\n",
    "    Args:\n",
    "        tokens: Tensor of token IDs\n",
    "        acts: Tensor of activations\n",
    "        seed: Optional random seed for reproducibility\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (shuffled_tokens, shuffled_acts)\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "    # Get the number of samples\n",
    "    num_samples = tokens.size(0)\n",
    "\n",
    "    # Generate random permutation indices\n",
    "    indices = torch.randperm(num_samples)\n",
    "\n",
    "    # Shuffle both tensors using the same indices\n",
    "    shuffled_tokens = tokens[indices]\n",
    "    shuffled_acts = acts[indices]\n",
    "\n",
    "    return shuffled_tokens, shuffled_acts\n",
    "\n",
    "\n",
    "# Function to display a single sample\n",
    "def display_sample(sample_idx):\n",
    "    with torch.no_grad():\n",
    "        with torch.autocast(device_type=cfg.device.type, dtype=cfg.dtype):\n",
    "            # Extract single sample as a \"batch\" of size 1\n",
    "            sample_tokens = target_generated_tokens[sample_idx : sample_idx + 1].to(\n",
    "                cfg.device, dtype=torch.long\n",
    "            )\n",
    "            sample_tokens, attention_mask = recode_and_strip(sample_tokens, tokenizer)\n",
    "            sample_tokens = sample_tokens.to(cfg.device)\n",
    "            attention_mask = attention_mask.to(cfg.device)\n",
    "            sample_acts = target_acts[sample_idx : sample_idx + 1].to(cfg.device)\n",
    "\n",
    "            # encoder = encoder_decoder.encoder\n",
    "            # encoder_output_logits = encoder(sample_acts) # [batch tok vocab]\n",
    "\n",
    "            # Convert logits to one-hot-like by making the max value very large and others small\n",
    "            # max_values, max_indices = torch.max(encoder_output_logits, dim=-1, keepdim=True)\n",
    "            # one_hot_logits = torch.ones_like(encoder_output_logits) * -100.0  # Set all values to a small number\n",
    "            # one_hot_logits.scatter_(dim=-1, index=max_indices, value=100.0)   # Set max values to a large number\n",
    "            # encoder_output_logits = one_hot_logits\n",
    "\n",
    "            # Get model outputs for this single sample\n",
    "            (decoder_logits_target, decoder_logits_encoding, virtual_embs) = (\n",
    "                encoder_decoder(sample_acts, sample_tokens, attention_mask, train_iter=-1)\n",
    "            )\n",
    "\n",
    "            # Calculate losses using existing functions\n",
    "            pred_loss = calculate_target_prediction_loss(\n",
    "                decoder_logits_target, sample_tokens, tokenizer\n",
    "            ).item()\n",
    "            # din_loss = calculate_dinalar_loss(\n",
    "            #     decoder_logits_encoding,\n",
    "            #     encoder_output_logits,\n",
    "            # ).item()\n",
    "\n",
    "            embeddings = get_embeddings_from_decoder(encoder_decoder.decoder).weight\n",
    "            distances = get_distances_to_embeddings(\n",
    "                virtual_embs, embeddings\n",
    "            )  # [batch tok vocab]\n",
    "\n",
    "            # print(\"distances.shape\", distances.shape)\n",
    "\n",
    "            # Get top 3 tokens by encoder output logits\n",
    "            top_k = 5\n",
    "            top_values, top_indices = torch.topk(\n",
    "                distances[0], k=top_k, dim=-1\n",
    "            )  # [batch tok]\n",
    "\n",
    "            print(top_indices)\n",
    "            print(top_indices.shape)\n",
    "\n",
    "            print(\"top_values.shape\", top_values.shape)\n",
    "\n",
    "            # Decode tokens for display\n",
    "            sample_decoded = tokenizer.decode(sample_tokens[0])\n",
    "\n",
    "            # Display results\n",
    "            with sample_output:\n",
    "                sample_output.clear_output(wait=True)\n",
    "                print(f\"Sample {sample_idx+1}/{batch_size}\")\n",
    "                print(f\"Target prediction loss: {pred_loss:.6f}\")\n",
    "                print(\"\\nTarget tokens:\")\n",
    "                print(sample_decoded, \"\\n\")\n",
    "\n",
    "                prompt_prefix, prompt_suffix = PROMPT_DECODER.split(\"<thought>\")\n",
    "                # Also decode and display the prefix and suffix tokens\n",
    "                prefix_tokens = tokenizer(prompt_prefix, return_tensors=\"pt\").input_ids[\n",
    "                    0\n",
    "                ]\n",
    "                suffix_tokens = tokenizer(prompt_suffix, return_tensors=\"pt\").input_ids[\n",
    "                    0\n",
    "                ]\n",
    "\n",
    "                print(tokenizer.decode(prefix_tokens))\n",
    "\n",
    "                # Prepare data for the tokens table\n",
    "                col_width = 15\n",
    "                headers = [\"Token\"] + [f\"Emb {i}\" for i in range(virtual_embs.shape[1])]\n",
    "\n",
    "                token_rows = []\n",
    "                for k in range(top_k):\n",
    "                    row = [f\"Top {k+1}:\"]\n",
    "                    for j in range(virtual_embs.shape[1]):\n",
    "                        token_id = top_indices[j, k].item()\n",
    "                        token_text = tokenizer.decode([token_id])\n",
    "                        # Replace newlines and tabs for cleaner display\n",
    "                        token_text = token_text.replace(\"\\n\", \"\\\\n\").replace(\n",
    "                            \"\\t\", \"\\\\t\"\n",
    "                        )\n",
    "                        # Truncate to fit in column\n",
    "                        token_display = token_text[: col_width - 2]\n",
    "                        row.append(token_display)\n",
    "                    token_rows.append(row)\n",
    "\n",
    "                # Create and display the token table\n",
    "                token_table = create_table(\n",
    "                    \"\", headers, token_rows, [8] + [col_width] * virtual_embs.shape[1]\n",
    "                )\n",
    "                print(token_table)\n",
    "\n",
    "                # Create table for logit values\n",
    "                logit_rows = []\n",
    "                for k in range(top_k):\n",
    "                    row = [f\"Top {k+1}:\"]\n",
    "                    for j in range(virtual_embs.shape[1]):\n",
    "                        # Format logit value with 5 decimal places\n",
    "                        prob = top_values[j, k].item()\n",
    "                        row.append(f\"{prob:.5f}\")\n",
    "                    logit_rows.append(row)\n",
    "\n",
    "                # Create and display the logits table\n",
    "                logit_table = create_table(\n",
    "                    \"Logit Values:\",\n",
    "                    headers,\n",
    "                    logit_rows,\n",
    "                    [8] + [col_width] * virtual_embs.shape[1],\n",
    "                )\n",
    "                print(logit_table)\n",
    "\n",
    "                print(tokenizer.decode(suffix_tokens))\n",
    "\n",
    "\n",
    "# Interactive sample investigation\n",
    "next_button = widgets.Button(description=\"Next Sample\")\n",
    "next_button.on_click(on_button_click)\n",
    "\n",
    "# Display the button and sample output in separate areas\n",
    "with button_output:\n",
    "    display(next_button)\n",
    "display(button_output)\n",
    "display(sample_output)\n",
    "\n",
    "# Show the first sample\n",
    "if batch_size > 0:\n",
    "    display_sample(current_sample)\n",
    "    current_sample += 1\n",
    "else:\n",
    "    with sample_output:\n",
    "        print(\"No samples in batch!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = \"Bob\"\n",
    "# tokens = tokenizer(text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "# Print each token separately\n",
    "# print(\"Tokens for text:\", text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[44484,   474,   330,   672,  1559,   373,   257,   922,  1048,    13,\n",
      "         14862,   220]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "tensor([[44484,   474,   330,   672,  1559,   373,   257,   922,  1048,    13,\n",
      "         14862,   220]])\n",
      "torch.Size([50257])\n",
      "Top 10 tokens by logit:\n",
      "1. Token: ' ', ID: 1849, Logit: -46.7644\n",
      "2. Token: 'iced', ID: 3711, Logit: -47.3270\n",
      "3. Token: 'ich', ID: 488, Logit: -48.3128\n",
      "4. Token: 'ive', ID: 425, Logit: -48.6372\n",
      "5. Token: 'irl', ID: 1901, Logit: -48.7395\n",
      "6. Token: 'ix', ID: 844, Logit: -49.0847\n",
      "7. Token: 'iz', ID: 528, Logit: -49.1540\n",
      "8. Token: 'izzy', ID: 40593, Logit: -49.2239\n",
      "9. Token: 'ike', ID: 522, Logit: -49.2902\n",
      "10. Token: '________', ID: 2602, Logit: -49.3509\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "text = \"Alice jacobson was a good person. Alice \"\n",
    "tokens = tokenizer(text, add_special_tokens=True, return_tensors=\"pt\")\n",
    "print(tokens)\n",
    "print(tokens.input_ids)\n",
    "tokens = torch.cat([torch.tensor([[tokenizer.bos_token_id]]), tokens.input_ids], dim=1)\n",
    "\n",
    "# Get model outputs\n",
    "outputs = model(tokens)\n",
    "logits = outputs.logits\n",
    "\n",
    "# Get the logits for the last token\n",
    "last_token_logits = logits[0, -1, :]\n",
    "print(last_token_logits.shape)\n",
    "\n",
    "# Get the top 3 token indices and their corresponding logits\n",
    "k = 10\n",
    "top_values, top_indices = torch.topk(last_token_logits, k)\n",
    "\n",
    "# Print the top 3 tokens and their logits\n",
    "print(f\"Top {k} tokens by logit:\")\n",
    "for i, (token_id, logit_value) in enumerate(zip(top_indices, top_values)):\n",
    "    token_text = tokenizer.decode([token_id.item()])\n",
    "    print(\n",
    "        f\"{i+1}. Token: '{token_text}', ID: {token_id.item()}, Logit: {logit_value.item():.4f}\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
