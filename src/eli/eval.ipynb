{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "import numpy as np\n",
    "import torch\n",
    "from IPython.display import clear_output, display\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from eli.config import cfg, encoder_cfg\n",
    "from eli.encoder import (\n",
    "    PROMPT_PREFIX,\n",
    "    PROMPT_SUFFIX,\n",
    "    Encoder,\n",
    "    EncoderDecoder,\n",
    "    EncoderTrainer,\n",
    "    calculate_dinalar_loss,\n",
    "    get_embeddings_from_decoder,\n",
    "    kl_div,\n",
    ")\n",
    "\n",
    "# Load encoder and decoder\n",
    "\n",
    "cfg.buffer_size_samples = cfg.target_model_batch_size_samples = cfg.train_batch_size_samples\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg.decoder_model_name)\n",
    "\n",
    "encoder_decoder = EncoderDecoder(cfg, encoder_cfg, tokenizer).to(cfg.device)\n",
    "\n",
    "encoder = Encoder(cfg, encoder_cfg).to(cfg.device)\n",
    "\n",
    "encoder_path = \"encoder-dinalar.pt\"\n",
    "encoder.load_state_dict(torch.load(encoder_path))\n",
    "\n",
    "encoder_decoder.encoder = encoder\n",
    "\n",
    "encoder_decoder = encoder_decoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing data collector\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:target_generated_tokens size: 30720 bytes (0.03 MB)\n",
      "INFO:root:target_logits size: 1648361472 bytes (1572.00 MB)\n",
      "INFO:root:target_acts size: 262144 bytes (0.25 MB)\n",
      "INFO:root:input_tokens size: 131072 bytes (0.12 MB)\n",
      "INFO:root:Total shared memory size: 1.54 GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4654bb9382c4daa8cc2e49f8f5a552f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "454ded06a2614ed5aa261980af12db1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Tokenize and concatenate called\n",
      "INFO:root:Full text length: 43727350\n",
      "INFO:root:Num tokens: 9672977\n",
      "INFO:root:Processing data directly on cuda without workers\n",
      "INFO:root:Processing chunk 0:512 on cuda\n",
      "INFO:root:Processing batch 0:512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model EleutherAI/pythia-14m into HookedTransformer\n",
      "Moving model to device:  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:CHUNK 0:512 COMPLETED, Max GPU memory allocated: 13.29 GB, Max GPU memory reserved: 19.27 GB\n",
      "INFO:root:Direct data processing completed\n"
     ]
    }
   ],
   "source": [
    "# Load eval data\n",
    "\n",
    "from eli.data import DataCollector\n",
    "\n",
    "print(\"Initializing data collector\")\n",
    "data_collector = DataCollector(use_workers=False)\n",
    "\n",
    "print(\"Collecting data\")\n",
    "data_collector.collect_data()\n",
    "\n",
    "data = data_collector.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target prediction loss: 0.4922386109828949\n",
      "Dinalar loss: 1.1478568315505981\n"
     ]
    }
   ],
   "source": [
    "# Print loss statistics\n",
    "target_generated_tokens = data[\"target_generated_tokens\"]\n",
    "target_logits = data[\"target_logits\"]\n",
    "target_acts = data[\"target_acts\"]\n",
    "\n",
    "buffer_size = target_acts.shape[0]\n",
    "batch_size = cfg.train_batch_size_samples\n",
    "num_batches = buffer_size // batch_size\n",
    "\n",
    "target_prediction_losses = []\n",
    "dinalar_losses = []\n",
    "\n",
    "for batch_idx in range(num_batches):\n",
    "    start_idx = batch_idx * batch_size\n",
    "    end_idx = start_idx + batch_size\n",
    "\n",
    "    # Extract batch data and move to device\n",
    "    batch_tokens = target_generated_tokens[start_idx:end_idx].to(cfg.device)\n",
    "    batch_logits = target_logits[start_idx:end_idx].to(cfg.device, dtype=torch.float32)\n",
    "    batch_acts = target_acts[start_idx:end_idx].to(cfg.device)\n",
    "\n",
    "    loss, target_prediction_loss, dinalar_loss = EncoderTrainer.loss(\n",
    "        cfg, encoder_decoder, batch_tokens, batch_logits, batch_acts, -1\n",
    "    )\n",
    "\n",
    "    target_prediction_losses.append(target_prediction_loss.item())\n",
    "    dinalar_losses.append(dinalar_loss.item())\n",
    "\n",
    "print(f\"Target prediction loss: {np.mean(target_prediction_losses)}\")\n",
    "print(f\"Dinalar loss: {np.mean(dinalar_losses)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ec6b3f640fb4e2d934c206bb2ba7aa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff3ea51217574a0790a15dc4f6414db7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create output widgets for displaying sample information\n",
    "sample_output = widgets.Output()\n",
    "button_output = widgets.Output()\n",
    "\n",
    "# Create a counter and button\n",
    "current_sample = 0\n",
    "\n",
    "\n",
    "def on_button_click(b):\n",
    "    global current_sample\n",
    "    if current_sample < batch_size:\n",
    "        display_sample(current_sample)\n",
    "        current_sample += 1\n",
    "    else:\n",
    "        with sample_output:\n",
    "            print(\"End of batch reached!\")\n",
    "\n",
    "\n",
    "def create_table(title, headers, rows, col_widths=None):\n",
    "    \"\"\"Helper function to create formatted tables\n",
    "\n",
    "    Args:\n",
    "        title: Table title string\n",
    "        headers: List of header strings\n",
    "        rows: List of rows, where each row is a list of values\n",
    "        col_widths: List of column widths (defaults to 15 for all columns)\n",
    "\n",
    "    Returns:\n",
    "        Formatted table string\n",
    "    \"\"\"\n",
    "    if col_widths is None:\n",
    "        col_widths = [15] * len(headers)\n",
    "\n",
    "    # Ensure first column width accommodates row labels\n",
    "    col_widths[0] = max(col_widths[0], 8)\n",
    "\n",
    "    # Create table string\n",
    "    table = f\"{title}\\n\"\n",
    "\n",
    "    # Create header\n",
    "    header_row = headers[0].ljust(col_widths[0])\n",
    "    for i, header in enumerate(headers[1:], 1):\n",
    "        header_row += header.ljust(col_widths[i])\n",
    "    table += header_row + \"\\n\"\n",
    "\n",
    "    # Add separator\n",
    "    table += \"-\" * len(header_row) + \"\\n\"\n",
    "\n",
    "    # Add rows\n",
    "    for row in rows:\n",
    "        row_str = str(row[0]).ljust(col_widths[0])\n",
    "        for i, cell in enumerate(row[1:], 1):\n",
    "            row_str += str(cell).ljust(col_widths[i])\n",
    "        table += row_str + \"\\n\"\n",
    "\n",
    "    return table\n",
    "\n",
    "\n",
    "# Function to display a single sample\n",
    "def display_sample(sample_idx):\n",
    "    with torch.no_grad():\n",
    "        with torch.autocast(device_type=cfg.device.type, dtype=cfg.dtype):\n",
    "            # Extract single sample as a \"batch\" of size 1\n",
    "            sample_tokens = target_generated_tokens[sample_idx : sample_idx + 1].to(\n",
    "                cfg.device\n",
    "            )\n",
    "            sample_logits = target_logits[sample_idx : sample_idx + 1].to(\n",
    "                cfg.device, dtype=torch.float32\n",
    "            )\n",
    "            sample_acts = target_acts[sample_idx : sample_idx + 1].to(cfg.device)\n",
    "\n",
    "            # Get model outputs for this single sample\n",
    "            (decoder_logits_target, decoder_logits_encoding, virtual_embs) = (\n",
    "                encoder_decoder(sample_acts, sample_tokens, -1)\n",
    "            )\n",
    "\n",
    "            # Calculate losses using existing functions\n",
    "            pred_loss = kl_div(decoder_logits_target, sample_logits).item()\n",
    "            din_loss = calculate_dinalar_loss(\n",
    "                decoder_logits_encoding,\n",
    "                virtual_embs,\n",
    "                encoder_decoder.decoder\n",
    "                if not isinstance(encoder_decoder, torch.nn.DataParallel)\n",
    "                else encoder_decoder.module.decoder,\n",
    "            ).item()\n",
    "\n",
    "            # Find top 3 closest tokens to each virtual embedding\n",
    "            decoder = (\n",
    "                encoder_decoder.decoder\n",
    "                if not isinstance(encoder_decoder, torch.nn.DataParallel)\n",
    "                else encoder_decoder.module.decoder\n",
    "            )\n",
    "            token_embeddings = get_embeddings_from_decoder(\n",
    "                decoder\n",
    "            ).weight  # [vocab_size, d_embed]\n",
    "\n",
    "            # Calculate L2 distances between virtual embeddings and token embeddings\n",
    "            # Reshape for broadcasting: [1, encoding_len, d_embed] and [vocab_size, 1, d_embed]\n",
    "            v_embs = virtual_embs[0].unsqueeze(0)  # [1, encoding_len, d_embed]\n",
    "            t_embs = token_embeddings.unsqueeze(1)  # [vocab_size, 1, d_embed]\n",
    "\n",
    "            # Calculate squared distances\n",
    "            distances = torch.sum(\n",
    "                (v_embs - t_embs) ** 2, dim=2\n",
    "            )  # [vocab_size, encoding_len]\n",
    "\n",
    "            # Get top 3 closest tokens for each virtual embedding\n",
    "            top_k = 3\n",
    "            top_values, top_indices = torch.topk(\n",
    "                distances, k=top_k, dim=0, largest=False\n",
    "            )\n",
    "\n",
    "            # Decode tokens for display\n",
    "            sample_decoded = tokenizer.decode(sample_tokens[0])\n",
    "\n",
    "            # Display results\n",
    "            with sample_output:\n",
    "                sample_output.clear_output(wait=True)\n",
    "                print(f\"Sample {sample_idx+1}/{batch_size}\")\n",
    "                print(f\"Target prediction loss: {pred_loss:.6f}\")\n",
    "                print(f\"Dinalar loss: {din_loss:.6f}\")\n",
    "                print(\"\\nTarget tokens:\")\n",
    "                print(sample_decoded, \"\\n\")\n",
    "\n",
    "                # Also decode and display the prefix and suffix tokens\n",
    "                prefix_tokens = tokenizer(PROMPT_PREFIX, return_tensors=\"pt\").input_ids[\n",
    "                    0\n",
    "                ]\n",
    "                suffix_tokens = tokenizer(PROMPT_SUFFIX, return_tensors=\"pt\").input_ids[\n",
    "                    0\n",
    "                ]\n",
    "\n",
    "                print(tokenizer.decode(prefix_tokens))\n",
    "\n",
    "                # Prepare data for the tokens table\n",
    "                col_width = 15\n",
    "                headers = [\"Token\"] + [f\"Emb {i}\" for i in range(virtual_embs.shape[1])]\n",
    "\n",
    "                token_rows = []\n",
    "                for k in range(top_k):\n",
    "                    row = [f\"Top {k+1}:\"]\n",
    "                    for j in range(virtual_embs.shape[1]):\n",
    "                        token_id = top_indices[k, j].item()\n",
    "                        token_text = tokenizer.decode([token_id])\n",
    "                        # Replace newlines and tabs for cleaner display\n",
    "                        token_text = token_text.replace(\"\\n\", \"\\\\n\").replace(\n",
    "                            \"\\t\", \"\\\\t\"\n",
    "                        )\n",
    "                        # Truncate to fit in column\n",
    "                        token_display = token_text[: col_width - 2]\n",
    "                        row.append(token_display)\n",
    "                    token_rows.append(row)\n",
    "\n",
    "                # Create and display the token table\n",
    "                token_table = create_table(\n",
    "                    \"\", headers, token_rows, [8] + [col_width] * virtual_embs.shape[1]\n",
    "                )\n",
    "                print(token_table)\n",
    "\n",
    "                # Create table for distances\n",
    "                distance_rows = []\n",
    "                for k in range(top_k):\n",
    "                    row = [f\"Top {k+1}:\"]\n",
    "                    for j in range(virtual_embs.shape[1]):\n",
    "                        # Format distance value with 5 decimal places\n",
    "                        distance = top_values[k, j].item()\n",
    "                        row.append(f\"{distance:.5f}\")\n",
    "                    distance_rows.append(row)\n",
    "\n",
    "                # Create and display the distances table\n",
    "                distance_table = create_table(\n",
    "                    \"L2 Distances:\",\n",
    "                    headers,\n",
    "                    distance_rows,\n",
    "                    [8] + [col_width] * virtual_embs.shape[1],\n",
    "                )\n",
    "                print(distance_table)\n",
    "\n",
    "                print(tokenizer.decode(suffix_tokens))\n",
    "\n",
    "\n",
    "# Interactive sample investigation\n",
    "next_button = widgets.Button(description=\"Next Sample\")\n",
    "next_button.on_click(on_button_click)\n",
    "\n",
    "# Display the button and sample output in separate areas\n",
    "with button_output:\n",
    "    display(next_button)\n",
    "display(button_output)\n",
    "display(sample_output)\n",
    "\n",
    "# Show the first sample\n",
    "if batch_size > 0:\n",
    "    display_sample(current_sample)\n",
    "    current_sample += 1\n",
    "else:\n",
    "    with sample_output:\n",
    "        print(\"No samples in batch!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
