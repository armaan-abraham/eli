{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "import ipywidgets as widgets\n",
    "import numpy as np\n",
    "import torch\n",
    "from IPython.display import clear_output, display\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import eli.encoder\n",
    "\n",
    "importlib.reload(eli.encoder)\n",
    "from einops import einsum\n",
    "\n",
    "from eli.config import cfg, encoder_cfg\n",
    "from eli.encoder import (\n",
    "    PROMPT_DECODER,\n",
    "    Encoder,\n",
    "    EncoderDecoder,\n",
    "    EncoderTrainer,\n",
    "    calculate_target_prediction_loss,\n",
    "    get_embeddings_from_decoder,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Encoder:\n\tsize mismatch for multiplex_heads.0.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([512, 768]).\n\tsize mismatch for output_heads.0.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 512]).\n\tsize mismatch for output_heads.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m encoder = Encoder(cfg, encoder_cfg).to(cfg.device)\n\u001b[32m     12\u001b[39m encoder_path = \u001b[33m\"\u001b[39m\u001b[33msaved_models/encoder-14m-2toks.pt\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[43mencoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m encoder_decoder.encoder = encoder\n\u001b[32m     17\u001b[39m encoder_decoder = encoder_decoder.eval()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/eli/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:2581\u001b[39m, in \u001b[36mModule.load_state_dict\u001b[39m\u001b[34m(self, state_dict, strict, assign)\u001b[39m\n\u001b[32m   2573\u001b[39m         error_msgs.insert(\n\u001b[32m   2574\u001b[39m             \u001b[32m0\u001b[39m,\n\u001b[32m   2575\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2576\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[32m   2577\u001b[39m             ),\n\u001b[32m   2578\u001b[39m         )\n\u001b[32m   2580\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) > \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2581\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   2582\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2583\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m, \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m\"\u001b[39m.join(error_msgs)\n\u001b[32m   2584\u001b[39m         )\n\u001b[32m   2585\u001b[39m     )\n\u001b[32m   2586\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[31mRuntimeError\u001b[39m: Error(s) in loading state_dict for Encoder:\n\tsize mismatch for multiplex_heads.0.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([512, 768]).\n\tsize mismatch for output_heads.0.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 512]).\n\tsize mismatch for output_heads.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768])."
     ]
    }
   ],
   "source": [
    "# Load encoder and decoder\n",
    "\n",
    "cfg.buffer_size_samples = cfg.target_model_batch_size_samples = (\n",
    "    cfg.train_batch_size_samples\n",
    ")\n",
    "cfg.site = \"resid_post\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg.decoder_model_name)\n",
    "\n",
    "encoder_decoder = EncoderDecoder(cfg, encoder_cfg, tokenizer).to(cfg.device)\n",
    "\n",
    "encoder = Encoder(cfg, encoder_cfg).to(cfg.device)\n",
    "\n",
    "encoder_path = \"saved_models/encoder-14m-2toks.pt\"\n",
    "encoder.load_state_dict(torch.load(encoder_path))\n",
    "\n",
    "encoder_decoder.encoder = encoder\n",
    "\n",
    "encoder_decoder = encoder_decoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(cfg.target_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:target_generated_tokens size: 4096 bytes (0.00 MB)\n",
      "INFO:root:target_acts size: 3145728 bytes (3.00 MB)\n",
      "INFO:root:input_tokens size: 262144 bytes (0.25 MB)\n",
      "INFO:root:Total shared memory size: 0.00 GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing data collector\n",
      "Collecting data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bf00bf06f4545728e266ccfcec155c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2ba6df7791544d0917b0bca4532ac5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Tokenize and concatenate called\n",
      "INFO:root:Full text length: 43727350\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (488415 > 1024). Running this sequence through the model will result in indexing errors\n",
      "INFO:root:Num tokens: 9655191\n",
      "/root/eli/.venv/lib/python3.12/site-packages/datasets/formatting/torch_formatter.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(value, **{**default_dtype, **self.torch_tensor_kwargs})\n",
      "INFO:root:Processing data directly on cuda without workers\n",
      "INFO:root:Processing chunk 0:1024 on cuda\n",
      "INFO:root:Processing batch 0:1024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2 into HookedTransformer\n",
      "Moving model to device:  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Direct data processing completed\n"
     ]
    }
   ],
   "source": [
    "# Load eval data\n",
    "\n",
    "from eli.data import DataCollector\n",
    "\n",
    "cfg.use_data_collector_workers = False\n",
    "\n",
    "print(\"Initializing data collector\")\n",
    "data_collector = DataCollector(cfg)\n",
    "\n",
    "print(\"Collecting data\")\n",
    "data_collector.collect_data()\n",
    "\n",
    "data = data_collector.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 1])\n",
      "tensor([[1, 0, 0, 0],\n",
      "        [1, 0, 0, 0],\n",
      "        [1, 1, 0, 0],\n",
      "        ...,\n",
      "        [1, 0, 0, 0],\n",
      "        [1, 0, 0, 0],\n",
      "        [1, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "target_tokens = data[\"target_generated_tokens\"]\n",
    "print(target_tokens.shape)\n",
    "decoded = tokenizer.batch_decode(\n",
    "    sequences=target_tokens, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    ")\n",
    "decoded = [r.strip() for r in decoded]\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "result = tokenizer(decoded, add_special_tokens=False, return_tensors=\"pt\", padding=True)\n",
    "encoded = result.input_ids\n",
    "attention_mask = result.attention_mask\n",
    "print(attention_mask)\n",
    "\n",
    "decoded = tokenizer.batch_decode(\n",
    "    sequences=encoded, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    ")\n",
    "decoded = [r.strip() for r in decoded]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Its',\n",
       " 'handle',\n",
       " 'environmental',\n",
       " 'yoga',\n",
       " ',',\n",
       " 'joint',\n",
       " 'yoga',\n",
       " 'to',\n",
       " 'the',\n",
       " 'can',\n",
       " '.',\n",
       " 'them',\n",
       " '.',\n",
       " 'that',\n",
       " 'flat',\n",
       " ',',\n",
       " 'of',\n",
       " 'sure',\n",
       " 'rewards',\n",
       " 'virus',\n",
       " 'a',\n",
       " 'several',\n",
       " 'for',\n",
       " 'reaction',\n",
       " 'highly',\n",
       " 'on',\n",
       " 'acking',\n",
       " 'at',\n",
       " 'the',\n",
       " 'keep',\n",
       " 'zone',\n",
       " '—',\n",
       " 'soil',\n",
       " 'or',\n",
       " 'soon',\n",
       " 'watering',\n",
       " 'plants',\n",
       " 'and',\n",
       " ',',\n",
       " 'plants',\n",
       " ',',\n",
       " 'from',\n",
       " 'have',\n",
       " 'a',\n",
       " 'are',\n",
       " 'over',\n",
       " 'certification',\n",
       " 'umatic',\n",
       " 'piston',\n",
       " 'on',\n",
       " 'aps',\n",
       " 'and',\n",
       " 'shall',\n",
       " 'also',\n",
       " 'st',\n",
       " 'and',\n",
       " 'able',\n",
       " ',',\n",
       " 'the',\n",
       " 'are',\n",
       " 'All',\n",
       " '.',\n",
       " 'my',\n",
       " 'mailing',\n",
       " 'David',\n",
       " 'me',\n",
       " 'aco',\n",
       " ':',\n",
       " 'has',\n",
       " 'the',\n",
       " ',',\n",
       " 'your',\n",
       " ',',\n",
       " ')',\n",
       " 'missions',\n",
       " '1',\n",
       " 'a',\n",
       " 'admission',\n",
       " 'with',\n",
       " '-',\n",
       " '',\n",
       " 'but',\n",
       " 'the',\n",
       " 'India',\n",
       " 'financial',\n",
       " 'the',\n",
       " 'is',\n",
       " 'With',\n",
       " 'fa',\n",
       " 'is',\n",
       " 'are',\n",
       " 'can',\n",
       " 'images',\n",
       " '-',\n",
       " 'that',\n",
       " '.',\n",
       " 'closer',\n",
       " 'father',\n",
       " '-',\n",
       " 'affluent',\n",
       " 'learn',\n",
       " 'I',\n",
       " 'beautiful',\n",
       " 'So',\n",
       " '',\n",
       " 'with',\n",
       " 'a',\n",
       " 'less',\n",
       " 'the',\n",
       " 'is',\n",
       " 'expected',\n",
       " 'my',\n",
       " '�',\n",
       " 'my',\n",
       " 'know',\n",
       " 'this',\n",
       " 'some',\n",
       " 'January',\n",
       " 'security',\n",
       " '',\n",
       " 'commem',\n",
       " 'Russia',\n",
       " 'S',\n",
       " 'ifting',\n",
       " 'were',\n",
       " 'favourite',\n",
       " 'understanding',\n",
       " 'appropriate',\n",
       " 'Native',\n",
       " 'worth',\n",
       " 'strategies',\n",
       " 'can',\n",
       " 'or',\n",
       " 'to',\n",
       " 'student',\n",
       " 'and',\n",
       " 'some',\n",
       " 'Harper',\n",
       " 'to',\n",
       " 'how',\n",
       " 'but',\n",
       " 'commanders',\n",
       " '.',\n",
       " 'plane',\n",
       " ',',\n",
       " '',\n",
       " 'have',\n",
       " 'for',\n",
       " 'other',\n",
       " 'a',\n",
       " '-',\n",
       " 'the',\n",
       " 'To',\n",
       " '�',\n",
       " 'an',\n",
       " 'Mar',\n",
       " 'being',\n",
       " 'apply',\n",
       " 'work',\n",
       " 'paid',\n",
       " 'another',\n",
       " 'about',\n",
       " 'M',\n",
       " 'July',\n",
       " '-',\n",
       " 'campaign',\n",
       " 'a',\n",
       " 'announcement',\n",
       " 'which',\n",
       " 'shot',\n",
       " 'B',\n",
       " 'She',\n",
       " '',\n",
       " 'as',\n",
       " 'design',\n",
       " 'and',\n",
       " 'still',\n",
       " '2010',\n",
       " 'name',\n",
       " 'be',\n",
       " 'of',\n",
       " 'in',\n",
       " ',',\n",
       " 'very',\n",
       " 'waiting',\n",
       " 'but',\n",
       " '.',\n",
       " 'I',\n",
       " 't',\n",
       " '�',\n",
       " 'doesn',\n",
       " 'least',\n",
       " 'do',\n",
       " 'And',\n",
       " 'to',\n",
       " 'm',\n",
       " '�',\n",
       " '�',\n",
       " 'hope',\n",
       " 'check',\n",
       " \"'s\",\n",
       " 'of',\n",
       " 'that',\n",
       " 'of',\n",
       " 'have',\n",
       " 'transparency',\n",
       " 'paradigm',\n",
       " 'mess',\n",
       " 'my',\n",
       " 'ull',\n",
       " 'd',\n",
       " 'look',\n",
       " 'strength',\n",
       " 'shapes',\n",
       " 'hope',\n",
       " 'takes',\n",
       " 'in',\n",
       " 'or',\n",
       " 'at',\n",
       " 'gems',\n",
       " 'archaeological',\n",
       " 'sizes',\n",
       " 'to',\n",
       " 'which',\n",
       " 'guide',\n",
       " 'a',\n",
       " 'not',\n",
       " '?',\n",
       " 'over',\n",
       " 'to',\n",
       " '',\n",
       " 'alarm',\n",
       " 'depletion',\n",
       " 'lot',\n",
       " 'we',\n",
       " 'believed',\n",
       " 'Al',\n",
       " 'Ar',\n",
       " 'oceans',\n",
       " 'he',\n",
       " 'government',\n",
       " 'w',\n",
       " 'by',\n",
       " 'uncertainty',\n",
       " 'saving',\n",
       " 'have',\n",
       " 'foot',\n",
       " 'or',\n",
       " 'colour',\n",
       " '5',\n",
       " 'yelling',\n",
       " '.',\n",
       " 'big',\n",
       " 'a',\n",
       " 'about',\n",
       " 'a',\n",
       " 'will',\n",
       " 'the',\n",
       " '/',\n",
       " 'ching',\n",
       " 'giving',\n",
       " 'some',\n",
       " 'at',\n",
       " 'of',\n",
       " 'spectrum',\n",
       " 'his',\n",
       " 'thing',\n",
       " '.',\n",
       " 'it',\n",
       " '.',\n",
       " 'sense',\n",
       " 'the',\n",
       " 'know',\n",
       " 'can',\n",
       " 'small',\n",
       " 'what',\n",
       " 'soon',\n",
       " 'in',\n",
       " 'ADHD',\n",
       " 'whatever',\n",
       " 'health',\n",
       " ',',\n",
       " '',\n",
       " 'If',\n",
       " '.',\n",
       " ',',\n",
       " ',',\n",
       " 'or',\n",
       " 'will',\n",
       " 'at',\n",
       " 'therapy',\n",
       " 'of',\n",
       " 'to',\n",
       " 'it',\n",
       " 'are',\n",
       " 'other',\n",
       " 'here',\n",
       " 'the',\n",
       " ',',\n",
       " 'the',\n",
       " 'for',\n",
       " 'is',\n",
       " ',',\n",
       " 'have',\n",
       " 'he',\n",
       " 'person',\n",
       " 'wife',\n",
       " 'PDF',\n",
       " 'with',\n",
       " 'different',\n",
       " 'you',\n",
       " '/',\n",
       " 'about',\n",
       " 'hair',\n",
       " 'like',\n",
       " 'ation',\n",
       " 'me',\n",
       " 'read',\n",
       " 'parents',\n",
       " 'then',\n",
       " 'amazed',\n",
       " 'on',\n",
       " 'the',\n",
       " 'two',\n",
       " 'where',\n",
       " 'ne',\n",
       " 'doesn',\n",
       " 'The',\n",
       " 'this',\n",
       " 'put',\n",
       " 'idea',\n",
       " 'will',\n",
       " 'but',\n",
       " 'for',\n",
       " 'Friday',\n",
       " 'therapist',\n",
       " 'end',\n",
       " 'he',\n",
       " 'know',\n",
       " 'the',\n",
       " 'will',\n",
       " 'a',\n",
       " 'man',\n",
       " 'toys',\n",
       " 'like',\n",
       " 'sure',\n",
       " 'wait',\n",
       " 'allergies',\n",
       " 'slippery',\n",
       " 'However',\n",
       " 'end',\n",
       " 'ach',\n",
       " 'told',\n",
       " 'autism',\n",
       " ',',\n",
       " 'inge',\n",
       " 'Blue',\n",
       " 'on',\n",
       " 'you',\n",
       " 'world',\n",
       " 'entire',\n",
       " 'simple',\n",
       " 'them',\n",
       " 'the',\n",
       " 'how',\n",
       " 'your',\n",
       " 'of',\n",
       " 'system',\n",
       " 'one',\n",
       " 'them',\n",
       " 'the',\n",
       " 'Pure',\n",
       " 'you',\n",
       " 'way',\n",
       " 'offer',\n",
       " 'I',\n",
       " 'to',\n",
       " 'if',\n",
       " '?',\n",
       " 'nor',\n",
       " 'in',\n",
       " 'the',\n",
       " 'inside',\n",
       " 'plate',\n",
       " 'C',\n",
       " 'Even',\n",
       " 'I',\n",
       " '.',\n",
       " 'are',\n",
       " 'the',\n",
       " 'oil',\n",
       " 'your',\n",
       " 'group',\n",
       " 'conference',\n",
       " 'wy',\n",
       " 'The',\n",
       " 'of',\n",
       " 'the',\n",
       " 'education',\n",
       " 'of',\n",
       " 'We',\n",
       " 'skill',\n",
       " 'companies',\n",
       " 'alle',\n",
       " 'transport',\n",
       " 'more',\n",
       " 'self',\n",
       " 'to',\n",
       " 'with',\n",
       " 'Smith',\n",
       " ',',\n",
       " 'ther',\n",
       " 'mind',\n",
       " 'a',\n",
       " 'with',\n",
       " \"',\",\n",
       " 'wanted',\n",
       " '',\n",
       " 'was',\n",
       " '.',\n",
       " 'miss',\n",
       " 'you',\n",
       " 'such',\n",
       " 'and',\n",
       " 'was',\n",
       " '!',\n",
       " 'well',\n",
       " 'and',\n",
       " 'VE',\n",
       " 'AT',\n",
       " 'all',\n",
       " 'love',\n",
       " 'My',\n",
       " 'because',\n",
       " 'jail',\n",
       " 'He',\n",
       " 'a',\n",
       " 'financial',\n",
       " 'Instit',\n",
       " 'developing',\n",
       " '',\n",
       " 'e',\n",
       " 'during',\n",
       " 'osis',\n",
       " 'from',\n",
       " 'The',\n",
       " 'be',\n",
       " 'gly',\n",
       " '.',\n",
       " ').',\n",
       " 'review',\n",
       " '?\"',\n",
       " 'all',\n",
       " 'to',\n",
       " 'yet',\n",
       " 'are',\n",
       " ',',\n",
       " 'help',\n",
       " 's',\n",
       " 'ably',\n",
       " 'taste',\n",
       " 'source',\n",
       " '�',\n",
       " 'any',\n",
       " 'little',\n",
       " '.',\n",
       " 'to',\n",
       " 'down',\n",
       " 'get',\n",
       " 'it',\n",
       " 'try',\n",
       " 'a',\n",
       " 'and',\n",
       " 'Free',\n",
       " 'right',\n",
       " 'have',\n",
       " 've',\n",
       " 'the',\n",
       " 'evolution',\n",
       " 'this',\n",
       " 'of',\n",
       " 'from',\n",
       " 'and',\n",
       " 'they',\n",
       " 'Man',\n",
       " '',\n",
       " 'right',\n",
       " '.',\n",
       " 'ite',\n",
       " 'to',\n",
       " 'downloaded',\n",
       " 'S',\n",
       " 'berry',\n",
       " ',',\n",
       " '14',\n",
       " '�',\n",
       " 'ever',\n",
       " '(',\n",
       " ',',\n",
       " 'be',\n",
       " ';',\n",
       " ',',\n",
       " 'making',\n",
       " '000',\n",
       " 'who',\n",
       " 'this',\n",
       " 'did',\n",
       " 'body',\n",
       " 'if',\n",
       " 'fuel',\n",
       " '.',\n",
       " '�',\n",
       " 'able',\n",
       " 'improve',\n",
       " 'ative',\n",
       " 'if',\n",
       " '�',\n",
       " 'th',\n",
       " 'being',\n",
       " 'more',\n",
       " 'to',\n",
       " ',',\n",
       " 'highly',\n",
       " 'such',\n",
       " 'has',\n",
       " 'a',\n",
       " 'of',\n",
       " '�',\n",
       " ':',\n",
       " 'as',\n",
       " 'most',\n",
       " \"'s\",\n",
       " 'fitted',\n",
       " 'your',\n",
       " 'lawyer',\n",
       " 'probably',\n",
       " 'therapist',\n",
       " 'you',\n",
       " 'massage',\n",
       " 'therapist',\n",
       " 'do',\n",
       " 'workplace',\n",
       " 'will',\n",
       " 'On',\n",
       " 'find',\n",
       " 'to',\n",
       " 'a',\n",
       " 'before',\n",
       " 'from',\n",
       " 'My',\n",
       " 'and',\n",
       " 'world',\n",
       " '-',\n",
       " 'is',\n",
       " '.',\n",
       " 'post',\n",
       " 'quality',\n",
       " 'are',\n",
       " 'working',\n",
       " 'the',\n",
       " '\"',\n",
       " 'the',\n",
       " 'to',\n",
       " 'drive',\n",
       " 'in',\n",
       " 'I',\n",
       " 'much',\n",
       " 'sense',\n",
       " 'shape',\n",
       " 'which',\n",
       " 'being',\n",
       " 'fourth',\n",
       " 'half',\n",
       " '4',\n",
       " '.',\n",
       " 'and',\n",
       " '�',\n",
       " 'disorder',\n",
       " '�',\n",
       " 'who',\n",
       " 'who',\n",
       " 'with',\n",
       " 'most',\n",
       " '.',\n",
       " '.',\n",
       " 'about',\n",
       " '�',\n",
       " 'that',\n",
       " 'The',\n",
       " '',\n",
       " '�',\n",
       " 'need',\n",
       " 'all',\n",
       " 'illnesses',\n",
       " '�',\n",
       " 'let',\n",
       " 'os',\n",
       " '2',\n",
       " 'areas',\n",
       " ',',\n",
       " '.',\n",
       " 'This',\n",
       " 'in',\n",
       " ',',\n",
       " 'only',\n",
       " 'is',\n",
       " 'this',\n",
       " 'is',\n",
       " 'food',\n",
       " 'w',\n",
       " 'so',\n",
       " 'up',\n",
       " 'that',\n",
       " 'IC',\n",
       " 'of',\n",
       " 'last',\n",
       " 'following',\n",
       " 'Revision',\n",
       " 'a',\n",
       " 'four',\n",
       " 'name',\n",
       " '',\n",
       " 'names',\n",
       " '',\n",
       " 'in',\n",
       " 'required',\n",
       " 'is',\n",
       " 'employees',\n",
       " 'Ontario',\n",
       " 'national',\n",
       " 'Filipino',\n",
       " 'work',\n",
       " '.',\n",
       " 'tax',\n",
       " 'D',\n",
       " 'original',\n",
       " 'bank',\n",
       " 'Social',\n",
       " 'law',\n",
       " 'at',\n",
       " 'citizenship',\n",
       " 'by',\n",
       " 'for',\n",
       " 'codes',\n",
       " 'banking',\n",
       " 'setting',\n",
       " 'and',\n",
       " 'The',\n",
       " 'the',\n",
       " 'is',\n",
       " 'UC',\n",
       " 'Number',\n",
       " 'is',\n",
       " 'about',\n",
       " ')',\n",
       " 'is',\n",
       " 'to',\n",
       " 'which',\n",
       " 'is',\n",
       " 'Car',\n",
       " 'the',\n",
       " 'SO',\n",
       " 'Governors',\n",
       " 'have',\n",
       " 'use',\n",
       " '.',\n",
       " 'students',\n",
       " 'Social',\n",
       " ',',\n",
       " '',\n",
       " 'and',\n",
       " '02',\n",
       " '.',\n",
       " 'on',\n",
       " 'crack',\n",
       " 'at',\n",
       " 'the',\n",
       " \"'s\",\n",
       " 'American',\n",
       " 'sweet',\n",
       " 'been',\n",
       " 'a',\n",
       " 'a',\n",
       " 'not',\n",
       " 'a',\n",
       " '',\n",
       " 'the',\n",
       " 'make',\n",
       " 'creamy',\n",
       " 'delicious',\n",
       " 'good',\n",
       " 'increase',\n",
       " 'most',\n",
       " 'main',\n",
       " 'These',\n",
       " 'a',\n",
       " 'for',\n",
       " 'around',\n",
       " 'games',\n",
       " '.',\n",
       " 'of',\n",
       " 'simple',\n",
       " '-',\n",
       " 'by',\n",
       " 'game',\n",
       " '.',\n",
       " 'of',\n",
       " ',',\n",
       " 'les',\n",
       " ']',\n",
       " \"'s\",\n",
       " 'an',\n",
       " 'ude',\n",
       " 'ment',\n",
       " 'te',\n",
       " '2010',\n",
       " 'game',\n",
       " '\".',\n",
       " 'made',\n",
       " ',',\n",
       " 'tweet',\n",
       " \"'\",\n",
       " 'ach',\n",
       " 'government',\n",
       " 'in',\n",
       " 'any',\n",
       " 'the',\n",
       " 'with',\n",
       " 'a',\n",
       " 'notice',\n",
       " 'got',\n",
       " '30',\n",
       " \"'t\",\n",
       " 'have',\n",
       " 'real',\n",
       " 'series',\n",
       " ').',\n",
       " 'can',\n",
       " 'chemical',\n",
       " '.',\n",
       " 'us',\n",
       " 'artists',\n",
       " 'ien',\n",
       " 'to',\n",
       " 'they',\n",
       " 'group',\n",
       " 'the',\n",
       " 'of',\n",
       " 't',\n",
       " 'comes',\n",
       " 'little',\n",
       " 'ers',\n",
       " 'the',\n",
       " '.',\n",
       " 'impressed',\n",
       " 'the',\n",
       " 'the',\n",
       " 'enforcement',\n",
       " 'have',\n",
       " '',\n",
       " 'the',\n",
       " 'the',\n",
       " '&',\n",
       " 'H',\n",
       " 'them',\n",
       " 'that',\n",
       " 'there',\n",
       " 'village',\n",
       " 'free',\n",
       " 'E',\n",
       " 'approaching',\n",
       " 'periods',\n",
       " 'over',\n",
       " 'Ram',\n",
       " 'accommodation',\n",
       " 'er',\n",
       " 'to',\n",
       " 'he',\n",
       " 'of',\n",
       " 'town',\n",
       " 'est',\n",
       " 'on',\n",
       " 'airport',\n",
       " ',',\n",
       " 'che',\n",
       " 'with',\n",
       " 'to',\n",
       " 'rast',\n",
       " '.',\n",
       " 'ever',\n",
       " 'longer',\n",
       " 'back',\n",
       " 'the',\n",
       " 'reacting',\n",
       " 'searching',\n",
       " 'Mast',\n",
       " '....',\n",
       " '...',\n",
       " 'ie',\n",
       " 'India',\n",
       " 'com',\n",
       " 'of',\n",
       " '.',\n",
       " '',\n",
       " 'are',\n",
       " '8',\n",
       " 'Athlet',\n",
       " 'around',\n",
       " 'behavior',\n",
       " 'and',\n",
       " 'were',\n",
       " 'it',\n",
       " 'for',\n",
       " 'I',\n",
       " \"'s\",\n",
       " 'that',\n",
       " 'of',\n",
       " 'However',\n",
       " '.',\n",
       " '.',\n",
       " 'about',\n",
       " 'feeling',\n",
       " 'or',\n",
       " '',\n",
       " 'spend',\n",
       " 'day',\n",
       " 'life',\n",
       " 'the',\n",
       " '.',\n",
       " 'piece',\n",
       " 'wrapped',\n",
       " '.',\n",
       " 'list',\n",
       " 'use',\n",
       " 'it',\n",
       " 'them',\n",
       " 'a',\n",
       " 'in',\n",
       " '$',\n",
       " 't',\n",
       " 'way',\n",
       " 'gives',\n",
       " 'completely',\n",
       " '.',\n",
       " '509',\n",
       " '.',\n",
       " 'is',\n",
       " 'a',\n",
       " '\"',\n",
       " 'app',\n",
       " 'more',\n",
       " 'which',\n",
       " 'Mac',\n",
       " 'up',\n",
       " 'their',\n",
       " 'our',\n",
       " 'in',\n",
       " 'and',\n",
       " '',\n",
       " 'are',\n",
       " '.',\n",
       " 'i',\n",
       " 'vo',\n",
       " ',',\n",
       " 'no',\n",
       " 'Year',\n",
       " 'de',\n",
       " 'ette',\n",
       " 'July',\n",
       " ',',\n",
       " 'private',\n",
       " \"'s\",\n",
       " 'based',\n",
       " 'no',\n",
       " 'staff',\n",
       " 'interview',\n",
       " 'manufacturing',\n",
       " 'and',\n",
       " 'career',\n",
       " 'field',\n",
       " 'We',\n",
       " 'service',\n",
       " 'Welcome',\n",
       " 'and',\n",
       " 'stay',\n",
       " 'a',\n",
       " 'CLI',\n",
       " 'or',\n",
       " 'other',\n",
       " 'careful',\n",
       " ',',\n",
       " 'the',\n",
       " 'paper',\n",
       " 'cargo',\n",
       " 'of',\n",
       " 'assist',\n",
       " 'an',\n",
       " 'needed',\n",
       " 'se',\n",
       " '2',\n",
       " '1999',\n",
       " \"'s\",\n",
       " '',\n",
       " 'we',\n",
       " 'ready',\n",
       " ',',\n",
       " 'of',\n",
       " 'huge',\n",
       " '',\n",
       " 'if',\n",
       " '.',\n",
       " 'the',\n",
       " 'do',\n",
       " 'best',\n",
       " 'the',\n",
       " 'Reynolds',\n",
       " ',',\n",
       " 'Ag',\n",
       " 'the',\n",
       " 'estimated',\n",
       " 'environment',\n",
       " 'about',\n",
       " 'The',\n",
       " 'but',\n",
       " 'accessibility',\n",
       " ',',\n",
       " 'brand',\n",
       " 'national',\n",
       " 'than',\n",
       " 'of',\n",
       " 'necessity',\n",
       " 'care',\n",
       " 'city',\n",
       " 'wave',\n",
       " 'ires',\n",
       " 'further',\n",
       " 'below',\n",
       " 'dinner',\n",
       " 'ac',\n",
       " 'in',\n",
       " 'are',\n",
       " 'out',\n",
       " 'to',\n",
       " 'of',\n",
       " 'Ways',\n",
       " 'ones',\n",
       " '.',\n",
       " '.',\n",
       " '',\n",
       " 'will',\n",
       " '',\n",
       " 'of',\n",
       " 'one',\n",
       " 'difficult',\n",
       " 'in',\n",
       " 'more',\n",
       " 'an',\n",
       " 'to',\n",
       " 'is',\n",
       " 'showing',\n",
       " 'how',\n",
       " '8',\n",
       " '-',\n",
       " '.',\n",
       " 'is',\n",
       " 'for',\n",
       " 'doing',\n",
       " 'to',\n",
       " 'a',\n",
       " 'you',\n",
       " 'at',\n",
       " 'interesting',\n",
       " 'v',\n",
       " 'in',\n",
       " 'were',\n",
       " 'celebrate',\n",
       " 'still',\n",
       " 'figured',\n",
       " 'a',\n",
       " 're',\n",
       " 'ed',\n",
       " 'on',\n",
       " 'last',\n",
       " 'was',\n",
       " ',',\n",
       " 'the',\n",
       " 'don',\n",
       " 'but',\n",
       " '.',\n",
       " 'the',\n",
       " 'ad',\n",
       " 'than',\n",
       " 'iots',\n",
       " 'numbered',\n",
       " '�',\n",
       " ',',\n",
       " 'and',\n",
       " 'Kate',\n",
       " ',',\n",
       " 'have',\n",
       " ...]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target prediction loss: 14.111886978149414\n",
      "Dinalar loss: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Print loss statistics\n",
    "target_generated_tokens = data[\"target_generated_tokens\"]\n",
    "target_acts = data[\"target_acts\"]\n",
    "\n",
    "buffer_size = target_acts.shape[0]\n",
    "batch_size = cfg.train_batch_size_samples\n",
    "num_batches = buffer_size // batch_size\n",
    "\n",
    "target_prediction_losses = []\n",
    "dinalar_losses = []\n",
    "\n",
    "for batch_idx in range(num_batches):\n",
    "    start_idx = batch_idx * batch_size\n",
    "    end_idx = start_idx + batch_size\n",
    "\n",
    "    # Extract batch data and move to device\n",
    "    batch_tokens = target_generated_tokens[start_idx:end_idx].to(cfg.device)\n",
    "    batch_acts = target_acts[start_idx:end_idx].to(cfg.device)\n",
    "\n",
    "    loss, target_prediction_loss, dinalar_loss = EncoderTrainer.loss(\n",
    "        cfg, encoder_decoder, batch_tokens, batch_acts, -1\n",
    "    )\n",
    "\n",
    "    target_prediction_losses.append(target_prediction_loss.item())\n",
    "    dinalar_losses.append(dinalar_loss.item())\n",
    "\n",
    "print(f\"Target prediction loss: {np.mean(target_prediction_losses)}\")\n",
    "print(f\"Dinalar loss: {np.mean(dinalar_losses)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jaxtyping import Float\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "def get_distances_to_embeddings(\n",
    "    embeddings: Float[Tensor, \"batch tok d_embed\"],\n",
    "    target_embeddings: Float[Tensor, \"vocab d_embed\"],\n",
    ") -> Float[Tensor, \"batch tok vocab\"]:\n",
    "    \"\"\"Computes the L2 distance between each token embedding and each target embedding.\"\"\"\n",
    "    # embeddings shape: [batch, tok, d_embed]\n",
    "    # target_embeddings shape: [vocab, d_embed]\n",
    "\n",
    "    # Reshape embeddings to [batch*tok, d_embed]\n",
    "    batch_size, seq_len, d_embed = embeddings.shape\n",
    "    embeddings_flat = embeddings.reshape(-1, d_embed)\n",
    "\n",
    "    # Compute pairwise distances between all embeddings and target embeddings\n",
    "    # Returns tensor of shape [batch*tok, vocab]\n",
    "    distances = torch.cdist(embeddings_flat, target_embeddings, p=2)\n",
    "\n",
    "    # Reshape back to [batch, tok, vocab]\n",
    "    return distances.reshape(batch_size, seq_len, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f21c2e5e8c2c4ac5bc32dcd94b3f9109",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d80d3dc4f31461da9e1c6219ce7618c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[31034,  7412, 13850, 19247, 12693],\n",
      "        [ 4031,  8223, 31350, 27743,  9646]], device='cuda:0')\n",
      "torch.Size([2, 5])\n",
      "top_values.shape torch.Size([2, 5])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[40871, 18388, 48497, 26023, 34892],\n",
      "        [ 8814, 17051, 17444,  7130, 42472]], device='cuda:0')\n",
      "torch.Size([2, 5])\n",
      "top_values.shape torch.Size([2, 5])\n",
      "tensor([[33902,  8397, 33356, 30988, 25630],\n",
      "        [28192, 15642, 49287, 23604, 17106]], device='cuda:0')\n",
      "torch.Size([2, 5])\n",
      "top_values.shape torch.Size([2, 5])\n",
      "tensor([[37424,  9986,  3665, 36891, 11480],\n",
      "        [47988,  1145,   308, 38868,  8389]], device='cuda:0')\n",
      "torch.Size([2, 5])\n",
      "top_values.shape torch.Size([2, 5])\n",
      "tensor([[  658,   854, 45152, 46170, 23502],\n",
      "        [ 1813, 33876, 42472, 50063, 49942]], device='cuda:0')\n",
      "torch.Size([2, 5])\n",
      "top_values.shape torch.Size([2, 5])\n",
      "tensor([[30988, 46315, 18147,   760, 39168],\n",
      "        [36771, 15703, 18221,  5769,  7388]], device='cuda:0')\n",
      "torch.Size([2, 5])\n",
      "top_values.shape torch.Size([2, 5])\n",
      "tensor([[ 8962, 14278,  5533,  2103, 27211],\n",
      "        [44232, 42053, 19505,  1360,  5013]], device='cuda:0')\n",
      "torch.Size([2, 5])\n",
      "top_values.shape torch.Size([2, 5])\n",
      "tensor([[39087, 23808,  1517,  9932, 15691],\n",
      "        [24446,  1222, 22368,   708, 38579]], device='cuda:0')\n",
      "torch.Size([2, 5])\n",
      "top_values.shape torch.Size([2, 5])\n",
      "tensor([[ 8649,   143,  2982, 26490, 12875],\n",
      "        [42053, 28320, 44447, 18221, 15272]], device='cuda:0')\n",
      "torch.Size([2, 5])\n",
      "top_values.shape torch.Size([2, 5])\n",
      "tensor([[37436, 11709,  4915,  2552, 24672],\n",
      "        [ 7415, 18291, 20919, 40527, 43743]], device='cuda:0')\n",
      "torch.Size([2, 5])\n",
      "top_values.shape torch.Size([2, 5])\n",
      "tensor([[    6, 13262, 15728, 21315, 49522],\n",
      "        [47132, 33771, 44425, 10131, 34867]], device='cuda:0')\n",
      "torch.Size([2, 5])\n",
      "top_values.shape torch.Size([2, 5])\n",
      "tensor([[20233, 50026,  7412, 25379, 47919],\n",
      "        [49275, 36291, 12899,   180,  4409]], device='cuda:0')\n",
      "torch.Size([2, 5])\n",
      "top_values.shape torch.Size([2, 5])\n",
      "tensor([[ 5527, 23589,  6047,  4915,  1960],\n",
      "        [14010, 23673, 29898, 27180, 46865]], device='cuda:0')\n",
      "torch.Size([2, 5])\n",
      "top_values.shape torch.Size([2, 5])\n",
      "tensor([[37299, 50186,  7583,  2853, 20255],\n",
      "        [25932,  5225, 46584, 18342, 26017]], device='cuda:0')\n",
      "torch.Size([2, 5])\n",
      "top_values.shape torch.Size([2, 5])\n"
     ]
    }
   ],
   "source": [
    "# Create output widgets for displaying sample information\n",
    "sample_output = widgets.Output()\n",
    "button_output = widgets.Output()\n",
    "\n",
    "# Create a counter and button\n",
    "current_sample = 0\n",
    "\n",
    "\n",
    "def on_button_click(b):\n",
    "    global current_sample\n",
    "    if current_sample < batch_size:\n",
    "        display_sample(current_sample)\n",
    "        current_sample += 1\n",
    "    else:\n",
    "        with sample_output:\n",
    "            print(\"End of batch reached!\")\n",
    "\n",
    "\n",
    "def create_table(title, headers, rows, col_widths=None):\n",
    "    \"\"\"Helper function to create formatted tables\n",
    "\n",
    "    Args:\n",
    "        title: Table title string\n",
    "        headers: List of header strings\n",
    "        rows: List of rows, where each row is a list of values\n",
    "        col_widths: List of column widths (defaults to 15 for all columns)\n",
    "\n",
    "    Returns:\n",
    "        Formatted table string\n",
    "    \"\"\"\n",
    "    if col_widths is None:\n",
    "        col_widths = [15] * len(headers)\n",
    "\n",
    "    # Ensure first column width accommodates row labels\n",
    "    col_widths[0] = max(col_widths[0], 8)\n",
    "\n",
    "    # Create table string\n",
    "    table = f\"{title}\\n\"\n",
    "\n",
    "    # Create header\n",
    "    header_row = headers[0].ljust(col_widths[0])\n",
    "    for i, header in enumerate(headers[1:], 1):\n",
    "        header_row += header.ljust(col_widths[i])\n",
    "    table += header_row + \"\\n\"\n",
    "\n",
    "    # Add separator\n",
    "    table += \"-\" * len(header_row) + \"\\n\"\n",
    "\n",
    "    # Add rows\n",
    "    for row in rows:\n",
    "        row_str = str(row[0]).ljust(col_widths[0])\n",
    "        for i, cell in enumerate(row[1:], 1):\n",
    "            row_str += str(cell).ljust(col_widths[i])\n",
    "        table += row_str + \"\\n\"\n",
    "\n",
    "    return table\n",
    "\n",
    "\n",
    "def shuffle_data(tokens, acts, seed=None):\n",
    "    \"\"\"\n",
    "    Shuffle the tokens and acts tensors in the same order.\n",
    "\n",
    "    Args:\n",
    "        tokens: Tensor of token IDs\n",
    "        acts: Tensor of activations\n",
    "        seed: Optional random seed for reproducibility\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (shuffled_tokens, shuffled_acts)\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "    # Get the number of samples\n",
    "    num_samples = tokens.size(0)\n",
    "\n",
    "    # Generate random permutation indices\n",
    "    indices = torch.randperm(num_samples)\n",
    "\n",
    "    # Shuffle both tensors using the same indices\n",
    "    shuffled_tokens = tokens[indices]\n",
    "    shuffled_acts = acts[indices]\n",
    "\n",
    "    return shuffled_tokens, shuffled_acts\n",
    "\n",
    "\n",
    "# Function to display a single sample\n",
    "def display_sample(sample_idx):\n",
    "    with torch.no_grad():\n",
    "        with torch.autocast(device_type=cfg.device.type, dtype=cfg.dtype):\n",
    "            # Extract single sample as a \"batch\" of size 1\n",
    "            sample_tokens = target_generated_tokens[sample_idx : sample_idx + 1].to(\n",
    "                cfg.device\n",
    "            )\n",
    "            sample_acts = target_acts[sample_idx : sample_idx + 1].to(cfg.device)\n",
    "\n",
    "            # encoder = encoder_decoder.encoder\n",
    "            # encoder_output_logits = encoder(sample_acts) # [batch tok vocab]\n",
    "\n",
    "            # Convert logits to one-hot-like by making the max value very large and others small\n",
    "            # max_values, max_indices = torch.max(encoder_output_logits, dim=-1, keepdim=True)\n",
    "            # one_hot_logits = torch.ones_like(encoder_output_logits) * -100.0  # Set all values to a small number\n",
    "            # one_hot_logits.scatter_(dim=-1, index=max_indices, value=100.0)   # Set max values to a large number\n",
    "            # encoder_output_logits = one_hot_logits\n",
    "\n",
    "            # Get model outputs for this single sample\n",
    "            (decoder_logits_target, decoder_logits_encoding, virtual_embs) = (\n",
    "                encoder_decoder(sample_acts, sample_tokens, train_iter=-1)\n",
    "            )\n",
    "\n",
    "            # Calculate losses using existing functions\n",
    "            pred_loss = calculate_target_prediction_loss(\n",
    "                decoder_logits_target, sample_tokens\n",
    "            ).item()\n",
    "            # din_loss = calculate_dinalar_loss(\n",
    "            #     decoder_logits_encoding,\n",
    "            #     encoder_output_logits,\n",
    "            # ).item()\n",
    "\n",
    "            embeddings = get_embeddings_from_decoder(encoder_decoder.decoder).weight\n",
    "            distances = get_distances_to_embeddings(\n",
    "                virtual_embs, embeddings\n",
    "            )  # [batch tok vocab]\n",
    "\n",
    "            # print(\"distances.shape\", distances.shape)\n",
    "\n",
    "            encoder_output_probs = torch.softmax(encoder_output_logits, dim=-1)\n",
    "\n",
    "            # Get top 3 tokens by encoder output logits\n",
    "            top_k = 5\n",
    "            top_values, top_indices = torch.topk(\n",
    "                encoder_output_probs[0], k=top_k, dim=-1\n",
    "            )  # [batch tok]\n",
    "\n",
    "            print(top_indices)\n",
    "            print(top_indices.shape)\n",
    "\n",
    "            print(\"top_values.shape\", top_values.shape)\n",
    "\n",
    "            # Decode tokens for display\n",
    "            sample_decoded = tokenizer.decode(sample_tokens[0])\n",
    "\n",
    "            # Display results\n",
    "            with sample_output:\n",
    "                sample_output.clear_output(wait=True)\n",
    "                print(f\"Sample {sample_idx+1}/{batch_size}\")\n",
    "                print(f\"Target prediction loss: {pred_loss:.6f}\")\n",
    "                print(\"\\nTarget tokens:\")\n",
    "                print(sample_decoded, \"\\n\")\n",
    "\n",
    "                prompt_prefix, prompt_suffix = PROMPT_DECODER.split(\"<thought>\")\n",
    "                # Also decode and display the prefix and suffix tokens\n",
    "                prefix_tokens = tokenizer(prompt_prefix, return_tensors=\"pt\").input_ids[\n",
    "                    0\n",
    "                ]\n",
    "                suffix_tokens = tokenizer(prompt_suffix, return_tensors=\"pt\").input_ids[\n",
    "                    0\n",
    "                ]\n",
    "\n",
    "                print(tokenizer.decode(prefix_tokens))\n",
    "\n",
    "                # Prepare data for the tokens table\n",
    "                col_width = 15\n",
    "                headers = [\"Token\"] + [f\"Emb {i}\" for i in range(virtual_embs.shape[1])]\n",
    "\n",
    "                token_rows = []\n",
    "                for k in range(top_k):\n",
    "                    row = [f\"Top {k+1}:\"]\n",
    "                    for j in range(virtual_embs.shape[1]):\n",
    "                        token_id = top_indices[j, k].item()\n",
    "                        token_text = tokenizer.decode([token_id])\n",
    "                        # Replace newlines and tabs for cleaner display\n",
    "                        token_text = token_text.replace(\"\\n\", \"\\\\n\").replace(\n",
    "                            \"\\t\", \"\\\\t\"\n",
    "                        )\n",
    "                        # Truncate to fit in column\n",
    "                        token_display = token_text[: col_width - 2]\n",
    "                        row.append(token_display)\n",
    "                    token_rows.append(row)\n",
    "\n",
    "                # Create and display the token table\n",
    "                token_table = create_table(\n",
    "                    \"\", headers, token_rows, [8] + [col_width] * virtual_embs.shape[1]\n",
    "                )\n",
    "                print(token_table)\n",
    "\n",
    "                # Create table for logit values\n",
    "                logit_rows = []\n",
    "                for k in range(top_k):\n",
    "                    row = [f\"Top {k+1}:\"]\n",
    "                    for j in range(virtual_embs.shape[1]):\n",
    "                        # Format logit value with 5 decimal places\n",
    "                        prob = top_values[j, k].item()\n",
    "                        row.append(f\"{prob:.5f}\")\n",
    "                    logit_rows.append(row)\n",
    "\n",
    "                # Create and display the logits table\n",
    "                logit_table = create_table(\n",
    "                    \"Logit Values:\",\n",
    "                    headers,\n",
    "                    logit_rows,\n",
    "                    [8] + [col_width] * virtual_embs.shape[1],\n",
    "                )\n",
    "                print(logit_table)\n",
    "\n",
    "                print(tokenizer.decode(suffix_tokens))\n",
    "\n",
    "\n",
    "# Interactive sample investigation\n",
    "next_button = widgets.Button(description=\"Next Sample\")\n",
    "next_button.on_click(on_button_click)\n",
    "\n",
    "# Display the button and sample output in separate areas\n",
    "with button_output:\n",
    "    display(next_button)\n",
    "display(button_output)\n",
    "display(sample_output)\n",
    "\n",
    "# Show the first sample\n",
    "if batch_size > 0:\n",
    "    display_sample(current_sample)\n",
    "    current_sample += 1\n",
    "else:\n",
    "    with sample_output:\n",
    "        print(\"No samples in batch!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = \"Bob\"\n",
    "# tokens = tokenizer(text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "# Print each token separately\n",
    "# print(\"Tokens for text:\", text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[44484,   474,   330,   672,  1559,   373,   257,   922,  1048,    13,\n",
      "         14862,   220]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "tensor([[44484,   474,   330,   672,  1559,   373,   257,   922,  1048,    13,\n",
      "         14862,   220]])\n",
      "torch.Size([50257])\n",
      "Top 10 tokens by logit:\n",
      "1. Token: ' ', ID: 1849, Logit: -46.7644\n",
      "2. Token: 'iced', ID: 3711, Logit: -47.3270\n",
      "3. Token: 'ich', ID: 488, Logit: -48.3128\n",
      "4. Token: 'ive', ID: 425, Logit: -48.6372\n",
      "5. Token: 'irl', ID: 1901, Logit: -48.7395\n",
      "6. Token: 'ix', ID: 844, Logit: -49.0847\n",
      "7. Token: 'iz', ID: 528, Logit: -49.1540\n",
      "8. Token: 'izzy', ID: 40593, Logit: -49.2239\n",
      "9. Token: 'ike', ID: 522, Logit: -49.2902\n",
      "10. Token: '________', ID: 2602, Logit: -49.3509\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "text = \"Alice jacobson was a good person. Alice \"\n",
    "tokens = tokenizer(text, add_special_tokens=True, return_tensors=\"pt\")\n",
    "print(tokens)\n",
    "print(tokens.input_ids)\n",
    "tokens = torch.cat([torch.tensor([[tokenizer.bos_token_id]]), tokens.input_ids], dim=1)\n",
    "\n",
    "# Get model outputs\n",
    "outputs = model(tokens)\n",
    "logits = outputs.logits\n",
    "\n",
    "# Get the logits for the last token\n",
    "last_token_logits = logits[0, -1, :]\n",
    "print(last_token_logits.shape)\n",
    "\n",
    "# Get the top 3 token indices and their corresponding logits\n",
    "k = 10\n",
    "top_values, top_indices = torch.topk(last_token_logits, k)\n",
    "\n",
    "# Print the top 3 tokens and their logits\n",
    "print(f\"Top {k} tokens by logit:\")\n",
    "for i, (token_id, logit_value) in enumerate(zip(top_indices, top_values)):\n",
    "    token_text = tokenizer.decode([token_id.item()])\n",
    "    print(\n",
    "        f\"{i+1}. Token: '{token_text}', ID: {token_id.item()}, Logit: {logit_value.item():.4f}\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
