# EXPLAP (EXPlanation generation from Language model Prediction)

Background
Neural network interpetability would be greatly benefited by a robust quantitative measure for the human interpretability of an explanation. Why are we not satisfied with the application of matmuls and activation functions during the forward pass as an explanation for how a neural network makes a decision? This explanation is perfectly precise and concrete, so what is it about it that us humans don’t like?

Let’s consider the hypothetical scenario in which we had an accurate, concrete answer to this question, to the extent that we could design a function that could produce a quantitative human interpretability measure given any representation as input. If we had such an interpretability measure, we could, for example, just add a regularization term to part of, or our entire, neural network during training. Or, if we wanted to take the post-hoc approach, we could train an autoencoder on a pretrained neural network’s activations, with this interpretability measure as a regularization penalty on the autoencoder’s internal activations.

In reality, we don’t know what makes an explanation human interpretable. Instead, we have discovered useful quantitative proxies for human interpretability such as sparsity (although I reference SAEs, a more common explanation for SAEs is not that sparsity is a proxy for interpretability in some fundamental sense, but instead that the linear superposition hypothesis is an accurate model of neural network function, however, recent work has shown that the first explanation is more likely) and description lengths. While these proxies have provided astounding progress in neural network interpetability, their limitations as proxies have become apparent and act as fundamental blockers of progress.

The fundamental motivation for this project is that while it may not be feasible to attain a first-principles understanding of what makes a human explanation interpretable, we can nonetheless design a quantitative measure that is a very close proxy to human interpretability by formulating it as the extent to which a pretrained, frozen LLM can utilize the explanation to make accurate predictions. One way to see why this might be true is that one property of a human interpretable explanation would be that it could be used by a human to make accurate predictions and therefore, to the extent that humans and LLMs have similar ontologies / circuitry (which we may expect as a result of pretraining), this may work well.
Project Proposal
I propose training an encoder (parameterized by any neural-based architecture) to transform the activations of a target LLM into the token embedding space of another frozen LLM and then having that frozen LLM predict tokens that are generated by the target LLM in the future (i.e., later in the sequence). The core intuition behind this idea is that the frozen LLM decoder will impose a human interpretability constraint on the encoder outputs, as those encoder outputs must feed into circuits that are human-oriented due to the decoder LLM's training on human data.

My current plan for this project has a few components:
A target model, which is some LLM that we are hoping to interpret. This will likely be post-trained, as we will likely be interested in analyzing intentions of the model in a chat-based setting.
An encoder, which takes as input the activations of the target model, and outputs a fixed number, $$n_{enc}$$, of vectors of dimension $$d_{dec}$$, which is the dimensionality of the token embedding space of the decoder, which I will explain next.
A decoder, which is another pretrained, frozen LLM. The decoder’s context includes fixed discrete tokens that are statically set before optimization, and continuous encoder-generated “token” embeddings (we’ll call them virtual token embeddings), which are at fixed locations in the context. Therefore, the token embeddings that correspond to fixed discrete tokens will (obviously) exactly map to discrete tokens, but the token embeddings that are produced by the encoder will almost certainly not correspond exactly to embeddings associated with discrete tokens in the vocab (and we will use their proximity to embeddings of the vocab to determine their meaning).

The procedure is as follows:
Run the target model on sequences of text, which may include web text, existing chat-based datasets, and text generated solely by the target model in an autoregressive fashion. Collect the residual stream activations at some layer of the target model, as well as the contiguous sequence of tokens including and immediately following the token being predicted by the target model, for each sample. So, for each token being predicted by the target model, we collect that token and the following $$n_{preempt}-1$$ tokens, for a total of $$n_{preempt}$$ tokens, which the decoder will try to predict given the encoder’s output.
For each sample, pass the target model activations to the encoder, and collect the $$n_{enc}$$ virtual embeddings that are produced.
Insert these virtual embeddings into their designated spots in the decoder context, which also includes static embeddings corresponding to tokens we chose as hyperparameters. For example, the decoder’s context may look something like "You are tasked with predicting what this model will say, given these 5 words which describe what the model is currently thinking: <virtual_embedding>,<virtual_embedding>, <virtual_embedding>, ...", where there are $$n_{enc}$$ instances of <virtual_embedding>.
Run the forward pass of the decoder given this context, with the objective to predict the next token in the sequence of $$n_{preempt}$$ tokens. Compute the cross entropy loss of this prediction.
Optimize only the encoder’s parameters to minimize the cross entropy loss of the decoder’s predictions.
(Optional) To encourage the encoder to produce sequences of tokens that are more likely coherent sequences of natural language, we can also compute what I’ll call a direct natural language regularization (DiNaLaR) term. One simple way to do this would be to penalize the distances between the virtual token embeddings and embeddings corresponding to actual tokens. In a more complicated setup, we could obtain this by applying $$W_{unembed}$$ and a softmax to each of the embeddings of the decoder model at the positions preceding where the virtual embeddings are inserted, obtaining a probability distribution of tokens for each location corresponding to a virtual embedding. Then, the DiNaLaR term could be the sum of the distance penalties between the virtual embeddings and the token embeddings corresponding to the decoder-generated probability distributions, weighted by each token’s probability.

The intended result is that the resulting encoder outputs in LLM token embedding space would come to correspond to sensible natural language descriptions of the target model's behavior (although this is the key question behind this project, and would need to be verified empirically). In some sense, the idea behind this project is to train an encoder to transform target model activations to human-interpretable tokens that can be used by an LLM to predict future target model outputs. This optimization in token embedding space is similar to the prefix tuning work from Percy Liang’s group.

The idea behind having the decoder predict tokens generated by future target LLM tokens, as opposed to, e.g., just the token corresponding to the activations used as encoder input, or previous tokens in context, is that we want the encoder outputs to map to descriptions of the model's intent, rather than just a compressed (i.e., fewer token embeddings outputted by the encoder than used in the target model's context window) representation of what the target model is currently outputting. Although predicting future tokens is admittedly likely not a perfect proxy for model intent, it does seem to catch more behaviors that we would be interested in from a safety perspective than would be caught by predicting the current token. For example, if the target model is being deceptive, this will correspond to future behavior in which the model eventually acts (e.g., in private) to advance its hidden intent, and we may expect the encoder outputs to produce token embeddings roughly corresponding to "deception" so that the decoder can accurately predict future deceptive tokens.

One core tension in this project is between decoder expressiveness and human interpretability. For example, you could, for example, start with a pretrained LLM for the decoder, but then allow the decoder weights to be adjusted during optimization, but then we would expect the encoder outputs to deviate from coherent sequences of tokens in embedding space (or deviate from all tokens). But, without any other strategies, the frozen decoder LLM would not necessarily be oriented to use the encoder outputs in the best way possible to predict the future target model outputs. My current solution to this is just to add fixed tokens to the decoder's input, in addition to the token embeddings produced by the encoder, which specify the task of the decoder LLM.

A successful outcome of this project would be that we find that the encoder outputs are close to actual tokens in token embedding space and that these tokens correspond to sensible descriptions of the target model's future outputs. For example, if the target model is asked a question about what the capital of Canada is, we might find that the $$n_{enc}$$ embeddings are close, in embedding space, to tokens "explanation", "ottawa", and "truthful" when the encoder is passed activations immediately preceding its answer. In the absolute best case, we find that the encoder outputs interpolate zero-shot to other behaviors. For example, if we somehow provoke the target model to act deceptively (e.g., possibly in a setup similar to "Alignment faking in large language models", we would find that the encoder outputs would accurately pick out that the target model is being deceptive, possibly by directly outputting "deception".

If we want to ensure that the encoder's outputs correspond to actual tokens, we could also have the encoder output directly into (discrete) token space and use RL for optimization, probably something like GRPO.
Experiments
Many of what I see to be the most important experiments for this method revolve around constructing scenarios in which we know the LLM will behave in a certain way (e.g., deceptively or untruthfully) and seeing if the encoder produces tokens that somehow indicate this behavior.

For example, we could start with the setup described in the sleeper agents paper from Anthropic, where the presence of a certain text snippet (in this case, that the year is 2024) in the prompt induces deceptive behavior. We would then apply the encoder setup to this model, and see whether including the deception-inducing text snippet in the target model’s prompt causes the encoder to produce tokens that are somewhat indicative of the future deceptive behavior. In the easiest case, we would train our encoder on actual examples with the deception-inducing text snippet, but a more difficult test (and one which I think could plausibly be achieved with this approach) would be to just train the encoder on other text, and see whether it catches deception zero-shot on these examples. This would be an incredibly impactful result for AI alignment, as it would likely be difficult to collect training data for actual deceptive behavior in increasingly capable models in the wild. We might expect our encoder to succeed at this if it has built up general circuits for translating target model activations to predictive natural language.

